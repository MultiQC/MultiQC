You are an expert programming assistant helping build a project called MultiQC.

## About MultiQC

MultiQC is a command-line reporting tool that discovers and parses results and logs from bioinformatics tools, such as: log files, console outputs, or reports they write to disk. Then MultiQC aggregates parsed data accross samples and tools, and generates a standalone poratble HTML report. This report helps to analyse containing multiple samples and multiple analysis steps and perform quality control.

MultiQC is typically called as a last step in bioinformatics pipelines to aggregate quality control metrics from all previous steps. When run on a pipeline output folder, it recursively searches it to discover and load all the relevant files.

Base MultiQC codebase is written in Python, and it also uses JavaScript with jQuery and CSS embedded in the HTML report.

## Output

MultiQC primary output is a standalone portable HTML file. It starts with a "General Statistics" table with key metrics across all samples and tools. Followed, more detailed sections split by specific tools, that can include tables, as well as plots of different types (bar plot, line plot, scatter plot, heatmap, etc.)

In addition to a HTML report, MultiQC generates a directory of parsed data files with consistent data structure. It also places there a JSON file with the same data, which can be used as an input for other tools, or for downstream analysis.


## MultiQC codebase structure

- `multiqc/modules` - all the MultiQC "modules", plus 3 special-case modules: "software_versions", "profile_runtime", and "custom_content". The latter includes code to parse custom, non-tool sections and plots passed by end user directly thought configs or TSV/CSV files.
- `multiqc/core` - core codebase for log discovery, running modules, logging, output writing, and AI summarization.
- `multiqc/plots` - plotting code: describes how to prepare data and layouts to render with Plotly. Plots can be rendered both statically with Python Plotly library, as well as dynamically in browser with Plotly-JS library, so this code also prepares required data dumps to be loaded with JavaScript.
- `multiqc/templates` - HTML templates for MultiQC report. Only `multiqc/templates/default` is worth attention here. It includes HTML templates to be combined and rendered with Jinja2 to produce the final HTML report, as well as `assets` - the folder with JavaScript code for all dynamic features like loading and decompressing the plot JSON dumps, rendering it with Plotly-JS, the toolbox with features to highlight/hide/rename samples in the report, etc. It also contains `default_multiqc.css` - all CSS goes there.
- `multiqc/base_module.py` - a base module class for all modules to inherit. Povides a lot of sample name cleaning and grouping logic.
- `multiqc/utils` - common Python utility functions.
- `multiqc/config.py` - a configuration class that contains all the configuration variables for MultiQC, as well as all the configuration discovery logic.
- `multiqc/interactive.py` - function helpers to construct a MultiQC report in interactive mode, e.g. in a Jupyter notebook.
- `multiqc/report.py` - a singleton class with globale variables that are passed to Jinja2 to render a report. Holds the "state" of the report, i.e. modules, sections, discovered files, list of HTML anchor to keep them unique, etc., as well as multiple helper functions.
- `multiqc/multiqc.py` - main entry point of MultiQC. Includes command line interface logic.
- `multiqc/validation.py` - helpers to validate plot configs and user custom content with Pydantic.
- `scripts` - auxiliarry scripts for development.
- `tests` - test suite for MultiQC.
- `docs` - documentation for MultiQC. `docs/markdown/modules` and `docs/markdown/modules.mdx` are autogenerated from the class docstrings in `multiqc/modules` using `scripts/make_module_docs.py`, the rest is written manually.

## Modules

MultiQC supports bioformatics tools through so-called "modules". Each "module" is a Python module placed in `multiqc/modules`, and it is dynamically loaded as an "entry point" specified in `pyproject.toml`. A module describes how MultiQC should parse outputs/logs from the corresponding tool, what data to extract, how to summarise it, and what kinds of plots and tables to use to present data in the report. There is also a file `multiqc/search_patterns.yaml` that describes file name patterns and content patterns to discover output files/logs from each tool/module.

## Guideline for writing module code

When writing new modules, you must follow the following rules:

- Raise `ModuleNoSamplesFound` when no samples are found. DO NOT RAISE `UserWarning`!
- Use f-strings and other MODERN Python 3 syntax. Do not use `__future__` imports or `OrderedDict`'s.
- Call `self.add_software_version()`, even if version is not found, as it's required by linting.
- Call `self.write_data_file(...)` in the very end of the module, after all sections are added. IT IS IMPORTANT TO CALL IT IN THE END!
- Do not add shebang lines to Python files, like `#!/usr/bin/env python`.
- Add entry point into `pyproject.toml`. Ignore `setup.py`.
- Do not add separate markdown files or module-level docstrings. Instead add a docstring to the module class.
- Module's `info` MUST start with a capital letter.
- Use single quotes for strings.

THIS IS VERY IMPORTANT. YOU MUST FOLLOW THESE GUIDELINES. 

### MultiqcModule Class

Example:

```python
from multiqc.base_module import BaseMultiqcModule

class MultiqcModule(BaseMultiqcModule):
    def __init__(self):
        super(MultiqcModule, self).__init__(
          name="My Module",
          anchor="mymodule",
          href="https://www.awesome_bioinfo.com/mymodule",
          info="Example analysis module used for writing documentation.",
          doi=["01.2345/journal/abc123", "01.2345/journal/abc124"],
        )
```

The available arguments when initialising a module as follows:

- `name` - Name of your module
- `anchor` - A HTML-safe anchor that will be used after the `#` in the URL
- `href` - Link(s) to the homepage for the tool
- `info` - Very short description text about the tool. MUST start with a capital letter.
- `doi` - One or more publication DOIs (can be a string ro a list).
- `extra` - Optional additional more detailed description. Will appear in the documentation and in the report, but not on the list of modules on the website.

### Module docstring

Module docstring must contain the following information:

- The list of supported subcommands of a toolkit;
- The list of supported use cases and sets of parameters;
- Versions of the tools that are supported or tested;
- Required outputs file naming and redirection;
- The way the sample name is found in the logs, if not obvious;
- Configuration parameters that the tool can read from the user config;
- Any post-processing needed to be done by the user before running the module;
- Performance considerations;
- Conflicts with other MultiQC modules.

### Search patterns

The first thing that module will need to do is to find analysis log
files. You can do this by searching for a filename fragment, or a string
within the file. It's possible to search for both (a match on either
will return the file) and also to have multiple strings possible.

First, add your default patterns to `multiqc/search_patterns.yaml`

Each search has a yaml key, with one or more search criteria.

The yaml key must begin with the name of your module. If you have multiple
search patterns for a single module, follow the module name with a forward
slash and then any string. For example, see the `fastqc` module search patterns:

```yaml
fastqc/data:
  fn: "fastqc_data.txt"
fastqc/zip:
  fn: "_fastqc.zip"
```

The following search criteria sub-keys can then be used:

- `fn`
  - A glob filename pattern, used with the Python [`fnmatch`](https://docs.python.org/2/library/fnmatch.html) function
- `fn_re`
  - A regex filename pattern
- `contents`
  - A string to match within the file contents (checked line by line)
- `contents_re`
  - A regex to match within the file contents (checked line by line)
  - NB: Regex must match entire line (add `.*` to start and end of pattern to avoid this)
- `exclude_fn`
  - A glob filename pattern which will exclude a file if matched
- `exclude_fn_re`
  - A regex filename pattern which will exclude a file if matched
- `exclude_contents`
  - A string which will exclude the file if matched within the file contents (checked line by line)
- `exclude_contents_re`
  - A regex which will exclude the file if matched within the file contents (checked line by line)
- `num_lines`
  - The number of lines to search through for the `contents`/`contents_re` string. Defaults to 1000 (configurable via `filesearch_lines_limit`). Set or a low number like 10 if it's e.g. a header of a TSV file. Do not set it to 1 (!!!) because there is a chance that other versions of this file can have extra-headers.
- `shared`
  - By default, once a file has been assigned to a module it is not searched again. Specify `shared: true` when your file is likely to be shared between multiple tools, or has a too generic search pattern.
- `max_filesize`
  - Files larger than the `log_filesize_limit` config key (default: 50MB) are skipped. If you know your files will be smaller than this and need to search by contents, you can specify this value (in bytes) to skip any files smaller than this limit.

Please try to use `num_lines` and `max_filesize` where possible as they will speed up
MultiQC execution time.

Please do not set `num_lines` to anything over 1000, as this will significantly slow
down the file search for all users.
If you do need to search more lines to detect a string, please combine it with
a `fn` pattern to limit which files are loaded _(as done with AfterQC)_.

For example, two typical modules could specify search patterns as follows:

```yaml
mymodule:
  fn: "_myprogram.txt"
myother_module:
  contents: "This is myprogram v1.3"
```

You can also supply a list of different patterns for a single log file type if needed.
If any of the patterns are matched, the file will be returned:

```yaml
mymodule:
  - fn: "mylog.txt"
  - fn: "different_fn.out"
```

You can use _AND_ logic by specifying keys within a single list item. For example:

```yaml
mymodule:
  fn: "mylog.txt"
  contents: "mystring"
myother_module:
  - fn: "different_fn.out"
    contents: "This is myprogram v1.3"
  - fn: "another.txt"
    contents: ["What are these files anyway?", "End of program"]
    contents_re: "^Metric: \d+\.\d+"
```

For `mymodule`, a file must have the filename `mylog.txt` _and_ contain the string `mystring`.

`myother_module` will match `different_fn.out` with the contents `This is myprogram v1.3`,
_or_ `another.txt` containing ALL of the lines `What are these files anyway?`, `End of program"`,
and `^Metric: \d+\.\d+`.

You can match subsets of files by using `exclude_` keys as follows:

```yaml
mymodule:
  fn: "*.myprog.txt"
  exclude_fn: "not_these_*"
myother_module:
  fn: "mylog.txt"
  exclude_contents:
    - "trimmed"
    - "sorted"
```

Note that the `exclude_` patterns can have either a single value or a list of values.
They are always considered using OR logic - any matches will reject the file.

Once your strings are added, you can find files in your module with the
base function `self.find_log_files()`, using the key you set in the YAML:

```python
self.find_log_files("fastqc/data")
```

This function yields a dictionary with various information about each matching
file. The `f` key contains the contents of the matching file:

```python
# Find all files for mymod
for f in self.find_log_files("fastqc/data"):
    print(f["f"])  # File contents
    print(f["s_name"])  # Sample name (from cleaned filename)
    print(f["fn"])  # Filename
    print(f["root"])  # Directory file was in
```

If `filehandles=True` is specified, the `f` key contains a file handle
instead:

```python
for f in self.find_log_files("mymodule", filehandles=True):
    # f["f"] is now a filehandle instead of contents
    for line in f["f"]:
        print(line)
```

This is good if the file is large, as Python doesn't read the entire
file into memory in one go.

### Identical sample names

If modules find samples with identical names, then the previous sample
is overwritten. It's good to print a log statement when this happens,
for debugging. However, most of the time it makes sense - programs often
create log files _and_ print to `stdout` for example.

```python
if f["s_name"] in data_by_sample:
    log.debug(f"Duplicate sample name found! Overwriting: {f['s_name']}")
```

### Custom sample names

Typically, sample names are taken from cleaned log filenames (the default
`f['s_name']` value returned). However, if the underlying tool records the sample
name in the logs somewhere, it's better to use that instead. Alternatively, it could
also record the name of the input file somewhere (e.g. adapter cleaning tools typically
save the input FASTQ file name in the log), in which case it's better to clean the
sample name from the input file name. For that, you should use the `self.clean_s_name()`
method, as this will prepend the directory name if requested on the command line:

```python
for f in self.find_log_files("mymodule"):
    input_fname, data = parse_file(f)
    s_name = self.clean_s_name(input_fname, f)
    ...
```
This function has already been applied to the contents of `f['s_name']`,
so it is only required when using something different for the sample identifier.

:::tip
`self.clean_s_name()` **must** be used on sample names parsed from the file
contents. Without it, features such as prepending directories (`--dirs`)
will not work.
:::

The second argument should be the dictionary returned by the `self.find_log_files()` function.
The root path is used for `--dirs` and the search pattern key is used
for fine-grained configuration of the config option `use_filename_as_sample_name`.

If you are using non-standard values for the logfile root, filename or search pattern
key, these can be specified. The function def looks like this:

```python
def clean_s_name(self, s_name, f, root=None):
```

A typical example is when the sample name is the log file directory.
In this case, the root should be the dirname of that directory.
This is non-standard, and would be specified as follows:

```python
s_name = self.clean_s_name(f["root"], f, root=os.path.dirname(f["root"]))
```

### Printing to the sources file

Once you've found your file we want to add this information to the
`multiqc_sources.txt` file in the MultiQC report data directory. This lists
every sample name and the file from which this data came from. This is especially
useful if sample names are being overwritten as it lists the source used. This code
is typically written immediately after the above warning.

If you've used the `self.find_log_files` function, writing to the sources file
is as simple as passing the log file variable to the `self.add_data_source` function:

```python
for f in self.find_log_files("mymodule"):
    self.add_data_source(f)
```

If you have different files for different sections of the module, or are
customising the sample name, you can tweak the fields. The default arguments
are as shown:

```python
self.add_data_source(f=None, s_name=None, source=None, module=None, section=None)
```
### Saving version information

Software version information may be present in the log files of some tools. The
version number can be included in the report by passing it to the method
`self.add_software_version`. Let's use this `samtools stats` log below as an example.

```bash
# This file was produced by samtools stats (1.3+htslib-1.3) and can be plotted using plot-bamstats
# This file contains statistics for all reads.
# The command line was:  stats /home/lp113/bcbio-nextgen/tests/test_automated_output/align/Test1/Test1.sorted.bam
# CHK, Checksum [2]Read Names   [3]Sequences    [4]Qualities
# CHK, CRC32 of reads which passed filtering followed by addition (32bit overflow)
CHK     560674ab        1165a6ca        7b309ac6
# Summary Numbers. Use `grep ^SN | cut -f 2-` to extract this part.
SN      raw total sequences:    101
...
```

The version number here (`1.3`) can be extracted using a regular expression (regex).
We then pass this to the `self.add_software_version()` function.
Note that we pass the sample name (`f["s_name"]` in this case) so that we don't
add versions for samples that are later ignored.

```python
import re

for line in f.splitlines():
    version = re.search(r"# This file was produced by samtools stats \(([\d\.]+)", line)
    if version is not None:
        self.add_software_version(version.group(1), sample=f["s_name"])

    # ..rest of file parsing
```

The version number will now appear after the module header in the report as
well as in the section _Software Versions_ in the end of the report.

:::tip
For tools that don't output software versions in their logs these can instead
be provided in a separate YAML file.
See [Customising Reports](../reports/customisation.md#listing-software-versions) for details.
:::

In some cases, a log may include multiple version numbers for a single tool.
In the example provided, the version of htslib is shown alongside the
previously extracted samtools version. This information is valuable and
should be incorporated into the report. To achieve this, we need to
extract the new version string and provide it to the
`self.add_software_version()` function. Include the relevant software
name (in this case, `htslib`) as well. This will ensure that the htslib
version is listed separately from the main module's software version.
Example:

```python
for line in f.splitlines():
    version = re.search(r"# This file was produced by samtools stats \(([\d\.]+)", line)
    if version is not None:
        self.add_software_version(version.group(1), sample=f["s_name"])

    htslib_version = re.search(r"\+htslib-([\d\.]+)", line)
    if htslib_version is not None:
        self.add_software_version(htslib_version.group(1), sample=f["s_name"], software_name="htslib")

    ...  # rest of file parsing
```

Even if the logs does not contain any version information, you should still
add a superfluous `self.add_software_version()` call to the module. This
will help maintainers to check if new modules or submodules parse any version
information that might exist. The call should also include a note that it is
a dummy call. Example:

```python
for f in self.find_log_files("mymodule/submodule"):
    sample = f["s_name"]
    data_by_sample[sample] = parse_file(f)

    # Superfluous function call to confirm that it is used in this module
    # Replace None with actual version if it is available
    self.add_software_version(None, sample)
```

### Adding to the general statistics table

Now that you have your parsed data, you can start inserting it into the
MultiQC report. At the top of every report is the 'General Statistics'
table. This contains metrics from all modules, allowing cross-module
comparison.

DO NOT ADD A LOT OF COLUMNS TO THE GENERAL STATISTICS TABLE. There should be 1-2 columns visible-by-default columns per module, and - optionally - maximum a dozen more hidden columns.

There is a helper function to add your data to this table. It can take
a lot of configuration options, but most have sensible defaults. At
it's simplest, it works as follows:

```python
data_by_sample: Dict[str, Dict[str, float]] = {
    'sample_1': {
        'first_col': 91.4,
        'second_col': 78.2,
    },
    'sample_2': {
        'first_col': 138.3,
        'second_col': 66.3,
    }
}
self.general_stats_addcols(data_by_sample)
```

To give more informative table headers and configure things like
data scales and colour schemes, you can supply an extra dict:

```python
from multiqc.plots.table_object import ColumnMeta
headers = {
    'first_col': ColumnMeta(
        title='First',
        description='My First Column',
        scale='RdYlGn-rev'
    ),
    'second_col': ColumnMeta(
        title='Second',
        description='My Second Column',
        max=100,
        min=0,
        scale='Blues',
        suffix='%'
    )
}
self.general_stats_addcols(data_by_sample, headers)
```

Here are all options for headers, with defaults:

```python
headers['name'] = TableColumn(
    namespace='',                # Module name. Auto-generated for core modules in General Statistics.
    title='[ dict key ]',        # Short title, table column title
    description='[ dict key ]',  # Longer description, goes in mouse hover text
    max=None,                    # Minimum value in range, for bar / colour coding
    min=None,                    # Maximum value in range, for bar / colour coding
    scale='GnBu',                # Colour scale for colour coding. Set to False to disable.
    suffix=None,                 # Suffix for value (eg. '%')
    format='{:,.1f}',            # Output format() string. Can also be a lambda function.
    shared_key=None,             # See below for description
    modify=None,                 # Lambda function to modify values
    hidden=False,                # Set to True to hide the column on page load
    placement= 1000.0,           # Alter the default ordering of columns in the table
)
```

- `namespace`
  - This prepends the column title in the mouse hover: _Namespace: Title_.
  - The 'Configure Columns' modal displays this under the 'Group' column.
  - It's automatically generated for core modules in the General Statistics table,
    though this can be overwritten (useful for example with custom-content).
- `scale`
  - Colour scales are the names of ColorBrewer palettes. See below for available scales.
  - Add `-rev` to the name of a colour scale to reverse it
  - Set to `False` to disable colouring and background bars
- `shared_key`
  - Any string can be specified here, if other columns are found that share
    the same key, a consistent colour scheme and data scale will be used in
    the table. Typically this is set to things like `read_count`, so that
    the read count in a sample can be seen varying across analysis modules.
- `modify`
  - A python `lambda` function to change the data in some way when it is
    inserted into the table.
- `format`
  - A format string or a python `lambda` function to format the data to display
    on screen.
- `hidden`
  - Setting this to `True` will hide the column when the report loads. It can
    then be shown through the _Configure Columns_ modal in the report. This can
    be useful when data could be sometimes useful. For example, some modules
    show "percentage aligned" on page load but hide "number of reads aligned".
- `placement`
  - If you feel that the results from your module should appear on the left side
    of the table set this value less than 1000. Or to move the column right, set
    it greater than 1000. This value can be any float.

The typical use for the `modify` string is to divide large numbers such as read counts,
to make them easier to interpret. If handling read counts, there are three config variables
that should be used to allow users to change the multiplier for read counts:
`read_count_multiplier`, `read_count_prefix` and `read_count_desc`. For example:

```python
from multiqc.plots.table_object import TableConfig
pconfig = TableConfig(
    title="Reads",
    description=f"Number of reads ({config.read_count_desc})",
    modify=lambda x: x * config.read_count_multiplier,
    suffix=f" {config.read_count_prefix}",
    ...
)
```

Similar config options apply for base pairs: `base_count_multiplier`, `base_count_prefix` and
`base_count_desc`.

And for the read count of long reads: `long_read_count_multiplier`, `long_read_count_prefix` and
`long_read_count_desc`.

Note that adding e.g. `"shared_key": "read_count"` will automatically add corresponding
`description`, `modify`, and `suffix` into the column, so in most cases the following
will be sufficient:

```python
pconfig = TableConfig(
    title="Reads",
    shared_key="read_count",
    ...
)
...
pconfig2 = TableConfig(
    title="Base pairs",
    shared_key="base_count",
    ...
)
```

A third parameter can be passed to this function, `namespace`. This is usually
not needed - MultiQC automatically takes the name of the module that is calling
the function and uses this. However, sometimes it can be useful to overwrite this.

### Color matters

Especially when creating tables, make sure that you think about the colour scheme for every single column.

- Ensure that adjacent columns do not share the same colour scheme.
- Think about what the colours suggest:
  - If a large value is a _bad_ thing (eg. percent duplication), use a strong red colour for large values
  - If values are centred around a point (eg. `0`), use a diverging colour scheme. Values close to the centre will have a weak colour and those at _both_ ends of the distribution will be strongly coloured.

Same ideas apply to bar plots.

Colour scales are taken from [ColorBrewer2](http://colorbrewer2.org/).
Colour scales can be reversed by adding the suffix `-rev` to the name. For example, `RdYlGn-rev`.

The following scales are available:

![color brewer](../../../docs/images/cbrewer_scales.png)

For categorical metrics that can take a value from a predefined set, use one of the categorical color scales: Set2, Accent, Set1, Set3, Dark2, Paired, Pastel2, Pastel1. For numerical metrics, consider one the "sequential" color scales from the table above.

### Writing data to a file

In addition to printing data to the General Stats, MultiQC modules typically
also write to text-files to allow people to easily use the data in downstream
applications. This also gives the opportunity to output additional data that
may not be appropriate for the General Statistics table.

Again, there is a base class function to help you with this - just supply it
with a dictionary and a filename:

```python
data_by_sample = {
    "sample_1": {
        "first_col": 91.4,
        "second_col": "78.2%",
    },
    "sample_2": {
        "first_col": 138.3,
        "second_col": "66.3%",
    },
}
self.write_data_file(data_by_sample, "multiqc_mymodule")
```

Make sure to call `self.write_data_file` in the end of the module, because it
may modify `data_by_sample` to be JSON-serializable.

If your output has a lot of columns, you can supply the additional
argument `sort_cols = True` to have the columns alphabetically sorted.

This function will also pay attention to the default / command line
supplied data format and behave accordingly. So the written file could
be a tab-separated file (default), `JSON` or `YAML`.

Note that any keys with more than 2 levels of nesting will be ignored
when being written to tab-separated files.

### Create report sections

It's time to start creating sections of the report with more information.
To do this, use the `self.add_section()` helper function. This supports the following arguments:

- `name`: Name of the section, used for the title
- `anchor`: The URL anchor - must be unique, used when clicking the name in the side-nav
- `description`: A very short descriptive text to go above the plot (markdown).
- `comment`: A comment to add under the description. Big and blue text, mostly for users to customise the report (markdown).
- `helptext`: Longer help text explaining what users should look for (markdown).
- `plot`: Results from one of the MultiQC plotting functions
- `content`: Any custom HTML

For example:

```python
from multiqc.plots import linegraph, bargraph
from multiqc.plots.linegraph import LinePlotConfig
from multiqc.plots.bargraph import BarPlotConfig

self.add_section(
    name="Second Module Section",
    anchor="mymodule-second",
    plot=linegraph.plot(data_by_sample2, pconfig=LinePlotConfig(
        id="mymodule-second",
        title="My Module: Duplication Rate"
    )),
)
self.add_section(
    name='First Module Section',
    anchor='mymodule-first',
    description='My amazing module output, from the first section',
    helptext="""
        If you're not sure _how_ to interpret the data, we can help!
        Most modules use multi-line strings for these text blocks,
        with triple quotation marks.

        * Markdown
        * Lists
        * Are
        * `Great`
    """,
    plot = bargraph.plot(data_by_sample, pconfig=BarPlotConfig(
        id="mymodule-first",
        title="My Module: Read Counts"
    ))
)
self.add_section(
    content='<p>Some custom HTML.</p>'
)
```

### User configuration

Instead of hard-coding the defaults, it's a great idea to allow users to configure
the behaviour of MultiQC module code.

It's pretty easy to use the built-in MultiQC configuration settings to do this,
so that users can set up their config as described in the
[Configuration docs](../getting_started/config.md).

To do this, just assume that your configuration variables are available in the
MultiQC `config` module and have sensible defaults. For example:

```python
from multiqc import config

mymod_config = getattr(config, 'mymod', {})
my_custom_config_var = mymod_config.get('my_custom_config_var', 5)
```

You now have a variable `my_custom_config_var` with a default value of 5, but that
can be configured by a user as follows:

```yaml
mymod:
  my_custom_config_var: 200
```

Please be sure to use a unique top-level config name to avoid clashes - prefixing
with your module name is a good idea as in the example above. Keep all module config
options under the same top-level name for clarity.

Finally, don't forget to document the usage of your module-specific configuration
in the `MultiqcModule` class docstring, so that people know how to use it.

### Example of a good module for a made-up tool Qualalyser:

#### File system structure:

```
├── multiqc
│   ├── modules
│   |   └── qualalyser
│   │       ├── __init__.py
│   │       ├── qualalyser.py
│   │       └── tests
│   │           ├── __init__.py
│   │           └── test_qualalyser.py
│   └── search_patterns.yaml
└── pyproject.toml
```

#### `__init__.py`

```python
from .qualalyser import MultiqcModule

__all__ = ["MultiqcModule"]
```

#### `qualalyser.py`

```python
import logging
import re
from collections import defaultdict
from copy import deepcopy
from typing import Callable, Dict, List, Any, Tuple, Union

from multiqc.base_module import BaseMultiqcModule, ModuleNoSamplesFound
from multiqc.plots import table, bargraph
from multiqc.plots.plotly.bar import BarPlotConfig
from multiqc.plots.table_object import TableConfig, ColumnMeta
from multiqc.utils import mqc_colour
from multiqc import config

log = logging.getLogger(__name__)


class MultiqcModule(BaseMultiqcModule):
    """
    Qualalyser provides multiple subcommands, and the MultiQC module currently only supports `quality`.

    Qualalyser outputs useful information into stdout, and you need to capture it to
    a file for the module to recognize. To pipe stderr into a file, run the tool
    as follows:

    qualalyser quality 2> sample1.log

    Note the that the sample name is parsed from the filename by default, in this case,
    the reported name will be "sample1".

    #### Configuration

    By default, Qualalyser uses the following quality threshold: 10.

    To override it, use the following config:

    qualalyser:
      min_quality: 10

    Version 1.1.0 of Qualalyser is tested.
    """

    def __init__(self):
        super(MultiqcModule, self).__init__(
            name="Qualalyser",
            anchor="qualalyser",
            href="https://github.com/bioinformatics-centre/qualalyser/",
            info="Reports read quality and length from sequencing data",
            doi="10.21105/joss.02991",
        )

        # Find and load any Qualalyser reports
        data_by_sample: Dict[str, Dict[str, float]] = {}
        for f in self.find_log_files("qualalyser/quality", filehandles=True):
            sample_data = parse_qualalyser_log(f)
            if sample_data:
                s_name = f['s_name']
                if s_name in data_by_sample:
                    log.debug(f"Duplicate sample name found! Overwriting: {s_name}")
                data_by_sample[s_name] = sample_data
                self.add_data_source(f)

        # Superfluous function call to confirm that it is used in this module
        # Replace None with actual version if it is available
        self.add_software_version(None)

        # Filter to strip out ignored sample names
        data_by_sample = self.ignore_samples(data_by_sample)
        if len(data_by_sample) == 0:
            raise ModuleNoSamplesFound
        log.info(f"Found {len(data_by_sample)} reports")

        # Add Qualalyser summary to the general stats table
        self.add_table(data_by_sample)

        # Quality distribution Plot
        self.reads_by_quality_plot(data_by_sample)

        # Read length distribution Plot
        self.reads_by_length_plot(data_by_sample)

        # Write parsed report data to a file
        self.write_data_file(data_by_sample, "multiqc_qualalyser")

    def add_table(self, data_by_sample: Dict[str, Dict[str, float]]) -> None:
        headers: Dict[str, Dict] = {
            "Number of reads": ColumnMeta(
                title="Reads",
                description="Number of reads",
                scale="Greens",
                shared_key="read_count",
            ),
            "Number of bases": ColumnMeta(
                title="Bases",
                description="Total bases sequenced",
                scale="Purples",
                shared_key="base_count",
            ),
            "N50 read length": ColumnMeta(
                title="Read N50",
                description="N50 read length",
                scale="Blues",
                suffix="bp",
                format="{:,.0f}",
            ),
            "Longest read": ColumnMeta(
                title="Longest Read",
                description="Longest read length",
                suffix="bp",
                scale="Oranges",
                format="{:,.0f}",
            ),
            "Mean read length": ColumnMeta(
                title="Mean Length",
                description="Mean read length",
                suffix="bp",
                scale="PuBuGn",
            ),
            "Median read length": ColumnMeta(
                title="Median Length",
                description="Median read length (bp)",
                scale="RdYlBu",
                format="{:,.0f}",
            ),
            "Mean read quality": ColumnMeta(
                title="Mean Qual",
                description="Mean read quality (Phred scale)",
                scale="PiYG",
            ),
            "Median read quality": ColumnMeta(
                title="Median Qual",
                description="Median read quality (Phred scale)",
                scale="Spectral",
            ),
        }

        self.add_section(
            name="Qualalyser Summary",
            anchor="qualalyser-summary",
            description="Statistics from Qualalyser reports",
            plot=table.plot(
                data_by_sample,
                headers,
                pconfig=TableConfig(
                    id="qualalyser_table",
                    title="Qualalyser Summary",
                ),
            ),
        )

        # Add general stats table - hide all columns except for two
        general_stats_headers = deepcopy(headers)
        for h in general_stats_headers.values():
            h["hidden"] = True
        general_stats_headers["Number of reads"]["hidden"] = False
        general_stats_headers["N50 read length"]["hidden"] = False

        # Add columns to the general stats table
        self.general_stats_addcols(data_by_sample, general_stats_headers)

    def reads_by_quality_plot(self, data_by_sample: Dict[str, Dict[str, float]]) -> None:
        barplot_data: Dict[str, Dict[str, float]] = defaultdict(dict)
        keys: List[str] = []
        min_quality = getattr(config, "qualalyser", {}).get("min_quality", 10)

        for name, d in data_by_sample.items():
            reads_by_q = {int(re.search(r"\d+", k).group(0)): v for k, v in d.items() if k.startswith("Reads > Q")}
            if not reads_by_q:
                continue

            thresholds = sorted(th for th in reads_by_q if th >= min_quality)
            if not thresholds:
                continue

            barplot_data[name], keys = get_ranges_from_cumsum(
                data=reads_by_q, thresholds=thresholds, total=d["Number of reads"], formatter=lambda x: f"Q{x}"
            )

        colours = mqc_colour.mqc_colour_scale("RdYlGn-rev", 0, len(keys))
        cats = {
            k: {"name": f"Reads {k}", "color": colours.get_colour(idx, lighten=1)} for idx, k in enumerate(keys[::-1])
        }

        # Plot
        self.add_section(
            name="Read quality",
            anchor="qualalyser_plot_quality",
            description="Read counts categorised by read quality (Phred score).",
            helptext="""
            Sequencing machines assign each generated read a quality score using the
            [Phred scale](https://en.wikipedia.org/wiki/Phred_quality_score).
            The phred score represents the liklelyhood that a given read contains errors.
            High quality reads have a high score.
            """,
            plot=bargraph.plot(
                barplot_data,
                cats,
                pconfig=bargraph.BarPlotConfig(
                    id="qualalyser_plot_quality_plot",
                    title="Qualalyser: read qualities",
                ),
            ),
        )


def parse_qualalyser_log(f) -> Dict[str, float]:
    """Parse output from Qualalyser"""
    stats: Dict[str, float] = dict()

    # Parse the file content
    segment = None
    summary_lines = []
    length_threshold_lines = []
    quality_threshold_lines = []

    for line in f["f"]:
        line = line.strip()
        if line.startswith("Qualalyser Read Summary"):
            segment = "summary"
            continue
        elif line.startswith("Read length thresholds"):
            segment = "length_thresholds"
            continue
        elif line.startswith("Read quality thresholds"):
            segment = "quality_thresholds"
            continue

        if segment == "summary":
            summary_lines.append(line)
        elif segment == "length_thresholds":
            length_threshold_lines.append(line)
        elif segment == "quality_thresholds":
            quality_threshold_lines.append(line)

    for line in summary_lines:
        if ":" in line:
            metric, value = line.split(":", 1)
            stats[metric.strip()] = float(value.strip())

    return stats
```

#### `pyproject.toml`

```toml
...
[project.entry-points."multiqc.modules.v1"]
qualalyser = "multiqc.modules.qualalyser:MultiqcModule"
...
```

#### `search_patterns.yaml`

```yaml
...
qualalyser/quality:
  fn: "*.log"
  contents: "Qualalyser Read Summary"
  num_lines: 10
...
```

#### `config_defaults.yaml`

```yaml
...
# Order that modules should appear in report. Try to list in order of analysis.
module_order:
  ...
  - qualalyser
  ...
...
```
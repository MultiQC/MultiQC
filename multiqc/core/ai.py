from ast import arg
import base64
import json
import logging
import os
from pathlib import Path
import re
from textwrap import indent
from typing import Dict, Optional, cast, TYPE_CHECKING
from markdown import markdown
from pydantic import BaseModel, Field
import requests

from pydantic.types import SecretStr

from multiqc import config, report
from multiqc.core.log_and_rich import run_with_spinner
from multiqc.types import Anchor, PlotType

if TYPE_CHECKING:
    from langchain_core.language_models.chat_models import BaseChatModel  # type: ignore


logger = logging.getLogger(__name__)


_MULTIQC_DESCRIPTION = """\
You are an expert in bioinformatics, sequencing technologies, genomics data analysis, and adjacent fields.

You are given findings from a MultiQC report, generated by a bioinformatics workflow.
MultiQC supports various bioinformatics tools that output QC metrics, and aggregates those metrics
into a single report. It outputs a "General Statistics" table with key metrics for each sample across 
all tools. That table is followed by more detailed sections from specific tools, that can include tables,
as well as plots of different types (bar plot, line plot, scatter plot, heatmap, etc.)
"""

_PROMPT_BASE = (
    _MULTIQC_DESCRIPTION
    + """
You are given data from such a report. Your task is to analyse the data, and
give 1-2 bullet points of a very short and concise overall summary for the results. 
Don't waste words: mention only the important QC issues. If there are no issues, just say so. 
Just print one or two bullet points, nothing else. 
Please do not add any extra headers to the response.

Use markdown to format your reponse for readability. Use directives with pre-defined classes
.text-green, .text-red, and .text-yellow to highlight severity, e.g. :span[39.2%]{.text-red}. 
Highlight any mentioned sample names or sample named prefixes or suffixes with a sample directive, 
and make sure to use the same color classes for severity, e.g. :sample[A1001.2003]{.text-yellow}
or :sample[A1001]{.text-yellow}. Do not put multiple sample names inside one directive.

You must use only multiples of 4 spaces to indent nested lists.
"""
)

_SHORT_EXAMPLE = """
Two examples of short summaries:

- :span[11/13 samples]{.text-green} show consistent metrics within expected ranges.
- :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} exhibit extremely high percentage of :span[duplicates]{.text-red} (:span[65.54%]{.text-red} and :span[83.14%]{.text-red}, respectively).

- All samples show good quality metrics with :span[75.7-77.0%]{.text-green} CpG methylation and :span[76.3-86.0%]{.text-green} alignment rates
- :sample[2wk]{.text-yellow} samples show slightly higher duplication (:span[11-15%]{.text-yellow}) compared to :sample[1wk]{.text-green} samples (:span[6-9%]{.text-green})'
"""

_PROMPT_FULL_DETAILED_SUMMARY_INSTRUCTIONS = """
Next, generate a more detailed summary of the results, and your recommendations for the next steps.

Make sure to use a multiple of 4 spaces to indent nested lists.

Highlight severity of the values and the sample names with directives and the pre-defined classes, similar to the short summary, 
as in the example below.
"""

_EXAMPLE_SUMMARY_FOR_FULL = """\
- :span[11/13 samples]{.text-green} show consistent metrics within expected ranges.
- :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} exhibit extremely high percentage of :span[duplicates]{.text-red} (:span[65.54%]{.text-red} and :span[83.14%]{.text-red}, respectively).
"""

_PROMPT_EXAMPLE_SUMMARY_FOR_FULL = f"""\
Example, formatted as YAML of 3 sections (summary, detailed_summary, and recommendations):

summary: |
{indent(_EXAMPLE_SUMMARY_FOR_FULL, "    ")}
"""

_EXAMPLE_DETAILED_SUMMARY = """\
**Analysis**

- :sample[A1002]{.text-yellow} and :sample[A1003]{.text-yellow} groups (:span[11/13 samples]{.text-green}) show good quality metrics, with consistent GC content (38-39%), read lengths (125 bp), and acceptable levels of duplicates and valid pairs.
- :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} show severe quality issues:
    - Extremely high duplicate rates (:span[65.54%]{.text-red} and :span[83.14%]{.text-red})
    - Low percentages of valid pairs (:span[37.2%]{.text-red} and :span[39.2%]{.text-red})
    - High percentages of failed modules in FastQC (:span[33.33%]{.text-red})
    - Significantly higher total sequence counts (:span[141.9M]{.text-red} and :span[178.0M]{.text-red}) compared to other samples
    - FastQC results indicate that :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} have a slight :span[GC content]{.text-red} bias at 39.5% against most other samples having 38.0%, which indicates a potential contamination that could be the source of other anomalies in quality metrics.

- :sample[A1002-1007]{.text-yellow} shows some quality concerns:
    - Low percentage of valid pairs (:span[48.08%]{.text-yellow})
    - Low percentage of passed Di-Tags (:span[22.51%]{.text-yellow})

- Overrepresented sequences analysis reveals adapter contamination in several samples, particularly in :sample[A1001.2003]{.text-yellow} (up to :span[35.82%]{.text-yellow} in Read 1).
- HiCUP analysis shows that most samples have acceptable levels of valid pairs, with :sample[A1003]{.text-green} group generally performing better than :sample[A1002]{.text-yellow} group.

**Recommendations**

- Remove :sample[A1001.2003]{.text-red} and :sample[A1200.2004]{.text-red} from further analysis due to severe quality issues.
- Investigate the cause of low valid pairs and passed Di-Tags in :sample[A1002-1007]{.text-yellow}. Consider removing it if the issue cannot be resolved.
- Perform adapter trimming on all samples, particularly focusing on :sample[A1001]{.text-red} group.
- Re-run the Hi-C analysis pipeline after removing problematic samples and performing adapter trimming.
- Investigate the cause of higher duplication rates in :sample[A1002]{.text-yellow} group compared to :sample[A1003]{.text-green} group, although they are still within acceptable ranges.
- Consider adjusting the Hi-C protocol or library preparation steps to improve the percentage of valid pairs, especially for :sample[A1002]{.text-yellow} group.
"""

_PROMPT_EXAMPLE_DETAILED_SUMMARY = f"""\
detailed_summary: |
{indent(_EXAMPLE_DETAILED_SUMMARY, "    ")}
"""

PROMPT_SHORT = f"""\
{_PROMPT_BASE}

{_SHORT_EXAMPLE}
"""

PROMPT_FULL = f"""\
{_PROMPT_BASE}

{_PROMPT_FULL_DETAILED_SUMMARY_INSTRUCTIONS}

{_PROMPT_EXAMPLE_SUMMARY_FOR_FULL}

{_PROMPT_EXAMPLE_DETAILED_SUMMARY}
"""


class InterpretationOutput(BaseModel):
    summary: str = Field(description="A very short and concise overall summary")
    detailed_analysis: Optional[str] = Field(default=None, description="Detailed analysis")

    def markdown_to_html(self, text: str) -> str:
        """
        Convert markdown to HTML
        """
        html = markdown(text)
        # Find and replace directives :span[1.23%]{.text-red} -> <span>..., handle multiple matches in one string
        html = re.sub(
            r":span\[([^\]]+?)\]\{\.text-(green|red|yellow)\}",
            r"<span class='text-\2'>\1</span>",
            html,
        )
        # similarly, find and replace directives:sample[A1001.2003]{.text-red} -> <sample>...
        html = re.sub(
            r":sample\[([^\]]+?)\]\{\.text-(green|red|yellow)\}",
            r"<sample data-toggle='tooltip' title='Click to highlight in the report' class='text-\2'>\1</sample>",
            html,
        )
        return html

    def format_text(self) -> str:
        """
        Format to markdown to display in Seqera AI
        """
        return f"## Analysis\n{self.summary}" + (f"\n\n{self.detailed_analysis}" if self.detailed_analysis else "")


class InterpretationResponse(BaseModel):
    interpretation: InterpretationOutput
    model: str
    uuid: Optional[str] = None


class Client:
    def __init__(self, model: str, api_key: Optional[str]):
        self.title: str
        self.model: str = model
        self.api_key: Optional[str] = api_key

    def interpret_report_short(self, report_content: str) -> Optional[InterpretationResponse]:
        raise NotImplementedError

    def interpret_report_full(self, report_content: str) -> Optional[InterpretationResponse]:
        raise NotImplementedError

    def max_tokens(self) -> int:
        raise NotImplementedError

    def n_tokens(self, text: str) -> int:
        """
        Estimate tokens for Seqera's API
        Using tiktoken for GPT models, falling back to Claude's tokenizer for others
        """
        try:
            if self.model.startswith("gpt"):
                import tiktoken  # type: ignore

                encoding = tiktoken.encoding_for_model(self.model)
                return len(encoding.encode(text))
            else:
                from anthropic import Anthropic  # type: ignore

                return Anthropic().count_tokens(text)
        except ImportError:
            logger.warning(
                "Fallback to rough estimation if tokenizers not available. Install `tiktoken` or `anthropic` to get more accurate token counts."
            )
            return len(text) // 4


class LangchainClient(Client):
    def __init__(self, model: str, api_key: Optional[str]):
        super().__init__(model, api_key)
        self.llm: BaseChatModel

    def interpret_report_short(self, report_content: str) -> Optional[InterpretationResponse]:
        if config.development and os.environ.get("MQC_STUB_AI_RESPONSE"):
            return InterpretationResponse(
                interpretation=InterpretationOutput(
                    summary="- All samples show :span[good quality metrics]{.text-green} with consistent CpG methylation (:span[75.7-77.0%]{.text-green}), alignment rates (:span[76-86%]{.text-green}), and balanced strand distribution (:span[~50/50]{.text-green})\n- :sample[2wk]{.text-yellow} samples show slightly higher duplication (:span[11-15%]{.text-yellow}) and trimming rates (:span[13-23%]{.text-yellow}) compared to :sample[1wk]{.text-green} samples (:span[6-9%]{.text-green} duplication, :span[2-3%]{.text-green} trimming)",
                ),
                model="test-model",
                uuid=None,
            )

        from langsmith import Client as LangSmithClient  # type: ignore
        from langchain_core.tracers.context import tracing_v2_enabled  # type: ignore

        llm = self.llm

        def send_request():
            if config.langchain_api_key and config.langchain_endpoint and config.langchain_project:
                with tracing_v2_enabled(
                    project_name=config.langchain_project,
                    client=LangSmithClient(
                        api_key=config.langchain_api_key,
                        api_url=config.langchain_endpoint,
                    ),
                ):
                    response = llm.invoke(
                        [
                            {"role": "system", "content": PROMPT_SHORT},
                            {"role": "user", "content": report_content},
                        ],
                    )
            else:
                response = llm.invoke(
                    [
                        {"role": "system", "content": PROMPT_SHORT},
                        {"role": "user", "content": report_content},
                    ],
                )
            return response

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Interpreting MultiQC report...", send_request)

        if not response:
            msg = f"Got empty response from the LLM {self.title} {self.model}"
            if config.strict:
                raise RuntimeError(msg)
            logger.error(msg)
            return None

        resolved_model_name = self.model
        if "model" in response.response_metadata:
            resolved_model_name = response.response_metadata["model"]
        elif "model_name" in response.response_metadata:
            resolved_model_name = response.response_metadata["model_name"]

        return InterpretationResponse(
            interpretation=InterpretationOutput(
                summary=cast(str, response.content),
            ),
            model=resolved_model_name,
        )

    def interpret_report_full(self, report_content: str) -> Optional[InterpretationResponse]:
        if config.development and os.environ.get("MQC_STUB_AI_RESPONSE"):
            return InterpretationResponse(
                interpretation=InterpretationOutput(
                    summary=_EXAMPLE_SUMMARY_FOR_FULL,
                    detailed_analysis=_EXAMPLE_DETAILED_SUMMARY,
                ),
                model="test-model",
                uuid=None,
            )

        from langsmith import Client as LangSmithClient  # type: ignore
        from langchain_core.tracers.context import tracing_v2_enabled  # type: ignore

        llm = self.llm.with_structured_output(InterpretationOutput, include_raw=True)

        def send_request():
            if config.langchain_api_key and config.langchain_endpoint and config.langchain_project:
                with tracing_v2_enabled(
                    project_name=config.langchain_project,
                    client=LangSmithClient(
                        api_key=config.langchain_api_key,
                        api_url=config.langchain_endpoint,
                    ),
                ):
                    response = llm.invoke(
                        [
                            {"role": "system", "content": PROMPT_FULL},
                            {"role": "user", "content": report_content},
                        ],
                    )
            else:
                response = llm.invoke(
                    [
                        {"role": "system", "content": PROMPT_FULL},
                        {"role": "user", "content": report_content},
                    ],
                )
            return response

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Interpreting MultiQC report...", send_request)

        response = cast(Dict, response)
        if not response["parsed"]:
            if response["raw"]:
                msg = f"Failed to parse the response from the LLM: {response['raw']}"
                if config.strict:
                    raise RuntimeError(msg)
                logger.error(msg)
                return None
            else:
                msg = f"Got empty response from the LLM {self.title} {self.model}"
                if config.strict:
                    raise RuntimeError(msg)
                logger.error(msg)
                return None

        resolved_model_name = self.model
        if "model" in response["raw"].response_metadata:
            resolved_model_name = response["raw"].response_metadata["model"]
        elif "model_name" in response["raw"].response_metadata:
            resolved_model_name = response["raw"].response_metadata["model_name"]

        return InterpretationResponse(
            interpretation=response["parsed"],
            model=resolved_model_name,
        )


class OpenAiClient(LangchainClient):
    def __init__(self, model: str, api_key: str):
        from langchain_openai import ChatOpenAI  # type: ignore

        openai_logger = logging.getLogger("openai._base_client")
        openai_logger.addFilter(TruncateOpenAiLogFilter())

        super().__init__(model, api_key)
        self.title = "OpenAI"
        self.llm = ChatOpenAI(
            model=self.model,
            api_key=SecretStr(api_key),
            temperature=0.0,
        )

    def max_tokens(self) -> int:
        return 128000


# Truncate the messages from the anthropic logger. It would print the entire user message which is too long.
class TruncateAnthropicLogFilter(logging.Filter):
    def filter(self, record):
        try:
            messages = record.args["json_data"]["messages"]  # type: ignore
            messages[0]["content"] = messages[0]["content"][:1000] + "<truncated>"
            record.args["json_data"]["system"] = record.args["json_data"]["system"][:500] + "<truncated>"  # type: ignore
        except Exception:
            pass
        return True


class TruncateOpenAiLogFilter(logging.Filter):
    def filter(self, record):
        try:
            for m in record.args["json_data"]["messages"]:  # type: ignore
                m["content"] = m["content"][:1000] + "<truncated>"  # type: ignore
        except Exception:
            pass
        return True


class AnthropicClient(LangchainClient):
    def __init__(self, model: str, api_key: str):
        from langchain_anthropic import ChatAnthropic  # type: ignore

        # Get the anthropic logger and add the filter
        anthropic_logger = logging.getLogger("anthropic._base_client")
        anthropic_logger.addFilter(TruncateAnthropicLogFilter())

        super().__init__(model, api_key)
        self.title = "Anthropic"
        self.llm = ChatAnthropic(
            model=self.model,  # type: ignore
            api_key=SecretStr(api_key),
            temperature=0.0,
        )  # type: ignore

    def max_tokens(self) -> int:
        return 200000


class SeqeraClient(Client):
    def __init__(self, model: str, api_key: Optional[str]):
        super().__init__(model, api_key)
        self.title = "Seqera AI"

    def max_tokens(self) -> int:
        if self.model.startswith("gpt"):
            return 128000
        return 200000

    def interpret_report_short(self, report_content: str) -> Optional[InterpretationResponse]:
        def send_request() -> requests.Response:
            return requests.post(
                f"{config.seqera_api_url}/invoke-with-token" if self.api_key else f"{config.seqera_api_url}/invoke",
                headers={"Authorization": f"Bearer {self.api_key}"} if self.api_key else {},
                json={
                    "system_message": PROMPT_SHORT,
                    "user_message": report_content,
                    "model": self.model,
                    "tags": ["multiqc", f"multiqc_version:{config.version}"],
                },
            )

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Summarizing report with AI...", send_request)

        if response.status_code != 200:
            msg = f"Failed to get a response from Seqera. Status code: {response.status_code} ({response.reason})"
            logger.error(msg)
            logger.debug(f"Response: {response.text}")
            if config.strict:
                raise RuntimeError(msg)
            return None

        response_dict = response.json()
        uuid = response_dict.get("uuid")
        generation = response_dict.get("generation")
        return InterpretationResponse(
            uuid=uuid,
            interpretation=InterpretationOutput(summary=generation),
            model=response_dict["raw"].response_metadata["model"],
        )

    def interpret_report_full(self, report_content: str) -> Optional[InterpretationResponse]:
        def send_request() -> requests.Response:
            return requests.post(
                f"{config.seqera_api_url}/invoke-with-token" if self.api_key else f"{config.seqera_api_url}/invoke",
                headers={"Authorization": f"Bearer {self.api_key}"} if self.api_key else {},
                json={
                    "system_message": PROMPT_FULL,
                    "user_message": report_content,
                    "model": self.model,
                    "tags": ["multiqc", f"multiqc_version:{config.version}"],
                    "response_schema": {
                        "name": "Interpretation",
                        "description": "Interpretation of a MultiQC report",
                        "input_schema": {
                            "type": "object",
                            "required": ["summary"],
                            "properties": {
                                key: {
                                    "type": "string",
                                    "description": value.description,
                                    **({"default": value.default} if value.default is None else {}),
                                }
                                for key, value in InterpretationOutput.model_fields.items()
                            },
                        },
                    },
                },
            )

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Summarizing report with AI...", send_request)

        if response.status_code != 200:
            msg = f"Failed to get a response from Seqera: {response.status_code} {response.text}"
            logger.error(msg)
            if config.strict:
                raise RuntimeError(msg)
            return None

        response_dict = response.json()
        uuid = response_dict.get("uuid")
        generation: Dict[str, str] = response_dict.get("generation")
        return InterpretationResponse(
            uuid=uuid,
            interpretation=InterpretationOutput(**generation),
            model=response_dict["raw"].response_metadata["model"],
        )


def get_llm_client() -> Optional[Client]:
    if not config.ai_summary:
        return None

    if config.ai_provider == "seqera":
        api_key = config.seqera_api_key or os.environ.get("SEQERA_API_KEY", os.environ.get("TOWER_ACCESS_TOKEN"))
        if not api_key:
            logger.warning(
                "config.ai_summary is set to true, and config.ai_provider is set to 'seqera', but TOWER_ACCESS_TOKEN "
                "is not set. Please set the TOWER_ACCESS_TOKEN environment variable or change config.ai_provider"
            )
            return None
        return SeqeraClient(config.ai_model, api_key)

    if config.ai_provider == "anthropic":
        api_key = config.anthropic_api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'anthropic', but Anthropic API "
                "key not set. Please set the ANTHROPIC_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        model = config.ai_model if config.ai_model.startswith("claude") else "claude-3-5-sonnet-20240620"
        try:
            return AnthropicClient(model, api_key)
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "AI summary requested through `config.ai_summary`, but required dependencies are not installed. Install them with `pip install multiqc[anthropic]`"
            )

    if config.ai_provider == "openai":
        api_key = config.openai_api_key or os.environ.get("OPENAI_API_KEY")
        if not api_key:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'openai', but OpenAI API "
                "key not set. Please set the OPENAI_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        model = config.ai_model if config.ai_model.startswith("gpt") else "gpt-4o"
        try:
            return OpenAiClient(model, api_key)
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "AI summary requested through `config.ai_summary`, but required dependencies are not installed. Install them with `pip install multiqc[openai]`"
            )

    return None


def _strip_html(text: str) -> str:
    return text.replace("<p>", "").replace("</p>", "")


class AiToolMetadata(BaseModel):
    name: str
    info: Optional[str] = None
    href: Optional[str] = None
    comment: Optional[str] = None


class AiSectionMetadata(BaseModel):
    name: str
    module_anchor: Anchor
    description: Optional[str] = None
    comment: Optional[str] = None
    helptext: Optional[str] = None
    plot_anchor: Optional[Anchor] = None
    content: Optional[str] = None
    content_before_plot: Optional[str] = None


class AiReportMetadata(BaseModel):
    tools: Dict[Anchor, AiToolMetadata]
    sections: Dict[Anchor, AiSectionMetadata]


def ai_section_metadata() -> AiReportMetadata:
    return AiReportMetadata(
        tools={
            mod.anchor: AiToolMetadata(
                name=mod.name,
                info=mod.info,
                href=", ".join(mod.href),
                comment=mod.comment,
            )
            for mod in report.modules
            if not mod.hidden and mod.name != "Software Versions"
        },
        sections={
            section.anchor: AiSectionMetadata(
                name=section.name,
                description=section.description,
                comment=section.comment,
                helptext=section.helptext,
                plot_anchor=section.plot_anchor,
                module_anchor=section.module_anchor,
                content=section.content,
                content_before_plot=section.content_before_plot,
            )
            for mod in report.modules
            for section in mod.sections
            if not mod.hidden and mod.name != "Software Versions"
        },
    )


def build_prompt(client: Client, metadata: AiReportMetadata) -> Optional[str]:
    system_prompt = PROMPT_FULL if config.ai_summary_full else PROMPT_SHORT

    # Account for system message, plus leave 5% buffer
    max_tokens = client.max_tokens() * 0.95 - client.n_tokens(system_prompt)

    prompt_parts: list[str] = []
    current_n_tokens = 0

    # Details about modules used in the report - include first in the prompt
    tools_context: str = ""
    tools_context += "Tools used in the report:\n\n"
    for i, tool in enumerate(metadata.tools.values()):
        tools_context += f"{i+1}. {tool.name} ({tool.info})\n"
        if tool.info:
            tools_context += f"Description: {tool.info}\n"
        if tool.href:
            tools_context += f"Links: {tool.href}\n"
        if tool.comment:
            tools_context += f"Comment: {tool.comment}\n"
        tools_context += "\n\n"
        tools_context += "\n----------------------\n\n"
    prompt_parts.append(tools_context)

    # General stats - also always include, otherwise we don't have anything to summarize
    if report.general_stats_plot:
        prompt_parts.append(f"""
MultiQC General Statistics (overview of key QC metrics for each sample, across all tools)
{report.general_stats_plot.format_for_ai_prompt()}
""")

    # Now calculate the length of the prompt, and if it's too long already, we truncate
    current_n_tokens = sum(client.n_tokens(part) for part in prompt_parts)
    if current_n_tokens > max_tokens:  # Leave 5% buffer
        logger.warning(
            f"General stats exceed the {client.title}'s context window ({client.max_tokens()} tokens). AI summary will not be generated."
        )
        return None

    for section in metadata.sections.values():
        tool = metadata.tools[section.module_anchor]
        section_context = "\n----------------------\n\n"
        section_context += f"Tool: {tool.name}\n"
        section_context += f"Section: {section.name}\n"
        if section.description:
            section_context += f"Section description: {_strip_html(section.description)}\n"
        if section.comment:
            section_context += f"Section comment: {_strip_html(section.comment)}\n"
        if section.helptext:
            section_context += f"Section help text: {_strip_html(section.helptext)}\n"

        if section.content_before_plot:
            section_context += section.content_before_plot + "\n\n"
        if section.content:
            section_context += section.content + "\n\n"

        if section.plot_anchor and section.plot_anchor in report.plot_by_id:
            plot = report.plot_by_id[section.plot_anchor]
            if plot_content := plot.format_for_ai_prompt():
                if plot.pconfig.title:
                    section_context += f"Title: {plot.pconfig.title}\n"
                section_context += "\n" + plot_content

        # Check if adding this section would exceed the limit
        # Using rough estimate of 4 chars per token
        section_n_tokens = client.n_tokens(section_context)
        if current_n_tokens + section_n_tokens > max_tokens:  # Leave 5% buffer
            logger.warning(
                f"Truncating report content to fit within {client.title}'s context window ({client.max_tokens()} tokens). "
                f"Used context length is {current_n_tokens} tokens, adding section {section.name} will sum up to {current_n_tokens + section_n_tokens} tokens."
            )
            break

        prompt_parts.append(section_context)
        current_n_tokens += section_n_tokens

    prompt = "".join(prompt_parts)
    prompt = re.sub(r"\n\n\n", "\n\n", prompt)  # strip triple newlines
    return prompt


def add_ai_summary_to_report(data_dir: Optional[Path] = None):
    metadata: AiReportMetadata = ai_section_metadata()
    # Set data for JS runtime
    report.ai_report_metadata_base64 = base64.b64encode(metadata.model_dump_json().encode()).decode()

    if not config.ai_summary:
        # Not generating report in Python, leaving for JS runtime
        return

    if not (client := get_llm_client()):
        return

    report.ai_provider_title = client.title
    report.ai_model = client.model

    prompt = build_prompt(client, metadata)
    if not prompt:
        return

    # Save content to file for debugging
    if data_dir:
        path = Path(data_dir) / "multiqc_ai_prompt.txt"
        path.write_text(prompt)
        logger.debug(f"Saving AI prompt to {path}")

    response: Optional[InterpretationResponse]
    if config.ai_summary_full:
        response = client.interpret_report_full(prompt)
    else:
        response = client.interpret_report_short(prompt)
    if not response:
        return None

    logger.debug(f"Interpretation: {response.interpretation}")
    if not response.interpretation:
        return None

    if response.uuid:
        report.ai_generation_id = response.uuid

    interpretation: InterpretationOutput = response.interpretation
    report.ai_global_summary = interpretation.markdown_to_html(interpretation.summary)

    if config.ai_summary_full and interpretation.detailed_analysis:
        report.ai_global_detailed_analysis = interpretation.markdown_to_html(interpretation.detailed_analysis)

    #     report.ai_global_summary = f"""
    #     <details>
    #     <summary>
    #     <div class="ai-summary-header">
    #         <b>Report AI Summary {seqera_ai_beta_icon if client.title == 'Seqera AI' else ''}</b>
    #         {continue_chat_btn}
    #     </div>
    #     {interpretation.markdown_to_html(interpretation.summary)}
    #     </summary>
    #     {detailed_summary}
    #     {recommendations}
    #     <p class="ai-summary-disclaimer" id="global_ai_summary_disclaimer">{disclaimer}</p>
    #     </details>
    #     <div class="mqc-table-expand ai-summary-expand ai-summary-expand-closed">
    #         <span class="glyphicon glyphicon-chevron-down" aria-hidden="true"></span>
    #     </div>
    #     """
    #
    # report.ai_summary = f"""
    # <div class="ai-short-summary">
    #     <div class="ai-summary-header">
    #         <b>Report AI Summary {seqera_ai_beta_icon if client.title == 'Seqera AI' else ''}</b>
    #     </div>
    #     {interpretation.markdown_to_html(interpretation.summary)}
    # </div>
    # <div class="ai-local-content" id="global_ai_summary" style="display: none; margin: 0px -12px 0px -10px; padding: 10px;">
    #     <p style="display: flex; justify-content: space-between; align-items: center;">
    #         <b>Analysis</b>
    #     </p>
    #     <div class="ai-detailed-summary" id="global_ai_detailed_summary"></div>
    #     <div class="ai-summary-error" id="global_ai_summary_error"></div>
    # </div>
    # <div style="display: flex; justify-content: space-between; align-items: center; margin-top: 8px; margin-bottom: 8px;">
    #     <button
    #         class="btn btn-sm btn-default ai-generate-more ai-generate-more-global"
    #     >
    #         Generate more details...
    #     </button>
    #     <span class="ai-summary-disclaimer" id="global_ai_summary_disclaimer" style="margin-top: 2px;">{disclaimer}</span>
    # </div>
    # """

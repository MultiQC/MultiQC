import logging
import os
from abc import abstractmethod
from textwrap import dedent
from typing import Dict, List, Optional
import requests

from pydantic.types import SecretStr
from langchain_core.language_models.base import BaseLanguageModel
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tracers.context import tracing_v2_enabled
from langsmith import Client as LangSmithClient


from multiqc import config, report

from dotenv import load_dotenv  # type: ignore

load_dotenv()

logger = logging.getLogger(__name__)


# class SectionSummary(BaseModel):
#     id: str
#     summary: str


# class SummaryResponse(BaseModel):
#     section_summaries: List[SectionSummary]


prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """\
You are an expert in bioinformatics, sequencing technologies, and genomics data analysis. 
You are given key findings from a MultiQC report, generated by a bioinformatics workflow.
MultiQC consists of so called modules that support different tools that output QC metrics 
(e.g. bclconvert, FastQC, samtools stats, bcftools stats, fastp, Picard, SnpEff, etc), and
it aggregates results from different tools. It outputs a "General Statistics" section that
has a table with a summary of key metrics from all modules across each sample. That table
is followed by module-specific sections that usually have more detail on the same samples.

You are given data from such a report, split by section. Your task is to analyse the data, and 
give a short and concise overall summary for the results. Don't waste words: mention only
critical QC issues, and only for the samples that have those. Do not list every section, but only
mention the sections that worth attention.

Use limited HTML to format your reponse for readability: p and ul/li tags for paragraphs and lists,
and pre-defined .good, .bad, and .warning classes to highlight the severity of the issue.

Do no add headers or intro words, but just get to the point. Put a short summary in a <summary> element,
and put any additional analysis and recommendations in a following <p> element.

Example of a nice output for the general stats section:

<summary>
    <b>âœ¨ AI Summary</b>
    <ul>
        <li>
            <span class="good">11/13 samples</span> show consistent metrics within expected ranges.
        </li>
        <li>
            <span class="bad">A1200.2003</span> and <span class="bad">A1200.2003</span> exhibit extremely 
            high percentage of <span class="bad">duplicates</span> (<span class="bad">65.54%</span> and 
            <span class="bad">83.14%</span>, respectively) and the percentage of <span class="bad">fails (33.33%)</span>. 
            <span class="bad">A1200.2003</span> also registers a very low <span class="bad"> mapping rate (38.61%)</span>.
        </li>
        <li>
            <span class="warning">Sample-1.007</span> displays a relatively low percentage of <span class="warning">valid pairs (48.08%)</span> 
            and <span class="warning">passed Di-Tags (22.51%)</span>.   
        </li>
    </ul>
</summary>
<p>
    FastQC results indicate that <span class="bad">A1200.2003</span> and <span class="bad">A1200.2003</span> 
    have a slight <span class="bad">GC content</span> bias: at 39.5% against most other samples having 38.0%, 
    which indicates a potential contamination that could be the source of other anomalities in quality metrics.
</p>
""",
        ),
        ("user", "{input}"),
    ]
)


class Client:
    def __init__(self):
        self.name: str
        self.model: Optional[str] = None
        self.llm: BaseLanguageModel

    @abstractmethod
    def invoke(self, input: List[Dict[str, str]]) -> Optional[str]:
        prompt = prompt_template.format_messages(input=input)
        return self.llm.invoke(prompt).content


class OpenAiClient(Client):
    def __init__(self):
        super().__init__()
        self.name = "OpenAI"
        self.model = "gpt-4o"
        self.llm = ChatOpenAI(
            model=self.model,
            api_key=SecretStr(os.environ["OPENAI_API_KEY"]),
            temperature=0.0,
        )


class AnthropicClient(Client):
    def __init__(self):
        super().__init__()
        self.name = "Anthropic"
        self.model = "claude-3-5-sonnet-20240620"
        self.llm = ChatAnthropic(
            model=self.model,  # type: ignore
            api_key=SecretStr(os.environ["ANTHROPIC_API_KEY"]),
            temperature=0.0,
        )  # type: ignore


class SeqeraClient(Client):
    def __init__(self):
        super().__init__()
        self.name = "Seqera Chat"
        self.token = os.environ.get("SEQERA_API_KEY")

    def invoke(self, input: List[Dict[str, str]]) -> Optional[str]:
        response = requests.post(
            "https://api.seqera.io/v1/chat",
            headers={"Authorization": f"Bearer {self.token}"} if self.token else {},
            json={"messages": input},
        )
        return response.json()["message"]["content"]


def get_llm_client() -> Optional[Client]:
    if not config.ai_summary:
        return None

    if config.ai_provider == "seqera":
        return SeqeraClient()

    client = None
    if config.ai_provider == "anthropic":
        token = os.environ.get("ANTHROPIC_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'anthropic', but Anthropic API "
                "key not set. Please set the ANTHROPIC_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        client = AnthropicClient()

    if config.ai_provider == "openai":
        token = os.environ.get("OPENAI_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'openai', but OpenAI API "
                "key not set. Please set the OPENAI_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        client = OpenAiClient()

    return client


def add_ai_summary_to_report():
    if not (client := get_llm_client()):
        return

    content: str = ""  # data formatted for the LLM
    if report.general_stats_plot:
        content += dedent(f"""
            Section: MultiQC General Statistics (Overview of key QC metrics for each sample)

            {report.general_stats_plot.data_for_ai_prompt()}
            """)

    for section in report.get_all_sections():
        if section.plot_anchor and section.plot_anchor in report.plot_by_id:
            plot = report.plot_by_id[section.plot_anchor]
            if plot_content := plot.data_for_ai_prompt():
                content += dedent(f"""\
                    Tool: {section.module} ({section.module_info})
                    Section: {section.name} {f"({section.description})" if section.description else ""}
                    {f"\nMore detail about interpreting the data: {section.helptext}" if section.helptext else ""}

                    {plot_content}

                    ----------------------
                """)

    if not content:
        return

    if langsmith_project := os.environ.get("LANGCHAIN_PROJECT"):
        with tracing_v2_enabled(
            project_name=langsmith_project,
            client=LangSmithClient(
                api_key=os.environ["LANGCHAIN_API_KEY"],
                api_url=os.environ["LANGCHAIN_ENDPOINT"],
            ),
        ):
            generated_content = client.invoke([{"role": "user", "content": content}])
    else:
        generated_content = client.invoke([{"role": "user", "content": content}])

    if generated_content:
        disclaimer = f"This summary is AI-generated. Take with a grain of salt. LLM provider: {client.name}"
        if client.model:
            disclaimer += f", model: {client.model}"
        report.ai_summary = (
            f'<details>{generated_content}<p style="color: gray; font-style: italic">{disclaimer}</p></details>'
        )

import base64
import logging
import os
import re
from textwrap import dedent
from typing import Dict, Optional, cast, TYPE_CHECKING
from markdown import markdown
from pydantic import BaseModel, Field
import requests

from pydantic.types import SecretStr

from multiqc import config, report
from multiqc.core.log_and_rich import run_with_spinner
from multiqc.types import PlotType

if TYPE_CHECKING:
    from langchain_core.language_models.chat_models import BaseChatModel  # type: ignore


logger = logging.getLogger(__name__)

seqera_api_url: str = os.environ.get("SEQERA_API_URL", "https://seqera.io")
seqera_website: str = os.environ.get("SEQERA_WEBSITE", "https://seqera.io")


_PROMPT_BASE = """\
You are an expert in bioinformatics, sequencing technologies, and genomics data analysis.
You are given findings from a MultiQC report, generated by a bioinformatics workflow.
MultiQC consists of so called modules that support different tools that output QC metrics
(e.g. bclconvert, FastQC, samtools stats, bcftools stats, fastp, Picard, SnpEff, etc), and
it aggregates results from different tools. It outputs a "General Statistics" section that
has a table with a summary of key metrics from all modules across each sample. That table
is followed by module-specific sections that usually have more detail on the same samples.

You are given data from such a report, split by section. Your task is to analyse the data, and
give a very short and concise overall summary for the results. Don't waste words: mention only
the important QC issues. If there are no issues, just say so. Limit it to 1-2 bullet points.

Use markdown to format your reponse for readability. Use directives with pre-defined classes
.text-green, .text-red, and .text-yellow to highlight severity, e.g. :span[39.2%]{.text-red}. 
Highlight any mentioned sample names or sample named prefixes or suffixes with a sample directive, 
and make sure to use the same color classes for severity, e.g. :sample[A1001.2003]{.text-yellow}
or :sample[A1001]{.text-yellow}. Do not put multiple sample names inside one directive.

Please do not add any extra headers to the response.
"""

_SHORT_EXAMPLE = """
Two examples of short summaries:

- :span[11/13 samples]{.text-green} show consistent metrics within expected ranges.
- :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} exhibit extremely high percentage of :span[duplicates]{.text-red} (:span[65.54%]{.text-red} and :span[83.14%]{.text-red}, respectively).

- All samples show good quality metrics with :span[75.7-77.0%]{.text-green} CpG methylation and :span[76.3-86.0%]{.text-green} alignment rates
- :sample[2wk]{.text-yellow} samples show slightly higher duplication (:span[11-15%]{.text-yellow}) compared to :sample[1wk]{.text-green} samples (:span[6-9%]{.text-green})'
"""

_PROMPT_FULL_DETAILED_SUMMARY_INSTRUCTIONS = """
Next, generate a more detailed summary of the results, and your recommendations for the next steps.

Make sure to use a multiple of 4 spaces to indent nested lists.

Highlight severity of the values and the sample names with directives and the pre-defined classes, similar to the short summary, 
as in the example below.
"""

_EXAMPLE_SUMMARY_FOR_FULL = """\
Example, formatted as YAML of 3 sections (summary, detailed_summary, and recommendations):

summary: |
    - :span[11/13 samples]{.text-green} show consistent metrics within expected ranges.
    - :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} exhibit extremely high percentage of :span[duplicates]{.text-red} (:span[65.54%]{.text-red} and :span[83.14%]{.text-red}, respectively).
"""

_EXAMPLE_DETAILED_SUMMARY = """\
detailed_summary: |
    - :sample[A1002]{.text-yellow} and :sample[A1003]{.text-yellow} groups (:span[11/13 samples]{.text-green}) show good quality metrics, with consistent GC content (38-39%), read lengths (125 bp), and acceptable levels of duplicates and valid pairs.
    - :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} show severe quality issues:
        - Extremely high duplicate rates (:span[65.54%]{.text-red} and :span[83.14%]{.text-red})
        - Low percentages of valid pairs (:span[37.2%]{.text-red} and :span[39.2%]{.text-red})
        - High percentages of failed modules in FastQC (:span[33.33%]{.text-red})
        - Significantly higher total sequence counts (:span[141.9M]{.text-red} and :span[178.0M]{.text-red}) compared to other samples
        - FastQC results indicate that :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} have a slight :span[GC content]{.text-red} bias at 39.5% against most other samples having 38.0%, which indicates a potential contamination that could be the source of other anomalies in quality metrics.

    - :sample[A1002-1007]{.text-yellow} shows some quality concerns:
        - Low percentage of valid pairs (:span[48.08%]{.text-yellow})
        - Low percentage of passed Di-Tags (:span[22.51%]{.text-yellow})

    - Overrepresented sequences analysis reveals adapter contamination in several samples, particularly in :sample[A1001.2003]{.text-yellow} (up to :span[35.82%]{.text-yellow} in Read 1).
    - HiCUP analysis shows that most samples have acceptable levels of valid pairs, with :sample[A1003]{.text-green} group generally performing better than :sample[A1002]{.text-yellow} group.

recommendations: |
    - Remove :sample[A1001.2003]{.text-red} and :sample[A1200.2004]{.text-red} from further analysis due to severe quality issues.
    - Investigate the cause of low valid pairs and passed Di-Tags in :sample[A1002-1007]{.text-yellow}. Consider removing it if the issue cannot be resolved.
    - Perform adapter trimming on all samples, particularly focusing on :sample[A1001]{.text-red} group.
    - Re-run the Hi-C analysis pipeline after removing problematic samples and performing adapter trimming.
    - Investigate the cause of higher duplication rates in :sample[A1002]{.text-yellow} group compared to :sample[A1003]{.text-green} group, although they are still within acceptable ranges.
    - Consider adjusting the Hi-C protocol or library preparation steps to improve the percentage of valid pairs, especially for :sample[A1002]{.text-yellow} group.
"""

PROMPT_SHORT = f"""\
{_PROMPT_BASE}

{_SHORT_EXAMPLE}
"""

PROMPT_FULL = f"""\
{_PROMPT_BASE}

{_PROMPT_FULL_DETAILED_SUMMARY_INSTRUCTIONS}

{_EXAMPLE_SUMMARY_FOR_FULL}

{_EXAMPLE_DETAILED_SUMMARY}
"""


class InterpretationOutput(BaseModel):
    summary: str = Field(description="A very short and concise overall summary")
    detailed_summary: Optional[str] = Field(default=None, description="A more detailed summary")
    recommendations: Optional[str] = Field(default=None, description="Recommendations for the next steps")

    def markdown_to_html(self, text: str) -> str:
        """
        Convert markdown to HTML
        """
        html = markdown(text)
        # Find and replace directives :span[1.23%]{.text-red} -> <span>..., handle multiple matches in one string
        html = re.sub(
            r":span\[([^\]]+?)\]\{\.text-(green|red|yellow)\}",
            r"<span class='text-\2'>\1</span>",
            html,
        )
        # similarly, find and replace directives:sample[A1001.2003]{.text-red} -> <sample>...
        html = re.sub(
            r":sample\[([^\]]+?)\]\{\.text-(green|red|yellow)\}",
            r"<sample data-toggle='tooltip' title='Click to highlight in the report' class='text-\2'>\1</sample>",
            html,
        )
        return html

    def format_text(self) -> str:
        """
        Format to markdown to display in Seqera AI
        """
        return (
            f"## Summary\n{self.summary}"
            + (f"\n## Detailed summary\n{self.detailed_summary}" if self.detailed_summary else "")
            + (f"\n## Recommendations\n{self.recommendations}" if self.recommendations else "")
        )


class InterpretationResponse(BaseModel):
    interpretation: InterpretationOutput
    uuid: Optional[str] = None


class Client:
    def __init__(self, model: str, token: Optional[str]):
        self.title: str
        self.model: str = model
        self.token: Optional[str] = token

    def interpret_report_short(self, report_content: str) -> Optional[InterpretationResponse]:
        raise NotImplementedError

    def interpret_report_full(self, report_content: str) -> Optional[InterpretationResponse]:
        raise NotImplementedError


class LangchainClient(Client):
    def __init__(self, model: str, token: Optional[str]):
        super().__init__(model, token)
        self.llm: BaseChatModel

    def interpret_report_short(self, report_content: str) -> Optional[InterpretationResponse]:
        from langsmith import Client as LangSmithClient  # type: ignore
        from langchain_core.tracers.context import tracing_v2_enabled  # type: ignore

        llm = self.llm

        def send_request() -> Optional[str]:
            with tracing_v2_enabled(
                project_name=os.environ.get("LANGCHAIN_PROJECT"),
                client=LangSmithClient(
                    api_key=os.environ.get("LANGCHAIN_API_KEY"),
                    api_url=os.environ.get("LANGCHAIN_ENDPOINT"),
                ),
            ):
                response = llm.invoke(
                    [
                        {"role": "system", "content": PROMPT_SHORT},
                        {"role": "user", "content": report_content},
                    ],
                )
            return cast(str, response.content)

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Interpreting MultiQC report...", send_request)

        if not response:
            msg = f"Got empty response from the LLM {self.title} {self.model}"
            if config.strict:
                raise RuntimeError(msg)
            logger.error(msg)
            return None
        return InterpretationResponse(interpretation=InterpretationOutput(summary=response))

    def interpret_report_full(self, report_content: str) -> Optional[InterpretationResponse]:
        from langsmith import Client as LangSmithClient  # type: ignore
        from langchain_core.tracers.context import tracing_v2_enabled  # type: ignore

        llm = self.llm.with_structured_output(InterpretationOutput, include_raw=True)

        def send_request() -> Optional[InterpretationOutput]:
            with tracing_v2_enabled(
                project_name=os.environ.get("LANGCHAIN_PROJECT"),
                client=LangSmithClient(
                    api_key=os.environ.get("LANGCHAIN_API_KEY"),
                    api_url=os.environ.get("LANGCHAIN_ENDPOINT"),
                ),
            ):
                response = llm.invoke(
                    [
                        {"role": "system", "content": PROMPT_FULL},
                        {"role": "user", "content": report_content},
                    ],
                )
            return cast(InterpretationOutput, response)

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Interpreting MultiQC report...", send_request)

        response = cast(Dict, response)
        if not response["parsed"]:
            if response["raw"]:
                msg = f"Failed to parse the response from the LLM: {response['raw']}"
                if config.strict:
                    raise RuntimeError(msg)
                logger.error(msg)
                return None
            else:
                msg = f"Got empty response from the LLM {self.title} {self.model}"
                if config.strict:
                    raise RuntimeError(msg)
                logger.error(msg)
                return None
        return InterpretationResponse(interpretation=response["parsed"])


class OpenAiClient(LangchainClient):
    def __init__(self, model: str, token: str):
        from langchain_openai import ChatOpenAI  # type: ignore

        super().__init__(model, token)
        self.title = "OpenAI"
        self.llm = ChatOpenAI(
            model=self.model,
            api_key=SecretStr(token),
            temperature=0.0,
        )


class AnthropicClient(LangchainClient):
    def __init__(self, model: str, token: str):
        from langchain_anthropic import ChatAnthropic  # type: ignore

        super().__init__(model, token)
        self.title = "Anthropic"
        self.llm = ChatAnthropic(
            model=self.model,  # type: ignore
            api_key=SecretStr(token),
            temperature=0.0,
        )  # type: ignore


class SeqeraClient(Client):
    def __init__(self, model: str, token: Optional[str]):
        super().__init__(model, token)
        self.title = "Seqera AI"

    def interpret_report_short(self, report_content: str) -> Optional[InterpretationResponse]:
        def send_request() -> requests.Response:
            return requests.post(
                f"{seqera_api_url}/invoke-with-token" if self.token else f"{seqera_api_url}/invoke",
                headers={"Authorization": f"Bearer {self.token}"} if self.token else {},
                json={
                    "system_message": PROMPT_SHORT,
                    "user_message": report_content,
                    "model": self.model,
                    "tags": ["multiqc", f"multiqc_version:{config.version}"],
                },
            )

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Summarizing report with AI...", send_request)

        if response.status_code != 200:
            msg = f"Failed to get a response from Seqera: {response.status_code} {response.text}"
            logger.error(msg)
            if config.strict:
                raise RuntimeError(msg)
            return None

        response_dict = response.json()
        uuid = response_dict.get("uuid")
        generation = response_dict.get("generation")
        return InterpretationResponse(
            uuid=uuid,
            interpretation=InterpretationOutput(summary=generation),
        )

    def interpret_report_full(self, report_content: str) -> Optional[InterpretationResponse]:
        def send_request() -> requests.Response:
            return requests.post(
                f"{seqera_api_url}/invoke-with-token" if self.token else f"{seqera_api_url}/invoke",
                headers={"Authorization": f"Bearer {self.token}"} if self.token else {},
                json={
                    "system_message": PROMPT_FULL,
                    "user_message": report_content,
                    "model": self.model,
                    "tags": ["multiqc", f"multiqc_version:{config.version}"],
                    "response_schema": {
                        "name": "Interpretation",
                        "description": "Interpretation of a MultiQC report",
                        "input_schema": {
                            "type": "object",
                            "required": ["summary"],
                            "properties": {
                                key: {
                                    "type": "string",
                                    "description": value.description,
                                    **({"default": value.default} if value.default is None else {}),
                                }
                                for key, value in InterpretationOutput.model_fields.items()
                            },
                        },
                    },
                },
            )

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Summarizing report with AI...", send_request)

        if response.status_code != 200:
            msg = f"Failed to get a response from Seqera: {response.status_code} {response.text}"
            logger.error(msg)
            if config.strict:
                raise RuntimeError(msg)
            return None

        response_dict = response.json()
        uuid = response_dict.get("uuid")
        generation: Dict[str, str] = response_dict.get("generation")
        return InterpretationResponse(
            uuid=uuid,
            interpretation=InterpretationOutput(**generation),
        )


def get_llm_client() -> Optional[Client]:
    if not config.ai_summary:
        return None

    if config.ai_provider == "seqera":
        token = os.environ.get("SEQERA_API_KEY", os.environ.get("TOWER_ACCESS_TOKEN"))
        if not token:
            logger.warning(
                "config.ai_summary is set to true, and config.ai_provider is set to 'seqera', but Seqera tower access "
                "token is not set. Please set the TOWER_ACCESS_TOKEN environment variable, or change config.ai_provider"
            )
        return SeqeraClient(config.ai_model, token)

    if config.ai_provider == "anthropic":
        token = os.environ.get("ANTHROPIC_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'anthropic', but Anthropic API "
                "key not set. Please set the ANTHROPIC_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        model = config.ai_model if config.ai_model.startswith("claude") else "claude-3-5-sonnet-20240620"
        return AnthropicClient(model, token)

    if config.ai_provider == "openai":
        token = os.environ.get("OPENAI_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'openai', but OpenAI API "
                "key not set. Please set the OPENAI_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        model = config.ai_model if config.ai_model.startswith("gpt") else "gpt-4o"
        return OpenAiClient(model, token)

    return None


def _strip_html(text: str) -> str:
    return text.replace("<p>", "").replace("</p>", "")


PLOT_TYPES_FOR_OVERALL_SUMMARY = [PlotType.TABLE.value, PlotType.BAR.value]


def add_ai_summary_to_report():
    if not (client := get_llm_client()):
        return

    report.ai_provider_title = client.title
    report.ai_model = client.model
    report.ai_token = client.token

    content: str = ""  # data formatted for the LLM
    if report.general_stats_plot:
        content += f"""
Section: MultiQC General Statistics (Overview of key QC metrics for each sample)

Plot type: table

{report.general_stats_plot.format_for_ai_prompt()}
"""

    for section in report.get_all_sections():
        if section.plot_anchor and section.plot_anchor in report.plot_by_id:
            plot = report.plot_by_id[section.plot_anchor]
            if plot.plot_type not in PLOT_TYPES_FOR_OVERALL_SUMMARY:
                continue
            if plot_content := plot.format_for_ai_prompt():
                helptext = f"More detail about interpreting the data: {section.helptext}" if section.helptext else ""
                description = f" ({_strip_html(section.description)})" if section.description else ""
                content += f"""
Tool: {section.module} ({section.module_info})

Section: {section.name}{description}

Plot type: {plot.plot_type}

{helptext}

{plot_content}

----------------------
"""

    if not content:
        return

    # strip triple newlines
    content = re.sub(r"\n\n\n", "\n\n", content)

    if config.ai_summary_full:
        response: Optional[InterpretationResponse] = client.interpret_report_full(content)
    else:
        response: Optional[InterpretationResponse] = client.interpret_report_short(content)
    if not response:
        return None

    logger.debug(f"Interpretation: {response.interpretation}")
    if not response.interpretation:
        return None

    interpretation: InterpretationOutput = response.interpretation

    sparkle_icon = """
    <span style="vertical-align: middle">
        <svg width="12" height="12" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M6.4375 7L7.9375 1.5L9.4375 7L14.9375 8.5L9.4375 10.5L7.9375 15.5L6.4375 10.5L0.9375 8.5L6.4375 7Z" stroke="black" stroke-width="0.75" stroke-linejoin="round"/>
            <path d="M13.1786 2.82143L13.5 4L13.8214 2.82143L15 2.5L13.8214 2.07143L13.5 1L13.1786 2.07143L12 2.5L13.1786 2.82143Z" stroke="#160F26" stroke-width="0.5" stroke-linejoin="round"/>
        </svg>
    </span>
    """

    if response.uuid:
        seqera_api_token = f"data-seqera-api-token={client.token} " if client.token else ""
        continue_chat_btn = (
            "<button class='btn btn-default btn-sm ai-continue-in-chat'"
            + f" data-generation-id={response.uuid}"
            + f" data-seqera-website={seqera_website}"
            + f" {seqera_api_token}"
            + " onclick='continueInChatHandler(event)'"
            + f">Continue with {sparkle_icon} <strong>Seqera AI</strong></button>"
        )
    else:
        continue_chat_btn = ""
    recommendations = ""
    if interpretation.recommendations:
        recommendations = dedent(f"""
        <p>
        <b>Recommendations</b> {interpretation.markdown_to_html(interpretation.recommendations)}
        </p>""")

    detailed_summary = ""
    if interpretation.detailed_summary:
        detailed_summary = f"""
        <p>
        <b>Detailed summary</b> {interpretation.markdown_to_html(interpretation.detailed_summary)}
        </p>
        """

    seqera_ai_beta_icon = """
    <span style="vertical-align: middle">
        <svg width="30" height="12" viewBox="0 0 49 19" fill="none" xmlns="http://www.w3.org/2000/svg">
            <rect x="1.4375" y="0.5" width="47" height="18" rx="9" fill="#160F26" fill-opacity="0.1"/>
            <rect x="1.4375" y="0.5" width="47" height="18" rx="9" stroke="#160F26"/>
            <path d="M13.0392 14V5.27273H16.0904C16.6983 5.27273 17.1998 5.37784 17.5946 5.58807C17.9895 5.79545 18.2836 6.07528 18.4767 6.42756C18.6699 6.77699 18.7665 7.16477 18.7665 7.59091C18.7665 7.96591 18.6998 8.27557 18.5662 8.51989C18.4355 8.7642 18.2623 8.95739 18.0463 9.09943C17.8333 9.24148 17.6017 9.34659 17.3517 9.41477V9.5C17.6188 9.51705 17.8873 9.6108 18.1571 9.78125C18.427 9.9517 18.6529 10.196 18.8347 10.5142C19.0165 10.8324 19.1074 11.2216 19.1074 11.6818C19.1074 12.1193 19.008 12.5128 18.8091 12.8622C18.6103 13.2116 18.2963 13.4886 17.8674 13.6932C17.4384 13.8977 16.8801 14 16.1926 14H13.0392ZM14.0961 13.0625H16.1926C16.883 13.0625 17.373 12.929 17.6628 12.6619C17.9554 12.392 18.1017 12.0653 18.1017 11.6818C18.1017 11.3864 18.0265 11.1136 17.8759 10.8636C17.7253 10.6108 17.5108 10.4091 17.2324 10.2585C16.954 10.1051 16.6245 10.0284 16.2438 10.0284H14.0961V13.0625ZM14.0961 9.10795H16.0563C16.3745 9.10795 16.6614 9.04545 16.9171 8.92045C17.1756 8.79545 17.3801 8.61932 17.5307 8.39205C17.6841 8.16477 17.7608 7.89773 17.7608 7.59091C17.7608 7.20739 17.6273 6.8821 17.3603 6.61506C17.0932 6.34517 16.6699 6.21023 16.0904 6.21023H14.0961V9.10795ZM23.4519 14.1364C22.8212 14.1364 22.2772 13.9972 21.8198 13.7188C21.3652 13.4375 21.0144 13.0455 20.7672 12.5426C20.5229 12.0369 20.4007 11.4489 20.4007 10.7784C20.4007 10.108 20.5229 9.51705 20.7672 9.00568C21.0144 8.49148 21.3581 8.09091 21.7985 7.80398C22.2417 7.5142 22.7587 7.36932 23.3496 7.36932C23.6905 7.36932 24.0272 7.42614 24.3596 7.53977C24.6919 7.65341 24.9945 7.83807 25.2672 8.09375C25.54 8.34659 25.7573 8.68182 25.9192 9.09943C26.0811 9.51705 26.1621 10.0312 26.1621 10.642V11.0682H21.1167V10.1989H25.1394C25.1394 9.82955 25.0655 9.5 24.9178 9.21023C24.7729 8.92045 24.5655 8.69176 24.2956 8.52415C24.0286 8.35653 23.7132 8.27273 23.3496 8.27273C22.949 8.27273 22.6025 8.37216 22.3098 8.57102C22.0201 8.76705 21.7971 9.02273 21.6408 9.33807C21.4846 9.65341 21.4064 9.99148 21.4064 10.3523V10.9318C21.4064 11.4261 21.4917 11.8452 21.6621 12.1889C21.8354 12.5298 22.0755 12.7898 22.3823 12.9688C22.6891 13.1449 23.0456 13.233 23.4519 13.233C23.7161 13.233 23.9547 13.196 24.1678 13.1222C24.3837 13.0455 24.5698 12.9318 24.726 12.7812C24.8823 12.6278 25.003 12.4375 25.0882 12.2102L26.0598 12.483C25.9576 12.8125 25.7857 13.1023 25.5442 13.3523C25.3027 13.5994 25.0044 13.7926 24.6493 13.9318C24.2942 14.0682 23.8951 14.1364 23.4519 14.1364ZM30.5385 7.45455V8.30682H27.1465V7.45455H30.5385ZM28.1351 5.88636H29.1408V12.125C29.1408 12.4091 29.182 12.6222 29.2644 12.7642C29.3496 12.9034 29.4576 12.9972 29.5882 13.0455C29.7218 13.0909 29.8624 13.1136 30.0101 13.1136C30.1209 13.1136 30.2118 13.108 30.2828 13.0966C30.3539 13.0824 30.4107 13.071 30.4533 13.0625L30.6578 13.9659C30.5897 13.9915 30.4945 14.017 30.3723 14.0426C30.2502 14.071 30.0953 14.0852 29.9078 14.0852C29.6238 14.0852 29.3453 14.0241 29.0726 13.902C28.8027 13.7798 28.5783 13.5938 28.3993 13.3438C28.2232 13.0938 28.1351 12.7784 28.1351 12.3977V5.88636ZM33.9775 14.1534C33.5627 14.1534 33.1863 14.0753 32.8482 13.919C32.5101 13.7599 32.2417 13.5312 32.0428 13.233C31.8439 12.9318 31.7445 12.5682 31.7445 12.142C31.7445 11.767 31.8184 11.4631 31.9661 11.2301C32.1138 10.9943 32.3113 10.8097 32.5584 10.6761C32.8056 10.5426 33.0783 10.4432 33.3766 10.3778C33.6777 10.3097 33.9803 10.2557 34.2843 10.2159C34.682 10.1648 35.0044 10.1264 35.2516 10.1009C35.5016 10.0724 35.6834 10.0256 35.7971 9.96023C35.9135 9.89489 35.9718 9.78125 35.9718 9.61932V9.58523C35.9718 9.16477 35.8567 8.83807 35.6266 8.60511C35.3993 8.37216 35.0542 8.25568 34.5911 8.25568C34.111 8.25568 33.7346 8.3608 33.4618 8.57102C33.1891 8.78125 32.9973 9.00568 32.8865 9.24432L31.932 8.90341C32.1025 8.50568 32.3297 8.19602 32.6138 7.97443C32.9007 7.75 33.2132 7.59375 33.5513 7.50568C33.8922 7.41477 34.2275 7.36932 34.557 7.36932C34.7672 7.36932 35.0087 7.39489 35.2814 7.44602C35.557 7.49432 35.8226 7.59517 36.0783 7.74858C36.3368 7.90199 36.5513 8.13352 36.7218 8.44318C36.8922 8.75284 36.9775 9.16761 36.9775 9.6875V14H35.9718V13.1136H35.9206C35.8525 13.2557 35.7388 13.4077 35.5797 13.5696C35.4206 13.7315 35.209 13.8693 34.9448 13.983C34.6806 14.0966 34.3581 14.1534 33.9775 14.1534ZM34.1309 13.25C34.5286 13.25 34.8638 13.1719 35.1365 13.0156C35.4121 12.8594 35.6195 12.6577 35.7587 12.4105C35.9007 12.1634 35.9718 11.9034 35.9718 11.6307V10.7102C35.9292 10.7614 35.8354 10.8082 35.6905 10.8509C35.5485 10.8906 35.3837 10.9261 35.1962 10.9574C35.0115 10.9858 34.8311 11.0114 34.655 11.0341C34.4817 11.054 34.3411 11.071 34.2331 11.0852C33.9718 11.1193 33.7275 11.1747 33.5002 11.2514C33.2757 11.3253 33.0939 11.4375 32.9547 11.5881C32.8184 11.7358 32.7502 11.9375 32.7502 12.1932C32.7502 12.5426 32.8794 12.8068 33.138 12.9858C33.3993 13.1619 33.7303 13.25 34.1309 13.25Z" fill="#160F26"/>
        </svg>
    </span>
    """

    if config.ai_summary_full:
        disclaimer = f"This summary is AI-generated. Take with a grain of salt. Provider: {client.title}"
        if client.model:
            disclaimer += f", model: {client.model}"

        report.ai_summary = f"""
        <details>
        <summary>
        <div class="ai-summary-header">
            <b>Report AI Summary {seqera_ai_beta_icon if client.title == 'Seqera AI' else ''}</b>
            {continue_chat_btn}
        </div>
        {interpretation.markdown_to_html(interpretation.summary)}
        </summary>
        {detailed_summary}
        {recommendations}
        <p class="ai-summary-disclaimer">{disclaimer}</p>
        </details>
        <div class="mqc-table-expand ai-summary-expand"><span class="glyphicon glyphicon-chevron-down" aria-hidden="true"></span></div>
        """
    else:
        content_base64 = base64.b64encode(content.encode()).decode()

        generate_more_button = f"""
        <button 
            class="btn btn-sm btn-default ai-generate-more" 
            data-seqera-api-url="{report.seqera_api_url}"
            data-ai-provider-title="{client.title}"
            data-ai-model="{client.model}"
            data-ai-token="{client.token}"
            data-content-base64="{content_base64}"
            onclick="generateMoreHandler(event)"
        >
            Generate more details
        </button>
        """

        report.ai_summary = f"""
        <div class="ai-short-summary">
        <div class="ai-summary-header">
            <b>Report AI Summary {seqera_ai_beta_icon if client.title == 'Seqera AI' else ''}</b> 
            <span class="ai-summary-disclaimer">&nbsp;Provider: {client.title}, model: {client.model}</span>
        </div>
        {interpretation.markdown_to_html(interpretation.summary)}
        </div>
        {generate_more_button}
        """

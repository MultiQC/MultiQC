import base64
import logging
import os
import re
from textwrap import indent
from typing import TYPE_CHECKING, Dict, Optional, Tuple, cast

import requests
from markdown import markdown
from pydantic import BaseModel, Field
from pydantic.types import SecretStr

from multiqc import config, report
from multiqc.core.log_and_rich import run_with_spinner
from multiqc.core.strict_helpers import lint_error
from multiqc.types import Anchor

if TYPE_CHECKING:
    from langchain_core.language_models.chat_models import BaseChatModel  # type: ignore


logger = logging.getLogger(__name__)


_MULTIQC_DESCRIPTION = """\
You are an expert in bioinformatics, sequencing technologies, genomics data analysis, and adjacent fields.

You are given findings from a MultiQC report, generated by a bioinformatics workflow.
MultiQC supports various bioinformatics tools that output QC metrics, and aggregates those metrics
into a single report. It outputs a "General Statistics" table with key metrics for each sample across
all tools. That table is followed by more detailed sections from specific tools, that can include tables,
as well as plots of different types (bar plot, line plot, scatter plot, heatmap, etc.)
"""

_PROMPT_BASE = (
    _MULTIQC_DESCRIPTION
    + """
You are given data from such a report. Your task is to analyse the data, and
give 1-2 bullet points of a very short and concise overall summary for the results.
Don't waste words: mention only the important QC issues. If there are no issues, just say so.
Just print one or two bullet points, nothing else.
Please do not add any extra headers to the response.

Use markdown to format your reponse for readability. Use directives with pre-defined classes
.text-green, .text-red, and .text-yellow to highlight severity, e.g. :span[39.2%]{.text-red}.
Highlight any mentioned sample names or sample named prefixes or suffixes with a sample directive,
and make sure to use the same color classes for severity, e.g. :sample[A1001.2003]{.text-yellow}
or :sample[A1001]{.text-yellow}. Do not put multiple sample names inside one directive.

You must use only multiples of 4 spaces to indent nested lists.
"""
)

_SHORT_EXAMPLE = """
Two examples of short summaries:

- :span[11/13 samples]{.text-green} show consistent metrics within expected ranges.
- :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} exhibit extremely high percentage of :span[duplicates]{.text-red} (:span[65.54%]{.text-red} and :span[83.14%]{.text-red}, respectively).

- All samples show good quality metrics with :span[75.7-77.0%]{.text-green} CpG methylation and :span[76.3-86.0%]{.text-green} alignment rates
- :sample[2wk]{.text-yellow} samples show slightly higher duplication (:span[11-15%]{.text-yellow}) compared to :sample[1wk]{.text-green} samples (:span[6-9%]{.text-green})'
"""

_PROMPT_FULL_DETAILED_SUMMARY_INSTRUCTIONS = """
Next, generate a more detailed summary of the results, and your recommendations for the next steps.

Make sure to use a multiple of 4 spaces to indent nested lists.

Highlight severity of the values and the sample names with directives and the pre-defined classes, similar to the short summary,
as in the example below.
"""

_EXAMPLE_SUMMARY_FOR_FULL = """\
- :span[11/13 samples]{.text-green} show consistent metrics within expected ranges.
- :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} exhibit extremely high percentage of :span[duplicates]{.text-red} (:span[65.54%]{.text-red} and :span[83.14%]{.text-red}, respectively).
"""

_PROMPT_EXAMPLE_SUMMARY_FOR_FULL = f"""\
Example, formatted as YAML of 3 sections (summary, detailed_summary, and recommendations):

summary: |
{indent(_EXAMPLE_SUMMARY_FOR_FULL, "    ")}
"""

_EXAMPLE_DETAILED_SUMMARY = """\
**Analysis**

- :sample[A1002]{.text-yellow} and :sample[A1003]{.text-yellow} groups (:span[11/13 samples]{.text-green}) show good quality metrics, with consistent GC content (38-39%), read lengths (125 bp), and acceptable levels of duplicates and valid pairs.
- :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} show severe quality issues:
    - Extremely high duplicate rates (:span[65.54%]{.text-red} and :span[83.14%]{.text-red})
    - Low percentages of valid pairs (:span[37.2%]{.text-red} and :span[39.2%]{.text-red})
    - High percentages of failed modules in FastQC (:span[33.33%]{.text-red})
    - Significantly higher total sequence counts (:span[141.9M]{.text-red} and :span[178.0M]{.text-red}) compared to other samples
    - FastQC results indicate that :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} have a slight :span[GC content]{.text-red} bias at 39.5% against most other samples having 38.0%, which indicates a potential contamination that could be the source of other anomalies in quality metrics.

- :sample[A1002-1007]{.text-yellow} shows some quality concerns:
    - Low percentage of valid pairs (:span[48.08%]{.text-yellow})
    - Low percentage of passed Di-Tags (:span[22.51%]{.text-yellow})

- Overrepresented sequences analysis reveals adapter contamination in several samples, particularly in :sample[A1001.2003]{.text-yellow} (up to :span[35.82%]{.text-yellow} in Read 1).
- HiCUP analysis shows that most samples have acceptable levels of valid pairs, with :sample[A1003]{.text-green} group generally performing better than :sample[A1002]{.text-yellow} group.

**Recommendations**

- Remove :sample[A1001.2003]{.text-red} and :sample[A1200.2004]{.text-red} from further analysis due to severe quality issues.
- Investigate the cause of low valid pairs and passed Di-Tags in :sample[A1002-1007]{.text-yellow}. Consider removing it if the issue cannot be resolved.
- Perform adapter trimming on all samples, particularly focusing on :sample[A1001]{.text-red} group.
- Re-run the Hi-C analysis pipeline after removing problematic samples and performing adapter trimming.
- Investigate the cause of higher duplication rates in :sample[A1002]{.text-yellow} group compared to :sample[A1003]{.text-green} group, although they are still within acceptable ranges.
- Consider adjusting the Hi-C protocol or library preparation steps to improve the percentage of valid pairs, especially for :sample[A1002]{.text-yellow} group.
"""

_PROMPT_EXAMPLE_DETAILED_SUMMARY = f"""\
detailed_summary: |
{indent(_EXAMPLE_DETAILED_SUMMARY, "    ")}
"""

PROMPT_SHORT = f"""\
{_PROMPT_BASE}

{_SHORT_EXAMPLE}
"""

PROMPT_FULL = f"""\
{_PROMPT_BASE}

{_PROMPT_FULL_DETAILED_SUMMARY_INSTRUCTIONS}

{_PROMPT_EXAMPLE_SUMMARY_FOR_FULL}

{_PROMPT_EXAMPLE_DETAILED_SUMMARY}
"""


class InterpretationOutput(BaseModel):
    summary: str = Field(description="A very short and concise overall summary")
    detailed_analysis: Optional[str] = Field(default=None, description="Detailed analysis")

    def markdown_to_html(self, text: str) -> str:
        """
        Convert markdown to HTML
        """
        html = markdown(text)
        # Find and replace directives :span[1.23%]{.text-red} -> <span..., handle multiple matches in one string
        html = re.sub(
            r":span\[([^\]]+?)\]\{\.text-(green|red|yellow)\}",
            r"<span class='text-\2'>\1</span>",
            html,
        )
        # similarly, find and replace directives :sample[A1001.2003]{.text-red} -> <sample...
        html = re.sub(
            r":sample\[([^\]]+?)\]\{\.text-(green|red|yellow)\}",
            r"<sample data-toggle='tooltip' title='Click to highlight in the report' class='text-\2'>\1</sample>",
            html,
        )
        return html

    def format_text(self) -> str:
        """
        Format to markdown to display in Seqera AI
        """
        return f"## Analysis\n{self.summary}" + (f"\n\n{self.detailed_analysis}" if self.detailed_analysis else "")


class InterpretationResponse(BaseModel):
    interpretation: InterpretationOutput
    model: str
    thread_id: Optional[str] = None


class Client:
    def __init__(self, model: str, api_key: str):
        self.name: str
        self.title: str
        self.model: str = model
        self.api_key: str = api_key

    def interpret_report_short(self, report_content: str) -> Optional[InterpretationResponse]:
        raise NotImplementedError

    def interpret_report_full(self, report_content: str) -> Optional[InterpretationResponse]:
        raise NotImplementedError

    def max_tokens(self) -> int:
        raise NotImplementedError

    def n_tokens(self, text: str) -> int:
        """
        Estimate tokens for Seqera's API
        Using tiktoken for GPT models, falling back to Claude's tokenizer for others
        """
        try:
            if self.name == "anthropic" or self.name == "seqera":
                from anthropic import Anthropic  # type: ignore

                return Anthropic().count_tokens(text)
            else:
                import tiktoken  # type: ignore

                encoding = tiktoken.encoding_for_model(self.model)
                return len(encoding.encode(text))
        except ImportError:
            logger.warning(
                "Fallback to rough estimation if tokenizers not available. Install `tiktoken` or `anthropic` to get more accurate token counts."
            )
            return int(len(text) / 1.5)


class LangchainClient(Client):
    def __init__(self, model: str, api_key: str):
        super().__init__(model, api_key)
        self.llm: BaseChatModel

    def interpret_report_short(self, report_content: str) -> Optional[InterpretationResponse]:
        from langchain_core.tracers.context import tracing_v2_enabled  # type: ignore
        from langsmith import Client as LangSmithClient  # type: ignore

        llm = self.llm

        def send_request():
            if os.environ.get("LANGCHAIN_API_KEY"):
                with tracing_v2_enabled(
                    project_name=os.environ.get("LANGCHAIN_PROJECT", config.langchain_project),
                    client=LangSmithClient(
                        api_key=os.environ.get("LANGCHAIN_API_KEY"),
                        api_url=os.environ.get("LANGCHAIN_ENDPOINT", config.langchain_endpoint),
                    ),
                ):
                    response = llm.invoke(
                        [
                            {"role": "user", "content": PROMPT_SHORT},
                            {"role": "user", "content": report_content},
                        ],
                    )
            else:
                response = llm.invoke(
                    [
                        {"role": "user", "content": PROMPT_SHORT},
                        {"role": "user", "content": report_content},
                    ],
                )
            return response

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Interpreting MultiQC report...", send_request)

        if not response:
            msg = f"Got empty response from the LLM {self.title} {self.model}"
            if config.strict:
                raise RuntimeError(msg)
            logger.error(msg)
            return None

        resolved_model_name = self.model
        if "model" in response.response_metadata:
            resolved_model_name = response.response_metadata["model"]
        elif "model_name" in response.response_metadata:
            resolved_model_name = response.response_metadata["model_name"]

        return InterpretationResponse(
            interpretation=InterpretationOutput(
                summary=cast(str, response.content),
            ),
            model=resolved_model_name,
        )

    def interpret_report_full(self, report_content: str) -> Optional[InterpretationResponse]:
        from langchain_core.tracers.context import tracing_v2_enabled  # type: ignore
        from langsmith import Client as LangSmithClient  # type: ignore

        llm = self.llm.with_structured_output(InterpretationOutput, include_raw=True)

        def send_request():
            if os.environ.get("LANGCHAIN_API_KEY"):
                with tracing_v2_enabled(
                    project_name=os.environ.get("LANGCHAIN_PROJECT", config.langchain_project),
                    client=LangSmithClient(
                        api_key=os.environ.get("LANGCHAIN_API_KEY"),
                        api_url=os.environ.get("LANGCHAIN_ENDPOINT", config.langchain_endpoint),
                    ),
                ):
                    response = llm.invoke(
                        [
                            {"role": "user", "content": PROMPT_FULL},
                            {"role": "user", "content": report_content},
                        ],
                    )
            else:
                response = llm.invoke(
                    [
                        {"role": "user", "content": PROMPT_FULL},
                        {"role": "user", "content": report_content},
                    ],
                )
            return response

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Interpreting MultiQC report...", send_request)

        response = cast(Dict, response)
        if not response["parsed"]:
            if response["raw"]:
                msg = f"Failed to parse the response from the LLM: {response['raw']}"
                if config.strict:
                    raise RuntimeError(msg)
                logger.error(msg)
                return None
            else:
                msg = f"Got empty response from the LLM {self.title} {self.model}"
                if config.strict:
                    raise RuntimeError(msg)
                logger.error(msg)
                return None

        resolved_model_name = self.model
        if "model" in response["raw"].response_metadata:
            resolved_model_name = response["raw"].response_metadata["model"]
        elif "model_name" in response["raw"].response_metadata:
            resolved_model_name = response["raw"].response_metadata["model_name"]

        return InterpretationResponse(
            interpretation=response["parsed"],
            model=resolved_model_name,
        )


# Truncate the messages from the anthropic or openai logger. It would print the entire user message which is too long.
class TruncateLogFilter(logging.Filter):
    def filter(self, record):
        try:
            for m in record.args["json_data"]["messages"]:  # type: ignore
                if isinstance(m["content"], list):
                    for chunk in m["content"]:
                        chunk["text"] = chunk["text"][:200] + "<truncated>"  # type: ignore
                elif isinstance(m["content"], str):
                    m["content"] = m["content"][:200] + "<truncated>"  # type: ignore
        except Exception:
            pass
        return True


class OpenAiClient(LangchainClient):
    def __init__(self, api_key: str):
        from langchain_openai import ChatOpenAI  # type: ignore

        openai_logger = logging.getLogger("openai._base_client")
        openai_logger.addFilter(TruncateLogFilter())

        model = config.ai_model if config.ai_model and config.ai_model.startswith("gpt") else "gpt-4o"

        super().__init__(model, api_key)
        self.name = "openai"
        self.title = "OpenAI"
        self.llm = ChatOpenAI(
            model=self.model,
            api_key=SecretStr(api_key),
            temperature=0.0,
        )

    def max_tokens(self) -> int:
        return 128000


class AnthropicClient(LangchainClient):
    def __init__(self, api_key: str):
        from langchain_anthropic import ChatAnthropic  # type: ignore

        # Get the anthropic logger and add the filter
        anthropic_logger = logging.getLogger("anthropic._base_client")
        anthropic_logger.addFilter(TruncateLogFilter())

        model = (
            config.ai_model if config.ai_model and config.ai_model.startswith("claude") else "claude-3-5-sonnet-latest"
        )

        super().__init__(model, api_key)
        self.name = "anthropic"
        self.title = "Anthropic"
        self.llm = ChatAnthropic(
            model=self.model,  # type: ignore
            api_key=SecretStr(api_key),
            temperature=0.0,
        )  # type: ignore

    def max_tokens(self) -> int:
        return 200000


class SeqeraClient(Client):
    def __init__(self, model: str, api_key: str):
        super().__init__(model, api_key)
        self.name = "seqera"
        self.title = "Seqera AI"

    def max_tokens(self) -> int:
        return 200000

    def interpret_report_short(self, report_content: str) -> Optional[InterpretationResponse]:
        def send_request() -> requests.Response:
            return requests.post(
                f"{config.seqera_api_url}/internal-ai/query",
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={
                    "message": PROMPT_SHORT + "\n\n" + report_content,
                    "tags": ["multiqc", f"multiqc_version:{config.version}"],
                },
            )

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Summarizing report with AI...", send_request)

        if response.status_code != 200:
            msg = f"Failed to get a response from Seqera. Status code: {response.status_code} ({response.reason})"
            logger.error(msg)
            logger.debug(f"Response: {response.text}")
            if config.strict:
                raise RuntimeError(msg)
            return None

        response_dict = response.json()
        thread_id = response_dict.get("thread_id")
        generation = response_dict.get("generation")
        return InterpretationResponse(
            thread_id=thread_id,
            interpretation=InterpretationOutput(summary=generation),
            model=self.model or "claude-3-5-sonnet-latest",
        )

    def interpret_report_full(self, report_content: str) -> Optional[InterpretationResponse]:
        def send_request() -> requests.Response:
            return requests.post(
                f"{config.seqera_api_url}/internal-ai/query",
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={
                    "message": PROMPT_FULL + "\n\n" + report_content,
                    "tags": ["multiqc", f"multiqc_version:{config.version}"],
                    "response_schema": {
                        "name": "Interpretation",
                        "description": "Interpretation of a MultiQC report",
                        "input_schema": {
                            "type": "object",
                            "required": ["summary"],
                            "properties": {
                                key: {
                                    "type": "string",
                                    "description": value.description,
                                    **({"default": value.default} if value.default is None else {}),
                                }
                                for key, value in InterpretationOutput.model_fields.items()
                            },
                        },
                    },
                },
            )

        if config.verbose:
            response = send_request()
        else:
            response = run_with_spinner("ai", "Summarizing report with AI...", send_request)

        if response.status_code != 200:
            msg = f"Failed to get a response from Seqera: {response.status_code} {response.text}"
            logger.error(msg)
            if config.strict:
                raise RuntimeError(msg)
            return None

        response_dict = response.json()
        thread_id = response_dict.get("thread_id")
        generation: Dict[str, str] = response_dict.get("generation")
        return InterpretationResponse(
            thread_id=thread_id,
            interpretation=InterpretationOutput(**generation),
            model=self.model or "claude-3-5-sonnet-latest",
        )


def get_llm_client() -> Optional[Client]:
    if not config.ai_summary:
        return None

    if config.ai_provider == "seqera":
        if api_key := os.environ.get("TOWER_ACCESS_TOKEN"):
            logger.debug("Using Seqera access token from $TOWER_ACCESS_TOKEN environment variable")
        elif api_key := os.environ.get("SEQERA_ACCESS_TOKEN"):
            logger.debug("Using Seqera access token from $SEQERA_ACCESS_TOKEN environment variable")
        else:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'seqera', "
                "but Seqera access token is not set. "
                "Please set the SEQERA_ACCESS_TOKEN / TOWER_ACCESS_TOKEN environment variable "
                "or change config.ai_provider"
            )
            return None
        return SeqeraClient(config.ai_model, api_key)

    elif config.ai_provider == "anthropic":
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'anthropic', but Anthropic API "
                "key not set. Please set the ANTHROPIC_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        logger.debug("Using Anthropic API key from $ANTHROPIC_API_KEY environment variable")
        try:
            return AnthropicClient(api_key)
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "AI summary requested through `config.ai_summary`, but required dependencies are not installed. Install them with `pip install multiqc[anthropic]`"
            )

    elif config.ai_provider == "openai":
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'openai', but OpenAI API "
                "key not set. Please set the OPENAI_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        logger.debug("Using OpenAI API key from $OPENAI_API_KEY environment variable")
        try:
            return OpenAiClient(api_key)
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "AI summary requested through `config.ai_summary`, but required dependencies are not installed. Install them with `pip install multiqc[openai]`"
            )

    else:
        msg = f'Unknown AI provider "{config.ai_provider}". Please set config.ai_provider to one of the following: [{", ".join(config.AVAILABLE_AI_PROVIDERS)}]'
        if config.strict:
            raise RuntimeError(msg)
        logger.error(msg + ". Skipping AI summary")
        return None


def _strip_html(text: str) -> str:
    return text.replace("<p>", "").replace("</p>", "")


class AiToolMetadata(BaseModel):
    name: str
    info: Optional[str] = None
    href: Optional[str] = None
    comment: Optional[str] = None


class AiSectionMetadata(BaseModel):
    name: str
    module_anchor: Anchor
    description: Optional[str] = None
    comment: Optional[str] = None
    helptext: Optional[str] = None
    plot_anchor: Optional[Anchor] = None
    content: Optional[str] = None
    content_before_plot: Optional[str] = None


class AiReportMetadata(BaseModel):
    tools: Dict[Anchor, AiToolMetadata]
    sections: Dict[Anchor, AiSectionMetadata]


def ai_section_metadata() -> AiReportMetadata:
    return AiReportMetadata(
        tools={
            mod.anchor: AiToolMetadata(
                name=mod.name,
                info=mod.info,
                href=", ".join(mod.href),
                comment=mod.comment,
            )
            for mod in report.modules
            if not mod.hidden and mod.name != "Software Versions"
        },
        sections={
            section.anchor: AiSectionMetadata(
                name=section.name,
                description=section.description,
                comment=section.comment,
                helptext=section.helptext,
                plot_anchor=section.plot_anchor,
                module_anchor=section.module_anchor,
                content=section.content,
                content_before_plot=section.content_before_plot,
            )
            for mod in report.modules
            for section in mod.sections
            if not mod.hidden and mod.name != "Software Versions"
        },
    )


def build_prompt(client: Client, metadata: AiReportMetadata) -> Tuple[str, bool]:
    system_prompt = PROMPT_FULL if config.ai_summary_full else PROMPT_SHORT

    # Account for system message, plus leave 10% buffer
    max_tokens = client.max_tokens()

    user_prompt: str = ""
    current_n_tokens = client.n_tokens(system_prompt)

    # Details about modules used in the report - include first in the prompt
    tools_context: str = ""
    tools_context += "Tools used in the report:\n\n"
    for i, tool in enumerate(metadata.tools.values()):
        tools_context += f"{i + 1}. {tool.name} ({tool.info})\n"
        if tool.info:
            tools_context += f"Description: {tool.info}\n"
        if tool.href:
            tools_context += f"Links: {tool.href}\n"
        if tool.comment:
            tools_context += f"Comment: {tool.comment}\n"
        tools_context += "\n\n"
        tools_context += "\n----------------------\n\n"
    user_prompt += tools_context

    # General stats - also always include, otherwise we don't have anything to summarize
    if report.general_stats_plot:
        genstats_context = f"""
MultiQC General Statistics (overview of key QC metrics for each sample, across all tools)
{report.general_stats_plot.format_for_ai_prompt()}
"""
        genstats_n_tokens = client.n_tokens(genstats_context)
        if current_n_tokens + genstats_n_tokens > max_tokens:
            # If it's too long already, try without hidden columns
            genstats_context = f"""
MultiQC General Statistics (overview of key QC metrics for each sample, across all tools)
{report.general_stats_plot.format_for_ai_prompt(keep_hidden=False)}
"""
            genstats_n_tokens = client.n_tokens(genstats_context)
            if current_n_tokens + genstats_n_tokens > max_tokens:
                logger.warning(
                    f"General stats (almost) exceeds the {client.title}'s context window ({current_n_tokens} + "
                    f"{genstats_n_tokens} tokens, max: {client.max_tokens()} tokens). "
                    "AI summary will not be generated. Try hiding some columns in the general stats table "
                    "(see https://docs.seqera.io/multiqc/reports/customisation#hiding-columns) to reduce the context. "
                    "You can also open the HTML report in the browser, hide columns or samples dynamically, and request "
                    "the AI summary dynamically, or copy the prompt into clipboard and use it with extrenal services."
                )
                return user_prompt + genstats_context, True
        user_prompt += genstats_context
        current_n_tokens += genstats_n_tokens

    user_prompt = re.sub(r"\n\n\n", "\n\n", user_prompt)  # strip triple newlines

    # Build sections context and use it unless it's too long for the LLM context
    sec_context: str = ""
    for section in metadata.sections.values():
        tool = metadata.tools[section.module_anchor]
        sec_context += "\n----------------------\n\n"
        sec_context += f"Tool: {tool.name}\n"
        sec_context += f"Section: {section.name}\n"
        if section.description:
            sec_context += f"Section description: {_strip_html(section.description)}\n"
        if section.comment:
            sec_context += f"Section comment: {_strip_html(section.comment)}\n"
        if section.helptext:
            sec_context += f"Section help text: {_strip_html(section.helptext)}\n"

        if section.content_before_plot:
            sec_context += section.content_before_plot + "\n\n"
        if section.content:
            sec_context += section.content + "\n\n"

        if section.plot_anchor and section.plot_anchor in report.plot_by_id:
            plot = report.plot_by_id[section.plot_anchor]
            if plot_content := plot.format_for_ai_prompt(keep_hidden=True):
                if plot.pconfig.title:
                    sec_context += f"Title: {plot.pconfig.title}\n"
                sec_context += "\n" + plot_content

        # Check if adding this section would exceed the limit
        # Using rough estimate of 4 chars per token
        sec_n_tokens = client.n_tokens(sec_context)
        if current_n_tokens + sec_n_tokens > max_tokens:
            logger.warning(
                f"Truncating prompt to only the general stats to fit within {client.title}'s context window ({client.max_tokens()} tokens). "
                f"Tokens estimate: {current_n_tokens}, with sections: at least {current_n_tokens + sec_n_tokens}"
            )
            return user_prompt, False

    user_prompt += sec_context
    user_prompt = re.sub(r"\n\n\n", "\n\n", user_prompt)  # strip triple newlines
    return user_prompt, False


def _save_prompt_to_file(prompt: str):
    """Save content to file for debugging"""
    path = report.data_tmp_dir() / "multiqc_ai_prompt.txt"
    system_prompt = PROMPT_FULL if config.ai_summary_full else PROMPT_SHORT
    path.write_text(f"{system_prompt}\n\n----------------------\n\n{prompt}")
    logger.debug(f"Saved AI prompt to {path}")


def add_ai_summary_to_report():
    metadata: AiReportMetadata = ai_section_metadata()
    # Set data for JS runtime
    report.ai_report_metadata_base64 = base64.b64encode(metadata.model_dump_json().encode()).decode()

    if not config.ai_summary:
        # Not generating report in Python, leaving for JS runtime
        return

    if not (client := get_llm_client()):
        return

    report.ai_provider_id = client.name
    report.ai_provider_title = client.title
    report.ai_model = client.model

    prompt, exceeded_context_window = build_prompt(client, metadata)

    if config.development or config.verbose:
        _save_prompt_to_file(prompt)

    if exceeded_context_window:
        return

    response: Optional[InterpretationResponse]
    if config.ai_summary_full:
        if config.development and os.environ.get("MQC_STUB_AI_RESPONSE"):
            response = InterpretationResponse(
                interpretation=InterpretationOutput(
                    summary=_EXAMPLE_SUMMARY_FOR_FULL,
                    detailed_analysis=_EXAMPLE_DETAILED_SUMMARY,
                ),
                model="test-model",
                thread_id="68bcead8-1bea-4b75-84d1-fc2ae6afed51" if client.name == "seqera" else None,
            )
        else:
            response = client.interpret_report_full(prompt)
    else:
        if config.development and os.environ.get("MQC_STUB_AI_RESPONSE"):
            response = InterpretationResponse(
                interpretation=InterpretationOutput(
                    summary="- All samples show :span[good quality metrics]{.text-green} with consistent CpG methylation (:span[75.7-77.0%]{.text-green}), alignment rates (:span[76-86%]{.text-green}), and balanced strand distribution (:span[~50/50]{.text-green})\n- :sample[2wk]{.text-yellow} samples show slightly higher duplication (:span[11-15%]{.text-yellow}) and trimming rates (:span[13-23%]{.text-yellow}) compared to :sample[1wk]{.text-green} samples (:span[6-9%]{.text-green} duplication, :span[2-3%]{.text-green} trimming)",
                ),
                model="test-model",
                thread_id="68bcead8-1bea-4b75-84d1-fc2ae6afed51" if client.name == "seqera" else None,
            )
        else:
            response = client.interpret_report_short(prompt)

    if not response:
        return None

    if not response.interpretation:
        return None

    if response.model:
        report.ai_model_resolved = response.model

    if response.thread_id:
        report.ai_thread_id = response.thread_id

    interpretation: InterpretationOutput = response.interpretation
    report.ai_global_summary = interpretation.markdown_to_html(interpretation.summary)

    if config.ai_summary_full and interpretation.detailed_analysis:
        report.ai_global_detailed_analysis = interpretation.markdown_to_html(interpretation.detailed_analysis)

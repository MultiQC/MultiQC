import base64
import json
import logging
import os
import re
from textwrap import dedent
from typing import Optional, cast, TYPE_CHECKING
import bs4
from markdown import markdown
from pydantic import BaseModel, Field
import requests

from pydantic.types import SecretStr
from bs4 import BeautifulSoup
from dotenv import load_dotenv  # type: ignore

from multiqc import config, report

if TYPE_CHECKING:
    from langchain_core.language_models.chat_models import BaseChatModel  # type: ignore


load_dotenv()

logger = logging.getLogger(__name__)


class InterpretationOutput(BaseModel):
    summary: str = Field(description="Summary")
    short_abstract: str = Field(description="Short abstract of the summary")
    recommendations: Optional[str] = Field(default=None, description="Recommendations for the next steps")

    def markdown_to_html(self, text: str) -> str:
        """
        Convert markdown to HTML. Convert directives like :sample[A1001.2003]{.text-yellow} to HTML tags <sample class="text-yellow">A1001.2003</sample>
        """
        html = markdown(text)
        # find and replace :span[11/13 samples]{.text-green}, handle multiple matches in one string
        html = re.sub(r":span\[([^\]]+?)\]\{\.text-(green|red|yellow)\}", r"<span class='text-\2'>\1</span>", html)
        html = re.sub(
            r":sample\[([^\]]+?)\]\{\.text-(green|red|yellow)\}", r"<sample class='text-\2'>\1</sample>", html
        )
        return html

    # def _html_to_markdown(self, text: str) -> str:
    #     soup = BeautifulSoup(text, "html.parser")
    #     tag: bs4.element.Tag
    #     for tag in soup.find_all(["ul", "li", "b", "p", "sample", "span"]):
    #         if tag.name == "ul":
    #             tag.insert_before("\n")
    #             tag.insert_after("\n")
    #         elif tag.name == "li":
    #             tag.insert_before("- ")
    #             tag.insert_after("\n")
    #         elif tag.name == "b":
    #             tag.insert_before("**")
    #             tag.insert_after("**")
    #         elif tag.name == "p":
    #             tag.insert_before("\n")
    #             tag.insert_after("\n")
    #         elif tag.name in ["sample", "span"]:
    #             clz = tag.get("class")
    #             if clz:
    #                 try:
    #                     tag.insert_before(":span[")
    #                     tag.insert_after(f"]{{.{clz[0]}}}")
    #                 except Exception as e:
    #                     logger.error(f"Error inserting span for sample tag: {e}")
    #     return soup.get_text()

    def format_html(self) -> str:
        html = dedent(f"""
        <summary>
        <b>âœ¨ AI Summary</b>
        {self.markdown_to_html(self.short_abstract)}
        </summary>
        <p>
        <b>Detailed summary</b> {self.markdown_to_html(self.summary)}
        </p>
        """)
        if self.recommendations:
            html += dedent(f"""
            <p>
            <b>Recommendations</b> {self.markdown_to_html(self.recommendations)}
            </p>""")
        return html

    def format_text(self) -> str:
        """
        Format to markdown to display in Seqera Chat
        """
        return (
            f"## Summary\n{self.short_abstract}"
            + (f"\n## Detailed summary\n{self.summary}")
            + (f"\n## Recommendations\n{self.recommendations}" if self.recommendations else "")
        )


PROMPT = """\
You are an expert in bioinformatics, sequencing technologies, and genomics data analysis.
You are given key findings from a MultiQC report, generated by a bioinformatics workflow.
MultiQC consists of so called modules that support different tools that output QC metrics
(e.g. bclconvert, FastQC, samtools stats, bcftools stats, fastp, Picard, SnpEff, etc), and
it aggregates results from different tools. It outputs a "General Statistics" section that
has a table with a summary of key metrics from all modules across each sample. That table
is followed by module-specific sections that usually have more detail on the same samples.

You are given data from such a report, split by section. Your task is to analyse the data, and
give a short and concise overall summary for the results, followed by an even more concise abstract. 
Don't waste words: mention only the important QC issues. Only mention the sections that worth 
attention. If there are no issues, just say so.

Use markdown to format your reponse for readability. Use directives with pre-defined classes 
.text-green, .text-red, and .text-yellow to highlight the severity of the issue, e.g. 
:sample[A1001.2003]{.text-yellow} for sample names, or :span[39.2%]{.text-red} for other text spans like values.

After the summary, add a very short abstract of the summary, limited to 1-2 bullet points. Highlight
sample names with the pre-defined classes as well.

Finally, add your recommendations for the next steps.

Example, formatted as YAML of 3 requires sections (summary, short_abstract, and recommendations):

summary: |
  - :sample[A1002]{.text-yellow} and :sample[A1003]{.text-yellow} groups (:span[11/13 samples]{.text-green}) show good quality metrics, with consistent GC content (38-39%), read lengths (125 bp), and acceptable levels of duplicates and valid pairs.
  - :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} show severe quality issues:
    - Extremely high duplicate rates (:span[65.54%]{.text-red} and :span[83.14%]{.text-red})
    - Low percentages of valid pairs (:span[37.2%]{.text-red} and :span[39.2%]{.text-red})
    - High percentages of failed modules in FastQC (:span[33.33%]{.text-red})
    - Significantly higher total sequence counts (:span[141.9M]{.text-red} and :span[178.0M]{.text-red}) compared to other samples
    - FastQC results indicate that :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} have a slight :span[GC content]{.text-red} bias at 39.5% against most other samples having 38.0%, which indicates a potential contamination that could be the source of other anomalies in quality metrics.
  - :sample[A1002-1007]{.text-yellow} shows some quality concerns:
    - Low percentage of valid pairs (:span[48.08%]{.text-yellow})
    - Low percentage of passed Di-Tags (:span[22.51%]{.text-yellow})
  - Overrepresented sequences analysis reveals adapter contamination in several samples, particularly in :sample[A1001.2003]{.text-yellow} (up to :span[35.82%]{.text-yellow} in Read 1).
  - HiCUP analysis shows that most samples have acceptable levels of valid pairs, with :sample[A1003]{.text-green} group generally performing better than :sample[A1002]{.text-yellow} group.

short_abstract: |
  - :span[11/13 samples]{.text-green} show consistent metrics within expected ranges.
  - :sample[A1001.2003]{.text-red} and :sample[A1001.2004]{.text-red} exhibit extremely high percentage of :span[duplicates]{.text-red} (:span[65.54%]{.text-red} and :span[83.14%]{.text-red}, respectively).

recommendations: |
  - Remove :sample[A1001.2003]{.text-red} and :sample[A1200.2004]{.text-red} from further analysis due to severe quality issues.
  - Investigate the cause of low valid pairs and passed Di-Tags in :sample[A1002-1007]{.text-yellow}. Consider removing it if the issue cannot be resolved.
  - Perform adapter trimming on all samples, particularly focusing on :sample[A1001]{.text-red} group.
  - Re-run the Hi-C analysis pipeline after removing problematic samples and performing adapter trimming.
  - Investigate the cause of higher duplication rates in :sample[A1002]{.text-yellow} group compared to :sample[A1003]{.text-green} group, although they are still within acceptable ranges.
  - Consider adjusting the Hi-C protocol or library preparation steps to improve the percentage of valid pairs, especially for :sample[A1002]{.text-yellow} group.
"""


class Client:
    def __init__(self):
        self.name: str
        self.model: Optional[str] = None

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        raise NotImplementedError


class LangchainClient(Client):
    def __init__(self):
        self.name: str
        self.model: str
        self.llm: BaseChatModel

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        from langsmith import Client as LangSmithClient  # type: ignore
        from langchain_core.tracers.context import tracing_v2_enabled  # type: ignore

        with tracing_v2_enabled(
            project_name=os.environ.get("LANGCHAIN_PROJECT"),
            client=LangSmithClient(
                api_key=os.environ.get("LANGCHAIN_API_KEY"),
                api_url=os.environ.get("LANGCHAIN_ENDPOINT"),
            ),
        ):
            structured_llm = self.llm.with_structured_output(InterpretationOutput)
            return cast(
                InterpretationOutput,
                structured_llm.invoke(
                    [
                        {"role": "system", "content": PROMPT},
                        {"role": "user", "content": report_content},
                    ]
                ),
            )


class OpenAiClient(LangchainClient):
    def __init__(self):
        from langchain_openai import ChatOpenAI  # type: ignore

        super().__init__()
        self.name = "OpenAI"
        self.model = "gpt-4o"
        self.llm = ChatOpenAI(
            model=self.model,
            api_key=SecretStr(os.environ["OPENAI_API_KEY"]),
            temperature=0.0,
        )


class AnthropicClient(LangchainClient):
    def __init__(self):
        super().__init__()
        from langchain_anthropic import ChatAnthropic  # type: ignore

        self.name = "Anthropic"
        self.model = "claude-3-5-sonnet-20240620"
        self.llm = ChatAnthropic(
            model=self.model,  # type: ignore
            api_key=SecretStr(os.environ["ANTHROPIC_API_KEY"]),
            temperature=0.0,
        )  # type: ignore


class SeqeraClient(Client):
    def __init__(self):
        super().__init__()
        self.name = "Seqera Chat"
        token = os.environ.get("SEQERA_API_KEY", os.environ.get("TOWER_ACCESS_TOKEN"))
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'seqera', but Seqera tower access "
                "token is not set. Please set the TOWER_ACCESS_TOKEN environment variable, or change config.ai_provider"
            )
            return None
        self.url = os.environ.get("SEQERA_API_URL", "https://seqera.io")
        self.token = token

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        response = requests.post(
            f"{self.url}/interpret-multiqc-report",
            headers={"Authorization": f"Bearer {self.token}"} if self.token else {},
            json={
                "system_message": PROMPT,
                "report": report_content,
            },
        )
        if response.status_code != 200:
            logger.error(f"Failed to get a response from Seqera: {response.status_code} {response.text}")
            return None
        return InterpretationOutput(**response.json())


def get_llm_client() -> Optional[Client]:
    if not config.ai_summary:
        return None

    if config.ai_provider == "seqera":
        return SeqeraClient()

    if config.ai_provider == "anthropic":
        token = os.environ.get("ANTHROPIC_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'anthropic', but Anthropic API "
                "key not set. Please set the ANTHROPIC_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        return AnthropicClient()

    if config.ai_provider == "openai":
        token = os.environ.get("OPENAI_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'openai', but OpenAI API "
                "key not set. Please set the OPENAI_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        return OpenAiClient()

    return None


def add_ai_summary_to_report():
    if not (client := get_llm_client()):
        return

    content: str = ""  # data formatted for the LLM
    if report.general_stats_plot:
        content += f"""
Section: MultiQC General Statistics (Overview of key QC metrics for each sample)

{report.general_stats_plot.data_for_ai_prompt()}
"""

    for section in report.get_all_sections():
        if section.plot_anchor and section.plot_anchor in report.plot_by_id:
            plot = report.plot_by_id[section.plot_anchor]
            if plot_content := plot.data_for_ai_prompt():
                helptext = (
                    ("\n" + f"More detail about interpreting the data: {section.helptext}") if section.helptext else ""
                )
                description = f" ({section.description})" if section.description else ""
                content += f"""
Tool: {section.module} ({section.module_info})
Section: {section.name}{description}{helptext}

{plot_content}

----------------------
"""

    if not content:
        return

    interpretation = client.interpret_report(content)
    logger.debug(f"Interpretation: {interpretation}")

    if not interpretation:
        return None

    disclaimer = f"This summary is AI-generated. Take with a grain of salt. LLM provider: {client.name}"
    if client.model:
        disclaimer += f", model: {client.model}"

    continue_chat = ""
    website_url = os.environ.get("SEQERA_WEBSITE", "https://seqera.io")
    messages = [
        {"role": "user", "content": content},
        {"role": "assistant", "content": interpretation.format_text()},
    ]
    messages_json = json.dumps(messages)
    encoded_chat_messages = base64.b64encode(messages_json.encode("utf-8")).decode("utf-8")
    encoded_system_message = base64.b64encode(PROMPT.encode("utf-8")).decode("utf-8")
    key = f"data-key={key} " if (key := os.environ.get("SEQERA_CHAT_KEY")) else ""

    continue_chat = (
        "<button class='btn btn-primary'"
        + "id='continue-in-chat'"
        + f" data-encoded-system-message={encoded_system_message}"
        + f" data-encoded-chat-messages={encoded_chat_messages}"
        + f" data-website={website_url}"
        + f" {key}"
        + ">Continue with Seqera Chat</button>"
    )

    report.ai_summary = f"""
    <details>
    {interpretation.format_html()}
    <p style="color: gray; font-style: italic">{disclaimer}</p>
    {continue_chat}
    </details>
    """

import base64
import json
import logging
import os
from textwrap import dedent
from typing import Optional, cast, TYPE_CHECKING
from pydantic import BaseModel, Field
import requests

from pydantic.types import SecretStr
from bs4 import BeautifulSoup
from dotenv import load_dotenv  # type: ignore

from multiqc import config, report

if TYPE_CHECKING:
    from langchain_core.language_models.chat_models import BaseChatModel  # type: ignore


load_dotenv()

logger = logging.getLogger(__name__)


class InterpretationOutput(BaseModel):
    summary: str = Field(description="A short and concise summary")
    analysis: Optional[str] = Field(default=None, description="An analysis of the results")
    recommendations: Optional[str] = Field(default=None, description="Recommendations for the next steps")

    def _html_to_markdown(self, text: str) -> str:
        soup = BeautifulSoup(text, "html.parser")
        for tag in soup.find_all(["ul", "li", "b", "p", "span"]):
            if tag.name == "ul":
                tag.insert_before("\n")
                tag.insert_after("\n")
            elif tag.name == "li":
                tag.insert_before("- ")
                tag.insert_after("\n")
            elif tag.name == "b":
                tag.insert_before("**")
                tag.insert_after("**")
            elif tag.name == "p":
                tag.insert_before("\n")
                tag.insert_after("\n")
            elif tag.name == "span":
                tag.insert_before(":span[")
                tag.insert_after(f"]{{.{tag.get('class')[0]}}}")
        return soup.get_text()

    def format_html(self) -> str:
        return dedent(f"""
        <summary>
        <b>âœ¨ AI Summary</b>
        {self.summary}
        </summary>
        <p>
        <b>Analysis</b> {self.analysis}
        </p>
        <p>
        <b>Recommendations</b> {self.recommendations}
        </p>""")

    def format_text(self) -> str:
        """
        Format to markdown to display in Seqera Chat
        """
        return (
            f"## Summary\n{self._html_to_markdown(self.summary)}"
            + (f"## Analysis\n{self._html_to_markdown(self.analysis)}" if self.analysis else "")
            + (f"## Recommendations\n{self._html_to_markdown(self.recommendations)}" if self.recommendations else "")
        )


PROMPT = """\
You are an expert in bioinformatics, sequencing technologies, and genomics data analysis.
You are given key findings from a MultiQC report, generated by a bioinformatics workflow.
MultiQC consists of so called modules that support different tools that output QC metrics
(e.g. bclconvert, FastQC, samtools stats, bcftools stats, fastp, Picard, SnpEff, etc), and
it aggregates results from different tools. It outputs a "General Statistics" section that
has a table with a summary of key metrics from all modules across each sample. That table
is followed by module-specific sections that usually have more detail on the same samples.

You are given data from such a report, split by segeneratorur task is to analyse the data, and
give a short and concise overall summary for the results. Don't waste words: mention only
critical QC issues, and only for the samples that have those. Do not list every section, but only
mention the sections that worth attention.

Use limited HTML to format your reponse for readability: p and ul/li tags for paragraphs and lists,
and pre-defined .text-green, .text-red, and .text-yellow classes to highlight the severity of the issue.

Do no add headers or intro words, rather just get to the point. Separately add a short summary,
your analysis, and your recommendations. Use lists to format analysis and recommendations as well!

Example of summary of the general stats section:

<ul>
    <li>
        <span class="text-green">11/13 samples</span> show consistent metrics within expected ranges.
    </li>
    <li>
        <span class="text-red">A1001.2003</span> and <span class="text-red">A1200.2004</span> exhibit extremely
        high percentage of <span class="text-red">duplicates</span> (<span class="text-red">65.54%</span> and
        <span class="text-red">83.14%</span>, respectively) and the percentage of <span class="text-red">fails (33.33%)</span>.
        <span class="text-red">A1001.2003</span> also registers a very low <span class="text-red"> mapping rate (38.61%)</span>.
    </li>
    <li>
        <span class="text-yellow">A1002-1007</span> displays a relatively low percentage of <span class="text-yellow">valid pairs (48.08%)</span>
        and <span class="text-yellow">passed Di-Tags (22.51%)</span>.
    </li>
</ul>

Analysis:

<p>
    FastQC results indicate that <span class="text-red">A1001.2003</span> and <span class="text-red">A1001.2004</span>
    have a slight <span class="text-red">GC content</span> bias: at 39.5% against most other samples having 38.0%,
    which indicates a potential contamination that could be the source of other anomalities in quality metrics.
</p>

Recommendations:

<p>
    It is recommended to check the samples from the group <span class="text-red">A1001</span> for contamination,
    and consider removing them from the analysis.
</p>
"""


class Client:
    def __init__(self):
        self.name: str
        self.model: Optional[str] = None

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        raise NotImplementedError


class LangchainClient(Client):
    def __init__(self):
        self.name: str
        self.model: str
        self.llm: BaseChatModel

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        from langsmith import Client as LangSmithClient  # type: ignore
        from langchain_core.tracers.context import tracing_v2_enabled  # type: ignore

        with tracing_v2_enabled(
            project_name=os.environ.get("LANGCHAIN_PROJECT"),
            client=LangSmithClient(
                api_key=os.environ.get("LANGCHAIN_API_KEY"),
                api_url=os.environ.get("LANGCHAIN_ENDPOINT"),
            ),
        ):
            structured_llm = self.llm.with_structured_output(InterpretationOutput)
            return cast(
                InterpretationOutput,
                structured_llm.invoke(
                    [
                        {"role": "system", "content": PROMPT},
                        {"role": "user", "content": report_content},
                    ]
                ),
            )


class OpenAiClient(LangchainClient):
    def __init__(self):
        from langchain_openai import ChatOpenAI  # type: ignore

        super().__init__()
        self.name = "OpenAI"
        self.model = "gpt-4o"
        self.llm = ChatOpenAI(
            model=self.model,
            api_key=SecretStr(os.environ["OPENAI_API_KEY"]),
            temperature=0.0,
        )


class AnthropicClient(LangchainClient):
    def __init__(self):
        super().__init__()
        from langchain_anthropic import ChatAnthropic  # type: ignore

        self.name = "Anthropic"
        self.model = "claude-3-5-sonnet-20240620"
        self.llm = ChatAnthropic(
            model=self.model,  # type: ignore
            api_key=SecretStr(os.environ["ANTHROPIC_API_KEY"]),
            temperature=0.0,
        )  # type: ignore


class SeqeraClient(Client):
    def __init__(self):
        super().__init__()
        self.name = "Seqera Chat"
        token = os.environ.get("SEQERA_API_KEY", os.environ.get("TOWER_ACCESS_TOKEN"))
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'seqera', but Seqera tower access "
                "token is not set. Please set the TOWER_ACCESS_TOKEN environment variable, or change config.ai_provider"
            )
            return None
        self.url = os.environ.get("SEQERA_API_URL", "https://seqera.io")
        self.token = token

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        response = requests.post(
            f"{self.url}/interpret-multiqc-report",
            headers={"Authorization": f"Bearer {self.token}"} if self.token else {},
            json={
                "system_message": PROMPT,
                "report": report_content,
            },
        )
        if response.status_code != 200:
            logger.error(f"Failed to get a response from Seqera: {response.status_code} {response.text}")
            return None
        return InterpretationOutput(**response.json())


def get_llm_client() -> Optional[Client]:
    if not config.ai_summary:
        return None

    if config.ai_provider == "seqera":
        return SeqeraClient()

    if config.ai_provider == "anthropic":
        token = os.environ.get("ANTHROPIC_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'anthropic', but Anthropic API "
                "key not set. Please set the ANTHROPIC_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        return AnthropicClient()

    if config.ai_provider == "openai":
        token = os.environ.get("OPENAI_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'openai', but OpenAI API "
                "key not set. Please set the OPENAI_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        return OpenAiClient()

    return None


def add_ai_summary_to_report():
    if not (client := get_llm_client()):
        return

    content: str = ""  # data formatted for the LLM
    if report.general_stats_plot:
        content += f"""
Section: MultiQC General Statistics (Overview of key QC metrics for each sample)

{report.general_stats_plot.data_for_ai_prompt()}
"""

    for section in report.get_all_sections():
        if section.plot_anchor and section.plot_anchor in report.plot_by_id:
            plot = report.plot_by_id[section.plot_anchor]
            if plot_content := plot.data_for_ai_prompt():
                helptext = (
                    ("\n" + f"More detail about interpreting the data: {section.helptext}") if section.helptext else ""
                )
                description = f" ({section.description})" if section.description else ""
                content += f"""
Tool: {section.module} ({section.module_info})
Section: {section.name}{description}{helptext}

{plot_content}

----------------------
"""

    if not content:
        return

    interpretation = client.interpret_report(content)

    if not interpretation:
        return None

    disclaimer = f"This summary is AI-generated. Take with a grain of salt. LLM provider: {client.name}"
    if client.model:
        disclaimer += f", model: {client.model}"

    continue_chat = ""
    website_url = os.environ.get("SEQERA_WEBSITE", "https://seqera.io")
    messages = [
        {"role": "user", "content": content},
        {"role": "assistant", "content": interpretation.format_text()},
    ]
    messages_json = json.dumps(messages)
    encoded_chat_messages = base64.b64encode(messages_json.encode("utf-8")).decode("utf-8")
    encoded_system_message = base64.b64encode(PROMPT.encode("utf-8")).decode("utf-8")
    key = f"data-key={key} " if (key := os.environ.get("SEQERA_CHAT_KEY")) else ""

    continue_chat = (
        "<button class='btn btn-primary'"
        + "id='continue-in-chat'"
        + f" data-encoded-system-message={encoded_system_message}"
        + f" data-encoded-chat-messages={encoded_chat_messages}"
        + f" data-website={website_url}"
        + f" {key}"
        + ">Continue with Seqera Chat</button>"
    )

    report.ai_summary = f"""
    <details>
    {interpretation.format_html()}
    <p style="color: gray; font-style: italic">{disclaimer}</p>
    {continue_chat}
    </details>
    """

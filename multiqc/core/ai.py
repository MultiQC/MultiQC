import base64
import json
import logging
import os
from textwrap import dedent
from typing import Optional, cast, TYPE_CHECKING
from pydantic import BaseModel, Field
import requests

from pydantic.types import SecretStr
from bs4 import BeautifulSoup
from dotenv import load_dotenv  # type: ignore

from multiqc import config, report

if TYPE_CHECKING:
    from langchain_core.language_models.chat_models import BaseChatModel  # type: ignore


load_dotenv()

logger = logging.getLogger(__name__)


class InterpretationOutput(BaseModel):
    summary: str = Field(description="Summary")
    short_abstract: str = Field(description="Short abstract of the summary")
    recommendations: Optional[str] = Field(default=None, description="Recommendations for the next steps")

    def _html_to_markdown(self, text: str) -> str:
        soup = BeautifulSoup(text, "html.parser")
        for tag in soup.find_all(["ul", "li", "b", "p", "sample", "span"]):
            if tag.name == "ul":
                tag.insert_before("\n")
                tag.insert_after("\n")
            elif tag.name == "li":
                tag.insert_before("- ")
                tag.insert_after("\n")
            elif tag.name == "b":
                tag.insert_before("**")
                tag.insert_after("**")
            elif tag.name == "p":
                tag.insert_before("\n")
                tag.insert_after("\n")
            elif tag.name == "sample":
                clz = tag.get("class")
                if clz:
                    tag.insert_before(":span[")
                    tag.insert_after(f"]{{.{clz}}}")
            elif tag.name == "span":
                clz = tag.get("class")
                if clz:
                    tag.insert_before(":span[")
                    tag.insert_after(f"]{{.{clz}}}")
        return soup.get_text()

    def format_html(self) -> str:
        html = dedent(f"""
        <summary>
        <b>âœ¨ AI Summary</b>
        {self.short_abstract}
        </summary>
        <p>
        <b>Detailed summary</b> {self.summary}
        </p>
        """)
        if self.recommendations:
            html += dedent(f"""
            <p>
            <b>Recommendations</b> {self.recommendations}
            </p>""")
        return html

    def format_text(self) -> str:
        """
        Format to markdown to display in Seqera Chat
        """
        return (
            f"## Summary\n{self._html_to_markdown(self.short_abstract)}"
            + (f"## Detailed summary\n{self._html_to_markdown(self.summary)}")
            + (f"## Recommendations\n{self._html_to_markdown(self.recommendations)}" if self.recommendations else "")
        )


PROMPT = """\
You are an expert in bioinformatics, sequencing technologies, and genomics data analysis.
You are given key findings from a MultiQC report, generated by a bioinformatics workflow.
MultiQC consists of so called modules that support different tools that output QC metrics
(e.g. bclconvert, FastQC, samtools stats, bcftools stats, fastp, Picard, SnpEff, etc), and
it aggregates results from different tools. It outputs a "General Statistics" section that
has a table with a summary of key metrics from all modules across each sample. That table
is followed by module-specific sections that usually have more detail on the same samples.

You are given data from such a report, split by section. Your task is to analyse the data, and
give a short and concise overall summary for the results, followed by an even more concise abstract. 
Don't waste words: mention only the important QC issues. Only mention the sections that worth 
attention. If there are no issues, just say so.

Use limited HTML to format your reponse for readability: p and ul/li tags for paragraphs and lists.
Use pre-defined .text-green, .text-red, and .text-yellow classes to highlight the severity of the issue.
Wrap sample names in <sample> tags, make sure to add one of .text-green, .text-red, .text-yellow classes.

After the summary, add a very short abstract of the summary, limited to 1-2 bullet points. Highlight
sample names with the pre-defined classes as well.

Finally, add your recommendations for the next steps.

Example:

summary: |-
    <ul>
        <li>
            <sample class="text-yellow">A1002</sample> and <sample class="text-yellow">A1003</sample> groups (<span class="text-green">11/13 samples</span>)
            show good quality metrics, with consistent GC content (38-39%), read lengths (125 bp), and acceptable levels of duplicates and valid pairs.
        </li>
        <li><sample class="text-red">A1001.2003</sample> and <sample class="text-red">A1001.2004</sample> show severe quality issues:
            <ul>
                <li>Extremely high duplicate rates (<span class="text-red">65.54%</span> and <span class="text-red">83.14%</span>)</li>
                <li>Low percentages of valid pairs (<span class="text-red">37.2%</span> and <span class="text-red">39.2%</span>)</li>
                <li>High percentages of failed modules in FastQC (<span class="text-red">33.33%</span>)</li>
                <li>Significantly higher total sequence counts (<span class="text-red">141.9M</span> and <span class="text-red">178.0M</span>) compared to other samples</li>
                <li>FastQC results indicate that <sample class="text-red">A1001.2003</sample> and <sample class="text-red">A1001.2004</span>
                    have a slight <span class="text-red">GC content</span> bias: at 39.5% against most other samples having 38.0%,
                    which indicates a potential contamination that could be the source of other anomalities in quality metrics.
                </li>
            </ul>
        </li>
        <li>
            <sample class="text-yellow">A1002-1007</sample> shows some quality concerns:
            <ul>
                <li>Low percentage of valid pairs (<span class="text-yellow">48.08%</span>)</li>
                <li>Low percentage of passed Di-Tags (<span class="text-yellow">22.51%</span>)</li>
            </ul>
        </li>
        <li>
            Overrepresented sequences analysis reveals adapter contamination in several samples, particularly in 
            <sample class="text-yellow">A1001.2003</sample> (up to <span class="text-yellow">35.82%</span> in Read 1).
        </li>
        <li>
            HiCUP analysis shows that most samples have acceptable levels of valid pairs, with <sample class="text-green">A1003</sample>
            group generally performing better than <sample class="text-yellow">A1002</sample> group.
        </li>
    </ul>

short_abstract: |-
    <ul>
        <li>
            <span class="text-green">11/13 samples</span> show consistent metrics within expected ranges.
        </li>
        <li>
            <sample class="text-red">A1001.2003</sample> and <sample class="text-red">A1001.2004</sample> exhibit extremely
            high percentage of <span class="text-red">duplicates</span> (<span class="text-red">65.54%</span> and
            <span class="text-red">83.14%</span>, respectively).
        </li>
    </ul>

recommendations: |-
    <ul>
        <li>Remove <sample class="text-red">A1001.2003</sample> and <sample class="text-red">A1200.2004</sample> from further analysis due to severe quality issues.</li>
        <li>Investigate the cause of low valid pairs and passed Di-Tags in <sample class="text-yellow">A1002-1007</sample>. Consider removing it if the issue cannot be resolved.</li>
        <li>Perform adapter trimming on all samples, particularly focusing on <sample class="text-red">A1001</sample> group.</li>
        <li>Re-run the Hi-C analysis pipeline after removing problematic samples and performing adapter trimming.</li>
        <li>Investigate the cause of higher duplication rates in <sample class="text-yellow">A1002</sample> group compared to <sample class="text-green">A1003</sample> group, although they are still within acceptable ranges.</li>
        <li>Consider adjusting the Hi-C protocol or library preparation steps to improve the percentage of valid pairs, especially for <sample class="text-yellow">A1002</sample> group.</li>
    </ul>
"""


class Client:
    def __init__(self):
        self.name: str
        self.model: Optional[str] = None

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        raise NotImplementedError


class LangchainClient(Client):
    def __init__(self):
        self.name: str
        self.model: str
        self.llm: BaseChatModel

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        from langsmith import Client as LangSmithClient  # type: ignore
        from langchain_core.tracers.context import tracing_v2_enabled  # type: ignore

        with tracing_v2_enabled(
            project_name=os.environ.get("LANGCHAIN_PROJECT"),
            client=LangSmithClient(
                api_key=os.environ.get("LANGCHAIN_API_KEY"),
                api_url=os.environ.get("LANGCHAIN_ENDPOINT"),
            ),
        ):
            structured_llm = self.llm.with_structured_output(InterpretationOutput)
            return cast(
                InterpretationOutput,
                structured_llm.invoke(
                    [
                        {"role": "system", "content": PROMPT},
                        {"role": "user", "content": report_content},
                    ]
                ),
            )


class OpenAiClient(LangchainClient):
    def __init__(self):
        from langchain_openai import ChatOpenAI  # type: ignore

        super().__init__()
        self.name = "OpenAI"
        self.model = "gpt-4o"
        self.llm = ChatOpenAI(
            model=self.model,
            api_key=SecretStr(os.environ["OPENAI_API_KEY"]),
            temperature=0.0,
        )


class AnthropicClient(LangchainClient):
    def __init__(self):
        super().__init__()
        from langchain_anthropic import ChatAnthropic  # type: ignore

        self.name = "Anthropic"
        self.model = "claude-3-5-sonnet-20240620"
        self.llm = ChatAnthropic(
            model=self.model,  # type: ignore
            api_key=SecretStr(os.environ["ANTHROPIC_API_KEY"]),
            temperature=0.0,
        )  # type: ignore


class SeqeraClient(Client):
    def __init__(self):
        super().__init__()
        self.name = "Seqera Chat"
        token = os.environ.get("SEQERA_API_KEY", os.environ.get("TOWER_ACCESS_TOKEN"))
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'seqera', but Seqera tower access "
                "token is not set. Please set the TOWER_ACCESS_TOKEN environment variable, or change config.ai_provider"
            )
            return None
        self.url = os.environ.get("SEQERA_API_URL", "https://seqera.io")
        self.token = token

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        response = requests.post(
            f"{self.url}/interpret-multiqc-report",
            headers={"Authorization": f"Bearer {self.token}"} if self.token else {},
            json={
                "system_message": PROMPT,
                "report": report_content,
            },
        )
        if response.status_code != 200:
            logger.error(f"Failed to get a response from Seqera: {response.status_code} {response.text}")
            return None
        return InterpretationOutput(**response.json())


def get_llm_client() -> Optional[Client]:
    if not config.ai_summary:
        return None

    if config.ai_provider == "seqera":
        return SeqeraClient()

    if config.ai_provider == "anthropic":
        token = os.environ.get("ANTHROPIC_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'anthropic', but Anthropic API "
                "key not set. Please set the ANTHROPIC_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        return AnthropicClient()

    if config.ai_provider == "openai":
        token = os.environ.get("OPENAI_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'openai', but OpenAI API "
                "key not set. Please set the OPENAI_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        return OpenAiClient()

    return None


def add_ai_summary_to_report():
    if not (client := get_llm_client()):
        return

    content: str = ""  # data formatted for the LLM
    if report.general_stats_plot:
        content += f"""
Section: MultiQC General Statistics (Overview of key QC metrics for each sample)

{report.general_stats_plot.data_for_ai_prompt()}
"""

    for section in report.get_all_sections():
        if section.plot_anchor and section.plot_anchor in report.plot_by_id:
            plot = report.plot_by_id[section.plot_anchor]
            if plot_content := plot.data_for_ai_prompt():
                helptext = (
                    ("\n" + f"More detail about interpreting the data: {section.helptext}") if section.helptext else ""
                )
                description = f" ({section.description})" if section.description else ""
                content += f"""
Tool: {section.module} ({section.module_info})
Section: {section.name}{description}{helptext}

{plot_content}

----------------------
"""

    if not content:
        return

    interpretation = client.interpret_report(content)
    logger.debug(f"Interpretation: {interpretation}")

    if not interpretation:
        return None

    disclaimer = f"This summary is AI-generated. Take with a grain of salt. LLM provider: {client.name}"
    if client.model:
        disclaimer += f", model: {client.model}"

    continue_chat = ""
    website_url = os.environ.get("SEQERA_WEBSITE", "https://seqera.io")
    messages = [
        {"role": "user", "content": content},
        {"role": "assistant", "content": interpretation.format_text()},
    ]
    messages_json = json.dumps(messages)
    encoded_chat_messages = base64.b64encode(messages_json.encode("utf-8")).decode("utf-8")
    encoded_system_message = base64.b64encode(PROMPT.encode("utf-8")).decode("utf-8")
    key = f"data-key={key} " if (key := os.environ.get("SEQERA_CHAT_KEY")) else ""

    continue_chat = (
        "<button class='btn btn-primary'"
        + "id='continue-in-chat'"
        + f" data-encoded-system-message={encoded_system_message}"
        + f" data-encoded-chat-messages={encoded_chat_messages}"
        + f" data-website={website_url}"
        + f" {key}"
        + ">Continue with Seqera Chat</button>"
    )

    report.ai_summary = f"""
    <details>
    {interpretation.format_html()}
    <p style="color: gray; font-style: italic">{disclaimer}</p>
    {continue_chat}
    </details>
    """

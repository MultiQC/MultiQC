from ast import arg
import base64
import json
import logging
import os
from abc import abstractmethod
from textwrap import dedent
from typing import Dict, List, Optional, cast
from langchain_core.language_models.chat_models import BaseChatModel
from pydantic import BaseModel, Field
import requests

from pydantic.types import SecretStr
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.tracers.context import tracing_v2_enabled
from langsmith import Client as LangSmithClient

from bs4 import BeautifulSoup

from multiqc import config, report

from dotenv import load_dotenv  # type: ignore

load_dotenv()

logger = logging.getLogger(__name__)


class InterpretationOutput(BaseModel):
    summary: str = Field(description="A short and concise summary")
    analysis: Optional[str] = Field(default=None, description="An analysis of the results")
    recommendations: Optional[str] = Field(default=None, description="Recommendations for the next steps")

    def _html_to_markdown(self, text: str) -> str:
        soup = BeautifulSoup(text, "html.parser")
        for tag in soup.find_all(["ul", "li", "b", "p"]):
            if tag.name == "ul":
                tag.insert_before("\n")
                tag.insert_after("\n")
            elif tag.name == "li":
                tag.insert_before("- ")
                tag.insert_after("\n")
            elif tag.name == "b":
                tag.insert_before("**")
                tag.insert_after("**")
            elif tag.name == "p":
                tag.insert_before("\n")
                tag.insert_after("\n")
        return soup.get_text()

    def format_html(self) -> str:
        return dedent(f"""
        <summary>
        <b>âœ¨ AI Summary</b>
        {self.summary}
        </summary>
        <p>
        <b>Analysis</b> {self.analysis}
        </p>
        <p>
        <b>Recommendations</b> {self.recommendations}
        </p>""")

    def format_text(self) -> str:
        """
        Format to markdown to display in Seqera Chat
        """
        return (
            f"## Summary\n{self._html_to_markdown(self.summary)}"
            + (f"## Analysis\n{self._html_to_markdown(self.analysis)}" if self.analysis else "")
            + (f"## Recommendations\n{self._html_to_markdown(self.recommendations)}" if self.recommendations else "")
        )


PROMPT = """\
You are an expert in bioinformatics, sequencing technologies, and genomics data analysis.
You are given key findings from a MultiQC report, generated by a bioinformatics workflow.
MultiQC consists of so called modules that support different tools that output QC metrics
(e.g. bclconvert, FastQC, samtools stats, bcftools stats, fastp, Picard, SnpEff, etc), and
it aggregates results from different tools. It outputs a "General Statistics" section that
has a table with a summary of key metrics from all modules across each sample. That table
is followed by module-specific sections that usually have more detail on the same samples.

You are given data from such a report, split by segeneratorur task is to analyse the data, and
give a short and concise overall summary for the results. Don't waste words: mention only
critical QC issues, and only for the samples that have those. Do not list every section, but only
mention the sections that worth attention.

Use limited HTML to format your reponse for readability: p and ul/li tags for paragraphs and lists,
and pre-defined .good, .bad, and .warning classes to highlight the severity of the issue.

Do no add headers or intro words, rather just get to the point. Separately add a short summary,
your analysis, and your recommendations. Use lists to format analysis and recommendations as well!

Example of summary of the general stats section:

<ul>
    <li>
        <span class="good">11/13 samples</span> show consistent metrics within expected ranges.
    </li>
    <li>
        <span class="bad">A1001.2003</span> and <span class="bad">A1200.2004</span> exhibit extremely
        high percentage of <span class="bad">duplicates</span> (<span class="bad">65.54%</span> and
        <span class="bad">83.14%</span>, respectively) and the percentage of <span class="bad">fails (33.33%)</span>.
        <span class="bad">A1001.2003</span> also registers a very low <span class="bad"> mapping rate (38.61%)</span>.
    </li>
    <li>
        <span class="warning">A1002-1007</span> displays a relatively low percentage of <span class="warning">valid pairs (48.08%)</span>
        and <span class="warning">passed Di-Tags (22.51%)</span>.
    </li>
</ul>

Analysis:

<p>
    FastQC results indicate that <span class="bad">A1001.2003</span> and <span class="bad">A1001.2004</span>
    have a slight <span class="bad">GC content</span> bias: at 39.5% against most other samples having 38.0%,
    which indicates a potential contamination that could be the source of other anomalities in quality metrics.
</p>

Recommendations:

<p>
    It is recommended to check the samples from the group <span class="bad">A1001</span> for contamination,
    and consider removing them from the analysis.
</p>
"""


class Client:
    def __init__(self):
        self.name: str
        self.model: Optional[str] = None
        self.llm: BaseChatModel

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        return _interpret_with_llm(self.llm, report_content)


def _interpret_with_llm(llm: BaseChatModel, report_content: str) -> Optional[InterpretationOutput]:
    structured_llm = llm.with_structured_output(InterpretationOutput)
    return cast(
        InterpretationOutput,
        structured_llm.invoke(
            [
                {"role": "system", "content": PROMPT},
                {"role": "user", "content": report_content},
            ]
        ),
    )


class OpenAiClient(Client):
    def __init__(self):
        super().__init__()
        self.name = "OpenAI"
        self.model = "gpt-4o"
        self.llm = ChatOpenAI(
            model=self.model,
            api_key=SecretStr(os.environ["OPENAI_API_KEY"]),
            temperature=0.0,
        )

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        return _interpret_with_llm(self.llm, report_content)


class AnthropicClient(Client):
    def __init__(self):
        super().__init__()
        self.name = "Anthropic"
        self.model = "claude-3-5-sonnet-20240620"
        self.llm = ChatAnthropic(
            model=self.model,  # type: ignore
            api_key=SecretStr(os.environ["ANTHROPIC_API_KEY"]),
            temperature=0.0,
        )  # type: ignore

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        return _interpret_with_llm(self.llm, report_content)


class SeqeraClient(Client):
    def __init__(self):
        super().__init__()
        self.name = "Seqera Chat"
        token = os.environ.get("SEQERA_API_KEY", os.environ.get("TOWER_ACCESS_TOKEN"))
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'seqera', but Seqera tower access "
                "token is not set. Please set the TOWER_ACCESS_TOKEN environment variable, or change config.ai_provider"
            )
            return None
        self.url = os.environ.get("SEQERA_API_URL", "https://seqera.io")
        self.token = token

    def interpret_report(self, report_content: str) -> Optional[InterpretationOutput]:
        response = requests.post(
            f"{self.url}/interpret-multiqc-report",
            headers={"Authorization": f"Bearer {self.token}"} if self.token else {},
            json={"report": report_content},
        )
        if response.status_code != 200:
            logger.error(f"Failed to get a response from Seqera: {response.status_code} {response.text}")
            return None
        return InterpretationOutput(**response.json())


def get_llm_client() -> Optional[Client]:
    if not config.ai_summary:
        return None

    if config.ai_provider == "seqera":
        return SeqeraClient()

    if config.ai_provider == "anthropic":
        token = os.environ.get("ANTHROPIC_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'anthropic', but Anthropic API "
                "key not set. Please set the ANTHROPIC_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        return AnthropicClient()

    if config.ai_provider == "openai":
        token = os.environ.get("OPENAI_API_KEY")
        if not token:
            logger.error(
                "config.ai_summary is set to true, and config.ai_provider is set to 'openai', but OpenAI API "
                "key not set. Please set the OPENAI_API_KEY environment variable, or change config.ai_provider"
            )
            return None
        return OpenAiClient()

    return None


def add_ai_summary_to_report():
    if not (client := get_llm_client()):
        return

    content: str = ""  # data formatted for the LLM
    if report.general_stats_plot:
        content += dedent(f"""
            Section: MultiQC General Statistics (Overview of key QC metrics for each sample)

            {report.general_stats_plot.data_for_ai_prompt()}
            """)

    for section in report.get_all_sections():
        if section.plot_anchor and section.plot_anchor in report.plot_by_id:
            plot = report.plot_by_id[section.plot_anchor]
            if plot_content := plot.data_for_ai_prompt():
                content += dedent(f"""
                    Tool: {section.module} ({section.module_info})
                    Section: {section.name} {f"({section.description})" if section.description else ""}
                    {("\n" + f"More detail about interpreting the data: {section.helptext}") if section.helptext else ""}

                    {plot_content}

                    ----------------------
                """)

    if not content:
        return

    if client.name != "Seqera Chat":
        with tracing_v2_enabled(
            project_name=os.environ.get("LANGCHAIN_PROJECT"),
            client=LangSmithClient(
                api_key=os.environ.get("LANGCHAIN_API_KEY"),
                api_url=os.environ.get("LANGCHAIN_ENDPOINT"),
            ),
        ):
            interpretation = client.interpret_report(content)
    else:
        interpretation = client.interpret_report(content)

    if not interpretation:
        return None

    disclaimer = f"This summary is AI-generated. Take with a grain of salt. LLM provider: {client.name}"
    if client.model:
        disclaimer += f", model: {client.model}"

    continue_chat = ""
    if url := os.environ.get("SEQERA_CHAT_URL"):
        messages = [
            {"role": "user", "content": content},
            {"role": "assistant", "content": interpretation.format_text()},
        ]
        history_json = json.dumps(messages)
        history_base64 = base64.b64encode(history_json.encode("utf-8")).decode("utf-8")
        continue_chat = (
            "<button class='btn btn-primary'"
            + "id='continue-in-chat' "
            + f"data-messages={history_base64} "
            + f"data-url={url}>"
            + "Continue with Seqera Chat"
            "</button>"
        )

    report.ai_summary = f"""
    <details>
    {interpretation.format_html()}
    <p style="color: gray; font-style: italic">{disclaimer}</p>
    {continue_chat}
    </details>
    """

import logging
import re

from multiqc.base_module import BaseMultiqcModule, ModuleNoSamplesFound
from multiqc.plots import bargraph, linegraph, violin

log = logging.getLogger(__name__)


class MultiqcModule(BaseMultiqcModule):
    """
    The module parses logs generated by the BISCUIT quality control script, `QC.sh`, which wraps
    `biscuit qc` and `biscuit qc_coverage` and adds an extra metric of base-averaged cytosine
    retention. It will search for all files output from `QC.sh`, though the user may run
    `biscuit qc` or `biscuit qc_coverage` separately, if desired.

    **Note**: As of MultiQC v1.9, the module supports only BISCUIT version v0.3.16 and onwards.
    If you have BISCUIT data from before this, please use MultiQC v1.8.

    #### Insert Size Distribution

    The second tab of this plot uses the config option `read_count_multiplier`,
    so if millions of reads is not useful for your data you can customise this.

    See [Number base (multiplier)](https://docs.seqera.io/multiqc/reports/customisation#number-base-multiplier)
    in the documentation.
    """

    def __init__(self):
        super(MultiqcModule, self).__init__(
            name="BISCUIT",
            anchor="biscuit",
            href="https://github.com/huishenlab/biscuit",
            info="Maps bisulfite converted DNA sequence reads and determines cytosine methylation states.",
            doi="10.1093/nar/gkae097",
        )

        # Set up data structures for collected data
        self.biscuit_data = {
            # General alignment statistics
            "align_mapq": {},
            "align_strand": {},
            "align_isize": {},
            # Duplicate reporting
            "dup_report": {},
            # Uniformity
            "qc_cv": {},
            # Base coverage
            "covdist_all_base_botgc": {},
            "covdist_all_base": {},
            "covdist_all_base_topgc": {},
            "covdist_q40_base_botgc": {},
            "covdist_q40_base": {},
            "covdist_q40_base_topgc": {},
            # CpG coverage
            "covdist_all_cpg_botgc": {},
            "covdist_all_cpg": {},
            "covdist_all_cpg_topgc": {},
            "covdist_q40_cpg_botgc": {},
            "covdist_q40_cpg": {},
            "covdist_q40_cpg_topgc": {},
            # Cytosine retention
            "cpg_retention_readpos": {},
            "cph_retention_readpos": {},
            "base_avg_retention_rate": {},
            "read_avg_retention_rate": {},
        }

        # Mapping quality distribution
        align_mapq = {}
        for f in self.find_log_files("biscuit/align_mapq"):
            parsed_data = parse_align_mapq(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in align_mapq:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="align_mapq")
                align_mapq[s_name] = parsed_data

        align_mapq = self.ignore_samples(align_mapq)
        self.biscuit_data["align_mapq"] = align_mapq

        # Bisulfite strand (OT/CTOT + OB/CTOB) distribution
        align_strand = {}
        for f in self.find_log_files("biscuit/align_strand"):
            parsed_data = parse_align_strand(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in align_strand:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="align_strand")
                align_strand[s_name] = parsed_data

        align_strand = self.ignore_samples(align_strand)
        self.biscuit_data["align_strand"] = align_strand

        # Insert size distribution
        align_isize = {}
        for f in self.find_log_files("biscuit/align_isize"):
            parsed_data = parse_align_isize(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in align_isize:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="align_isize")
                align_isize[s_name] = parsed_data

        align_isize = self.ignore_samples(align_isize)
        self.biscuit_data["align_isize"] = align_isize

        # Insert size distribution
        dup_report = {}
        for f in self.find_log_files("biscuit/dup_report"):
            parsed_data = parse_dup_report(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in dup_report:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="dup_report")
                dup_report[s_name] = parsed_data

        dup_report = self.ignore_samples(dup_report)
        self.biscuit_data["dup_report"] = dup_report

        # Coefficient of variation table
        qc_cv = {}
        for f in self.find_log_files("biscuit/qc_cv"):
            parsed_data = parse_qc_cv(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in qc_cv:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="qc_cv")
                qc_cv[s_name] = parsed_data

        qc_cv = self.ignore_samples(qc_cv)
        self.biscuit_data["qc_cv"] = qc_cv

        # Coverage distribution - all bases, bottom GC-content
        n_covdist_samples = 0
        covdist_all_base_botgc = {}
        for f in self.find_log_files("biscuit/covdist_all_base_botgc"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_all_base_botgc:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_all_base_botgc")
                covdist_all_base_botgc[s_name] = parsed_data

        covdist_all_base_botgc = self.ignore_samples(covdist_all_base_botgc)
        self.biscuit_data["covdist_all_base_botgc"] = covdist_all_base_botgc
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - all bases
        covdist_all_base = {}
        for f in self.find_log_files("biscuit/covdist_all_base"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_all_base:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_all_base")
                covdist_all_base[s_name] = parsed_data

        covdist_all_base = self.ignore_samples(covdist_all_base)
        self.biscuit_data["covdist_all_base"] = covdist_all_base
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - all bases, top GC-content
        covdist_all_base_topgc = {}
        for f in self.find_log_files("biscuit/covdist_all_base_topgc"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_all_base_topgc:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_all_base_topgc")
                covdist_all_base_topgc[s_name] = parsed_data

        covdist_all_base_topgc = self.ignore_samples(covdist_all_base_topgc)
        self.biscuit_data["covdist_all_base_topgc"] = covdist_all_base_topgc
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - q40 bases, bottom GC-content
        covdist_q40_base_botgc = {}
        for f in self.find_log_files("biscuit/covdist_q40_base_botgc"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_q40_base_botgc:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_q40_base_botgc")
                covdist_q40_base_botgc[s_name] = parsed_data

        covdist_q40_base_botgc = self.ignore_samples(covdist_q40_base_botgc)
        self.biscuit_data["covdist_q40_base_botgc"] = covdist_q40_base_botgc
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - q40 bases
        covdist_q40_base = {}
        for f in self.find_log_files("biscuit/covdist_q40_base"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_q40_base:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_q40_base")
                covdist_q40_base[s_name] = parsed_data

        covdist_q40_base = self.ignore_samples(covdist_q40_base)
        self.biscuit_data["covdist_q40_base"] = covdist_q40_base
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - q40 bases, top GC-content
        covdist_q40_base_topgc = {}
        for f in self.find_log_files("biscuit/covdist_q40_base_topgc"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_q40_base_topgc:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_q40_base_topgc")
                covdist_q40_base_topgc[s_name] = parsed_data

        covdist_q40_base_topgc = self.ignore_samples(covdist_q40_base_topgc)
        self.biscuit_data["covdist_q40_base_topgc"] = covdist_q40_base_topgc
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - all cpgs, bottom GC-content
        covdist_all_cpg_botgc = {}
        for f in self.find_log_files("biscuit/covdist_all_cpg_botgc"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_all_cpg_botgc:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_all_cpg_botgc")
                covdist_all_cpg_botgc[s_name] = parsed_data

        covdist_all_cpg_botgc = self.ignore_samples(covdist_all_cpg_botgc)
        self.biscuit_data["covdist_all_cpg_botgc"] = covdist_all_cpg_botgc
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - all cpgs
        covdist_all_cpg = {}
        for f in self.find_log_files("biscuit/covdist_all_cpg"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_all_cpg:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_all_cpg")
                covdist_all_cpg[s_name] = parsed_data

        covdist_all_cpg = self.ignore_samples(covdist_all_cpg)
        self.biscuit_data["covdist_all_cpg"] = covdist_all_cpg
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - all cpgs, top GC-content
        covdist_all_cpg_topgc = {}
        for f in self.find_log_files("biscuit/covdist_all_cpg_topgc"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_all_cpg_topgc:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_all_cpg_topgc")
                covdist_all_cpg_topgc[s_name] = parsed_data

        covdist_all_cpg_topgc = self.ignore_samples(covdist_all_cpg_topgc)
        self.biscuit_data["covdist_all_cpg_topgc"] = covdist_all_cpg_topgc
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - q40 cpgs, bottom GC-content
        covdist_q40_cpg_botgc = {}
        for f in self.find_log_files("biscuit/covdist_q40_cpg_botgc"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_q40_cpg_botgc:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_q40_cpg_botgc")
                covdist_q40_cpg_botgc[s_name] = parsed_data

        covdist_q40_cpg_botgc = self.ignore_samples(covdist_q40_cpg_botgc)
        self.biscuit_data["covdist_q40_cpg_botgc"] = covdist_q40_cpg_botgc
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - q40 cpgs
        covdist_q40_cpg = {}
        for f in self.find_log_files("biscuit/covdist_q40_cpg"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_q40_cpg:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_q40_cpg")
                covdist_q40_cpg[s_name] = parsed_data

        covdist_q40_cpg = self.ignore_samples(covdist_q40_cpg)
        self.biscuit_data["covdist_q40_cpg"] = covdist_q40_cpg
        n_covdist_samples += len(covdist_all_base_botgc)

        # Coverage distribution - q40 cpgs, top GC-content
        covdist_q40_cpg_topgc = {}
        for f in self.find_log_files("biscuit/covdist_q40_cpg_topgc"):
            parsed_data = parse_covdist(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in covdist_q40_cpg_topgc:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="covdist_q40_cpg_topgc")
                covdist_q40_cpg_topgc[s_name] = parsed_data

        covdist_q40_cpg_topgc = self.ignore_samples(covdist_q40_cpg_topgc)
        self.biscuit_data["covdist_q40_cpg_topgc"] = covdist_q40_cpg_topgc
        n_covdist_samples += len(covdist_all_base_botgc)

        # CpG Retention by Position in Read
        cpg_retention_readpos = {}
        for f in self.find_log_files("biscuit/cpg_retention_readpos"):
            parsed_data = parse_retention_readpos(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in cpg_retention_readpos:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="cpg_retention_readpos")
                cpg_retention_readpos[s_name] = parsed_data

        cpg_retention_readpos = self.ignore_samples(cpg_retention_readpos)
        self.biscuit_data["cpg_retention_readpos"] = cpg_retention_readpos

        # CpH Retention by Position in Read
        cph_retention_readpos = {}
        for f in self.find_log_files("biscuit/cph_retention_readpos"):
            parsed_data = parse_retention_readpos(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in cph_retention_readpos:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="cph_retention_readpos")
                cph_retention_readpos[s_name] = parsed_data

        cph_retention_readpos = self.ignore_samples(cph_retention_readpos)
        self.biscuit_data["cph_retention_readpos"] = cph_retention_readpos
        n_retention_readpos = len(cpg_retention_readpos) + len(cph_retention_readpos)

        # Base-averaged cytosine retention rate
        base_avg_retention_rate = {}
        for f in self.find_log_files("biscuit/base_avg_retention_rate"):
            parsed_data = parse_avg_retention(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in base_avg_retention_rate:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="base_avg_retention_rate")
                base_avg_retention_rate[s_name] = parsed_data

        base_avg_retention_rate = self.ignore_samples(base_avg_retention_rate)
        self.biscuit_data["base_avg_retention_rate"] = base_avg_retention_rate

        # Read-averaged cytosine retention rate
        read_avg_retention_rate = {}
        for f in self.find_log_files("biscuit/read_avg_retention_rate"):
            parsed_data = parse_avg_retention(f["f"], f["fn"])
            if parsed_data is not None:
                s_name = self.clean_s_name(f["s_name"], f)

                if s_name in read_avg_retention_rate:
                    log.debug(f"Duplicate sample name found in {f['fn']}! Overwriting: {s_name}")

                # Add source file to multiqc_sources.txt
                self.add_data_source(f, s_name=s_name, section="read_avg_retention_rate")
                read_avg_retention_rate[s_name] = parsed_data

        read_avg_retention_rate = self.ignore_samples(read_avg_retention_rate)
        self.biscuit_data["read_avg_retention_rate"] = read_avg_retention_rate
        n_avg_retention = len(base_avg_retention_rate) + len(read_avg_retention_rate)

        n_samples = max([len(self.biscuit_data[k]) for k in self.biscuit_data.keys()])
        if n_samples == 0:
            raise ModuleNoSamplesFound
        log.info(f"Found {n_samples} samples")

        # Setup general statistics table columns
        self.biscuit_stats_table()

        # Superfluous function call to confirm that it is used in this module
        # Replace None with actual version if it is available
        self.add_software_version(None)

        # Make report sections
        if len(self.biscuit_data["align_mapq"]) > 0:
            log.info(f"Found {len(self.biscuit_data['align_mapq'])} BISCUIT MAPQ reports")
            self.chart_align_mapq()
        if len(self.biscuit_data["align_strand"]) > 0:
            log.info(f"Found {len(self.biscuit_data['align_strand'])} BISCUIT bisulfite strand reports")
            self.chart_align_strand()
        if len(self.biscuit_data["align_isize"]) > 0:
            log.info(f"Found {len(self.biscuit_data['align_isize'])} BISCUIT insert size reports")
            self.chart_align_isize()
        if len(self.biscuit_data["dup_report"]) > 0:
            log.info(f"Found {len(self.biscuit_data['dup_report'])} BISCUIT duplicate reports")
            self.chart_dup_report()
        if len(self.biscuit_data["qc_cv"]) > 0:
            log.info(f"Found {len(self.biscuit_data['qc_cv'])} BISCUIT coefficient of variation reports")
            self.chart_qc_cv()
        if n_covdist_samples > 0:
            log.info(f"Found {n_covdist_samples} BISCUIT coverage distribution reports")
            self.chart_covdist()
        if n_retention_readpos > 0:
            log.info(f"Found {n_retention_readpos} BISCUIT retention by read position reports")
            self.chart_retention_readpos()
        if n_avg_retention > 0:
            log.info(f"Found {n_avg_retention} BISCUIT average cytosine retention reports")
            self.chart_avg_retention()

        # Write data to file
        self.write_data_file(self.biscuit_data, "biscuit", data_format="yaml")

    def biscuit_stats_table(self):
        """BISCUIT data for the general statistics table"""
        pd = {}

        # Calculate percent aligned
        for s_name, data in self.biscuit_data["align_mapq"].items():
            pd[s_name] = {"aligned": data["frac_align"]}

        # Calculate percent duplicated
        for s_name, data in self.biscuit_data["dup_report"].items():
            if s_name not in pd:
                pd[s_name] = {}
            if "all" in data:
                pd[s_name]["dup_all"] = data["all"]
            if "q40" in data:
                pd[s_name]["dup_q40"] = data["q40"]

        header = {
            "aligned": {
                "title": "% Aligned",
                "description": "Percentage of Reads Aligned",
                "min": 0,
                "max": 100,
                "scale": "YlGn",
                "suffix": "%",
                "format": "{:,.2f}",
                "hidden": False,
            },
            "dup_q40": {
                "title": "Dup. % for Q40 Reads",
                "description": "Percentage of Duplicate Reads with MAPQ >= 40",
                "min": 0,
                "max": 100,
                "scale": "Reds",
                "suffix": "%",
                "format": "{:,.2f}",
                "hidden": True,
            },
            "dup_all": {
                "title": "Dup. % for All Reads",
                "description": "Percentage of Duplicate Reads",
                "min": 0,
                "max": 100,
                "scale": "Reds",
                "suffix": "%",
                "format": "{:,.2f}",
                "hidden": False,
            },
        }

        self.general_stats_addcols(pd, header)

    ################################################################################
    ##                             Plotting Functions                             ##
    ################################################################################
    def chart_align_mapq(self):
        """Mapping overview plots"""
        # Extract data
        pd1 = {}
        pd2 = {}
        for s_name, d in self.biscuit_data["align_mapq"].items():
            if len(d) > 0:
                pd1[s_name] = {
                    "opt_align": d["opt_align"],
                    "sub_align": d["sub_align"],
                    "not_align": d["not_align"],
                }
                pd2[s_name] = d["mapqs"]

        # Mapping Overview bar graph
        pheader1 = {
            "opt_align": {"color": "#1f78b4", "name": "Optimally Aligned Reads"},
            "sub_align": {"color": "#a6cee3", "name": "Suboptimally Aligned Reads"},
            "not_align": {"color": "#b2df8a", "name": "Unaligned Reads"},
        }

        pconfig1 = {
            "id": "biscuit_mapping_overview",
            "title": "BISCUIT: Mapping Overview",
            "ylab": "Number of Reads",
            "cpswitch_counts_label": "# Reads",
            "tt_decimals": 0,
        }

        self.add_section(
            name="Mapping Overview",
            anchor="biscuit-mapping-overview",
            description="""
                Number of optimally aligned reads (`MAPQ>=40`), suboptimally
                aligned reads (`MAPQ<40`), and unmapped reads. Primary alignments only.
            """,
            helptext="""
                A good library should have a high fraction of reads
                that are optimally aligned. Note, suboptimally aligned reads
                include both non-unique alignments and imperfect alignments.
            """,
            plot=bargraph.plot(pd1, pheader1, pconfig1),
        )

        # Mapping Quality Distribution line graph
        pconfig2 = {
            "id": "biscuit_mapq",
            "title": "BISCUIT: Distribution of Mapping Qualities",
            "ymin": 0,
            "xmin": 0,
            "tt_label": "<strong>Q{point.x}:</strong> {point.y:.2f}% of mapped reads",
            "ysuffix": "%",
            "ylab": "% of primary mapped reads",
            "xlab": "Mapping Quality Score",
        }

        self.add_section(
            name="Mapping Quality Distribution",
            anchor="biscuit-mapq",
            description="""
                The percentage of the total number of mapped reads
                for each mapping quality score. Primary alignments only.
            """,
            helptext="""
                A good quality sample should have a high quality mapping score
                for the majority of alignments.
            """,
            plot=linegraph.plot(pd2, pconfig2),
        )

    def chart_align_strand(self):
        """Chart _strand_table.txt"""
        pd1 = {}
        pd2 = {}
        for s_name, d in self.biscuit_data["align_strand"].items():
            if len(d["read1"]) > 0:
                pd1[s_name] = d["read1"]
            if len(d["read2"]) > 0:
                pd2[s_name] = d["read2"]

        pheader = {
            "ff": {"color": "#F53855", "name": "OT: Original Top strand"},
            "fr": {"color": "#E37B40", "name": "CTOT: Complement to the Original Top strand"},
            "rf": {"color": "#46B29D", "name": "CTOB: Complement to the Original Bottom strand"},
            "rr": {"color": "#324D5C", "name": "OB: Original Bottom strand"},
        }
        pconfig = {
            "id": "biscuit_strand",
            "title": "BISCUIT: Mapping Strand Distribution",
            "ylab": "Number of Reads",
            "cpswitch_counts_label": "# Reads",
            "tt_decimals": 0,
            "data_labels": [{"name": "Read 1"}, {"name": "Read 2"}],
        }

        if max(len(pd1), len(pd2)) > 0:
            self.add_section(
                name="Mapping Strand Distribution",
                anchor="biscuit-strand",
                description="For primary alignments, shows the number of reads mapped to each strand.",
                helptext="""
                    Most bisulfite libraries typically map Read 1 to the original
                    strand (`OT`, `OB`) and Read 2 to the synthesized complement
                    strand (`CTOT`, `CTOB`).

                    Note that PBAT or PBAT-based libraries (like single-cell or
                    other low-inputs preps) usually map to the opposite set of
                    strands - Read 1 maps to CTOT/CTOB and Read 2 maps to OT/OB.
                """,
                plot=bargraph.plot([pd1, pd2], [pheader, pheader], pconfig),
            )

    def chart_align_isize(self):
        """Chart _isize_table.txt"""
        pd1 = {}
        pd2 = {}
        for s_name, d in self.biscuit_data["align_isize"].items():
            pd1[s_name] = d["percent"]
            pd2[s_name] = d["readcnt"]

        pconfig = {
            "id": "biscuit_isize",
            "title": "BISCUIT: Insert Size Distribution",
            "ymin": 0,
            "xmin": 0,
            "smooth_points": 1000,  # limit number of points / smooth data
            "xlab": "Insert Size",
            "data_labels": [
                {
                    "name": "% of Mapped Reads",
                    "ylab": "% of Mapped Reads",
                    "ysuffix": "%",
                    "tt_label": "<strong>IS%{x} bp:</strong> %{y:.2f}%",
                },
                {
                    "name": "Mapped Reads",
                    "ylab": "Mapped Reads",
                    "ysuffix": "",
                    "tt_label": "<strong>IS%{x} bp:</strong> %{y:,.0f}",
                },
            ],
        }

        self.add_section(
            name="Insert Size Distribution",
            anchor="biscuit-isize",
            description="Shows the distribution of insert sizes.",
            helptext="""
                Insert size is defined as:

                ```
                (right-most coordinate of reverse-mate read) - (left-most coordinate of forward-mate read)
                ```

                Insert sizes are calculated for reads with a _"mapped in
                proper pair"_ `samtools` flag, and `MAPQ >= 40`.
            """,
            plot=linegraph.plot([pd1, pd2], pconfig),
        )

    def chart_dup_report(self):
        """Charts _dup_report.txt"""
        pd1 = {}
        pd2 = {}
        for s_name, d in self.biscuit_data["dup_report"].items():
            if "all" in d and d["all"] > 0:
                pd1[s_name] = {"dup_rate": d["all"]}
            if "q40" in d and d["q40"] > 0:
                pd2[s_name] = {"dup_rate": d["q40"]}

        pheader = {
            "dup_rate": {"color": "#a50f15", "name": "Duplicate Rate"},
        }
        pconfig = {
            "id": "biscuit_dup_report",
            "cpswitch": False,
            "cpswitch_c_active": False,
            "title": "BISCUIT: Percentage of Duplicate Reads",
            "data_labels": [{"name": "Overall Duplicate Rate"}, {"name": "MAPQ>=40 Duplicate Rate"}],
            "ylab": "Duplicate rate",
            "ymin": 0,
            "ymax": 100,
            "y_clipmax": 110,
            "use_legend": False,
            "tt_decimals": 1,
            "tt_suffix": "%",
        }

        if len(pd1) > 0 or len(pd2) > 0:
            self.add_section(
                name="Duplicate Rates",
                anchor="biscuit-dup-report",
                description="Shows the percentage of reads that are duplicates.",
                helptext="""
                    `MAPQ >= 40` shows the duplicate rate for just the reads
                    with a mapping quality score of `MAPQ >= 40`. `All` shows
                    the overall duplicate rate.
                """,
                plot=bargraph.plot([pd1, pd2], [pheader, pheader], pconfig),
            )

    def chart_qc_cv(self):
        """Charts _cv_table.txt"""

        cats = [
            ("all_base", "a_b"),
            ("q40_base", "q_b"),
            ("all_base_botgc", "a_b_b"),
            ("q40_base_botgc", "q_b_b"),
            ("all_base_topgc", "a_b_t"),
            ("q40_base_topgc", "q_b_t"),
            ("all_cpg", "a_c"),
            ("q40_cpg", "q_c"),
            ("all_cpg_botgc", "a_c_b"),
            ("q40_cpg_botgc", "q_c_b"),
            ("all_cpg_topgc", "a_c_t"),
            ("q40_cpg_topgc", "q_c_t"),
        ]

        pd = {}
        for s_name, d in self.biscuit_data["qc_cv"].items():
            data = {}
            for cat, key in cats:
                if cat in d:
                    data["mu_" + key] = d[cat]["mu"]
                    data["cv_" + key] = d[cat]["cv"]
            if len(data) > 0:
                pd[s_name] = data

        shared_mean = {"min": 0, "format": "{:,.3f}", "minrange": 10}
        shared_cofv = {"min": 0, "format": "{:,.3f}", "minrange": 50}

        pheader = {
            "mu_a_b": dict(
                shared_mean, **{"title": "All Genome Mean", "description": "Mean Sequencing Depth for All Reads"}
            ),
            "mu_q_b": dict(
                shared_mean, **{"title": "Q40 Genome Mean", "description": "Mean Sequencing Depth for Q40 Reads"}
            ),
            "mu_a_b_b": dict(
                shared_mean,
                **{
                    "title": "Low GC All Gen. Mean",
                    "description": "Mean Sequencing Depth for All Reads in Low GC-Content Regions",
                },
            ),
            "mu_q_b_b": dict(
                shared_mean,
                **{
                    "title": "Low GC Q40 Gen. Mean",
                    "description": "Mean Sequencing Depth for Q40 Reads in Low GC-Content Regions",
                },
            ),
            "mu_a_b_t": dict(
                shared_mean,
                **{
                    "title": "High GC All Gen. Mean",
                    "description": "Mean Sequencing Depth for All Reads in High GC-Content Regions",
                },
            ),
            "mu_q_b_t": dict(
                shared_mean,
                **{
                    "title": "High GC Q40 Gen. Mean",
                    "description": "Mean Sequencing Depth for Q40 Reads in High GC-Content Regions",
                },
            ),
            "cv_a_b": dict(
                shared_cofv, **{"title": "All Genome CoV", "description": "Sequencing Depth CoV for All Reads"}
            ),
            "cv_q_b": dict(
                shared_cofv, **{"title": "Q40 Genome CoV", "description": "Sequencing Depth CoV for Q40 Reads"}
            ),
            "cv_a_b_b": dict(
                shared_cofv,
                **{
                    "title": "Low GC All Gen. CoV",
                    "description": "Sequencing Depth CoV for All Reads in Low GC-Content Regions",
                },
            ),
            "cv_q_b_b": dict(
                shared_cofv,
                **{
                    "title": "Low GC Q40 Gen. CoV",
                    "description": "Sequencing Depth CoV for Q40 Reads in Low GC-Content Regions",
                },
            ),
            "cv_a_b_t": dict(
                shared_cofv,
                **{
                    "title": "High GC All Gen. CoV",
                    "description": "Sequencing Depth CoV for All Reads in High GC-Content Regions",
                },
            ),
            "cv_q_b_t": dict(
                shared_cofv,
                **{
                    "title": "High GC Q40 Gen. CoV",
                    "description": "Sequencing Depth CoV for Q40 Reads in High GC-Content Regions",
                },
            ),
            "mu_a_c": dict(
                shared_mean, **{"title": "All CpGs Mean", "description": "Mean Sequencing Depth for All CpGs"}
            ),
            "mu_q_c": dict(
                shared_mean, **{"title": "Q40 CpGs Mean", "description": "Mean Sequencing Depth for Q40 CpGs"}
            ),
            "mu_a_c_b": dict(
                shared_mean,
                **{
                    "title": "Low GC All CpGs Mean",
                    "description": "Mean Sequencing Depth for All CpGs in Low GC-Content Regions",
                },
            ),
            "mu_q_c_b": dict(
                shared_mean,
                **{
                    "title": "Low GC Q40 CpGs Mean",
                    "description": "Mean Sequencing Depth for Q40 CpGs in Low GC-Content Regions",
                },
            ),
            "mu_a_c_t": dict(
                shared_mean,
                **{
                    "title": "High GC All CpGs Mean",
                    "description": "Mean Sequencing Depth for All CpGs in High GC-Content Regions",
                },
            ),
            "mu_q_c_t": dict(
                shared_mean,
                **{
                    "title": "High GC Q40 CpGs Mean",
                    "description": "Mean Sequencing Depth for Q40 CpGs in High GC-Content Regions",
                },
            ),
            "cv_a_c": dict(
                shared_cofv, **{"title": "All CpGs CoV", "description": "Sequencing Depth CoV for All CpGs"}
            ),
            "cv_q_c": dict(
                shared_cofv, **{"title": "Q40 CpGs CoV", "description": "Sequencing Depth CoV for Q40 CpGs"}
            ),
            "cv_a_c_b": dict(
                shared_cofv,
                **{
                    "title": "Low GC All CpGs CoV",
                    "description": "Sequencing Depth CoV for All CpGs in Low GC-Content Regions",
                },
            ),
            "cv_q_c_b": dict(
                shared_cofv,
                **{
                    "title": "Low GC Q40 CpGs CoV",
                    "description": "Sequencing Depth CoV for Q40 CpGs in Low GC-Content Regions",
                },
            ),
            "cv_a_c_t": dict(
                shared_cofv,
                **{
                    "title": "High GC All CpGs CoV",
                    "description": "Sequencing Depth CoV for All CpGs in High GC-Content Regions",
                },
            ),
            "cv_q_c_t": dict(
                shared_cofv,
                **{
                    "title": "High GC Q40 CpGs CoV",
                    "description": "Sequencing Depth CoV for Q40 CpGs in High GC-Content Regions",
                },
            ),
        }

        pconfig = {
            "id": "biscuit_seq_depth",
            "title": "BISCUIT: Sequencing Depth",
            "sort_rows": False,
        }

        if len(pd) > 0:
            self.add_section(
                name="Sequencing Depth Statistics",
                anchor="biscuit-seq-depth",
                description="""
                    Shows the sequence depth mean and uniformity measured by the Coefficient of Variation
                    (`CoV`, defined as `(std. dev.) / mean`).
                """,
                helptext="""
                    The plot shows coverage across different selections:

                    * _Genome_ (Gen.) - Statistics for all bases across the entire genome
                    * _CpGs_ - Statistics for CpGs
                    * _All_ - Statistics for any mapped bases/CpGs
                    * _Q40_ - Statistics only those bases/CpGs with mapping quality `MAPQ >= 40`
                    * _High GC_ - Bases / CpGs that overlap with the top 10% of 100bp windows for GC-content
                    * _Low GC_ - Bases / CpGs that overlap with the bottom 10% of 100bp windows for GC-content

                """,
                plot=violin.plot(pd, pheader, pconfig),
            )

    def chart_covdist(self):
        """Charts _covdist_*.txt"""
        pd = [
            self.biscuit_data["covdist_all_base"],
            self.biscuit_data["covdist_q40_base"],
            self.biscuit_data["covdist_all_cpg"],
            self.biscuit_data["covdist_q40_cpg"],
            self.biscuit_data["covdist_all_base_botgc"],
            self.biscuit_data["covdist_q40_base_botgc"],
            self.biscuit_data["covdist_all_cpg_botgc"],
            self.biscuit_data["covdist_q40_cpg_botgc"],
            self.biscuit_data["covdist_all_base_topgc"],
            self.biscuit_data["covdist_q40_base_topgc"],
            self.biscuit_data["covdist_all_cpg_topgc"],
            self.biscuit_data["covdist_q40_cpg_topgc"],
        ]

        pconfig = {
            "id": "biscuit_cumulative_coverage",
            "title": "BISCUIT: Cumulative Coverage",
            "ymin": 0,
            "tt_label": "<strong>{point.x}X:</strong> {point.y:.2f}M",
            "xlab": "Coverage",
            "ylab": "Millions of Bases (or CpGs)",
            "data_labels": [
                {"name": "All Bases", "ylab": "Millions of Bases"},
                {"name": "Q40 Bases", "ylab": "Millions of Bases"},
                {"name": "All CpGs", "ylab": "Millions of CpGs"},
                {"name": "Q40 CpGs", "ylab": "Millions of CpGs"},
                {"name": "Low GC All Bases", "ylab": "Millions of Bases"},
                {"name": "Low GC Q40 Bases", "ylab": "Millions of Bases"},
                {"name": "Low GC All CpGs", "ylab": "Millions of CpGs"},
                {"name": "Low GC Q40 CpGs", "ylab": "Millions of CpGs"},
                {"name": "High GC All Bases", "ylab": "Millions of Bases"},
                {"name": "High GC Q40 Bases", "ylab": "Millions of Bases"},
                {"name": "High GC All CpGs", "ylab": "Millions of CpGs"},
                {"name": "High GC Q40 CpGs", "ylab": "Millions of CpGs"},
            ],
        }

        self.add_section(
            name="Cumulative Coverage",
            anchor="biscuit-cumulative-coverage",
            description="Shows the number of bases or CpGs covered by a given number of reads.",
            helptext="""
                * _All_ - Coverage for any mapped reads
                * _Q40_ - Coverage for reads with mapping quality `MAPQ >= 40`
                * _High GC_ - Coverage for reads that overlap with the top 10% of 100bp windows for GC-content
                * _Low GC_ - Coverage for reads that overlap with the bottom 10% of 100bp windows for GC-content
            """,
            plot=linegraph.plot(pd, pconfig),
        )

    def chart_retention_readpos(self):
        """Charts _*RetentionByReadPos.txt"""
        pd = [
            dict([(s_name, dd["1"]) for s_name, dd in self.biscuit_data["cpg_retention_readpos"].items()]),
            dict([(s_name, dd["2"]) for s_name, dd in self.biscuit_data["cpg_retention_readpos"].items()]),
            dict([(s_name, dd["1"]) for s_name, dd in self.biscuit_data["cph_retention_readpos"].items()]),
            dict([(s_name, dd["2"]) for s_name, dd in self.biscuit_data["cph_retention_readpos"].items()]),
        ]

        pconfig = {
            "id": "biscuit_retention_cytosine",
            "title": "BISCUIT: Retention vs. Base Position in Read",
            "xlab": "Position in Read",
            "xsuffix": "bp",
            "ylab": "Retention Rate (%)",
            "ymin": 0,
            "ymax": 100,
            "y_minrange": 0,
            "y_clipmin": 0,
            "tt_label": "<strong>Position {point.x}:</strong> {point.y:.2f}%",
            "data_labels": [
                {"name": "CpG Read 1", "ylab": "CpG Retention Rate (%)"},
                {"name": "CpG Read 2", "ylab": "CpG Retention Rate (%)"},
                {"name": "CpH Read 1", "ylab": "CpH Retention Rate (%)"},
                {"name": "CpH Read 2", "ylab": "CpH Retention Rate (%)"},
            ],
        }

        self.add_section(
            name="Retention vs. Base Position in Read",
            anchor="biscuit-retention-cytosine",
            description="Distribution of cytosine retention rates across base positions in the read (a.k.a. _M-bias_ plot).",
            plot=linegraph.plot(pd, pconfig),
        )

    def chart_avg_retention(self):
        """Charts _total*ReadConversionRate.txt"""
        pd1 = self.biscuit_data["read_avg_retention_rate"]
        pd2 = self.biscuit_data["base_avg_retention_rate"]

        pheader = {
            "ca": {"color": "#D81B60", "name": "CpA Retention"},
            "cc": {"color": "#1E88E5", "name": "CpC Retention"},
            "cg": {"color": "#A0522D", "name": "CpG Retention"},
            "ct": {"color": "#004D40", "name": "CpT Retention"},
        }

        pconfig = {
            "id": "biscuit_retention",
            "cpswitch": False,
            "cpswitch_c_active": False,
            "title": "BISCUIT: Cytosine Retention",
            "data_labels": [{"name": "Read-averaged Retention"}, {"name": "Base-averaged Retention"}],
            "ylab": "Percent Retained",
            "ymin": 0,
            "ymax": 100,
            "y_clipmax": 110,
            "stacking": "group",
            "tt_decimals": 1,
            "tt_suffix": "%",
        }

        self.add_section(
            name="Cytosine Retention",
            anchor="biscuit-retention",
            description="Shows the cytosine retention rate for different contexts.",
            helptext="""
                The cytosine retention rate is calculated as `1 - (cytosine conversion rate)`.

                Assuming complete, but not over, bisulfite conversion, the cytosine retention rate
                is the average cytosine modification (including 5mC, 5hmC, etc) rate.

                Note, if a sample is missing from the Base-averaged Retention table,
                there wasn't sufficient data to plot that sample.
            """,
            plot=bargraph.plot([pd1, pd2], [pheader, pheader], pconfig),
        )


################################################################################
##                             Parsing Functions                              ##
################################################################################
def parse_align_mapq(f, fn):
    """Parse _mapq_table.txt"""
    file_data = f.splitlines()[2:]

    # Handle missing data
    if len(file_data) == 0:
        log.debug(f"No data available in {fn}. Will not fill corresponding entries.")
        return None

    mapq = {}
    for line in file_data:
        s = line.split()
        mapq[s[0]] = s[1]  # mapq[MAPQ] = number of reads

    data = {
        "frac_align": 0,
        "opt_align": 0,
        "sub_align": 0,
        "not_align": 0,
        "mapqs": dict(zip(range(61), [0 for _ in range(61)])),
    }
    if len(mapq) > 0:
        total = sum([int(cnt) for _, cnt in mapq.items() if _ != "unmapped"])
        for mq, cnt in mapq.items():
            if mq == "unmapped":
                data["not_align"] += int(cnt)
            else:
                data["mapqs"][int(mq)] = 100.0 * float(cnt) / total
                if int(mq) >= 40:
                    data["opt_align"] += int(cnt)
                else:
                    data["sub_align"] += int(cnt)

        data["frac_align"] = (
            100 * (data["opt_align"] + data["sub_align"]) / (data["opt_align"] + data["sub_align"] + data["not_align"])
        )

    return data


def parse_align_strand(f, fn):
    """Parse _strand_table.txt"""
    # Handle missing data
    if len(f.splitlines()) <= 2:
        log.debug(f"No data available in {fn}. Will not fill corresponding entries.")
        return None

    patterns = [
        r"(R1)\s+\((f)\)\:\s+(\d+)\s+(\d+)",
        r"(R1)\s+\((r)\)\:\s+(\d+)\s+(\d+)",
        r"(R2)\s+\((f)\)\:\s+(\d+)\s+(\d+)",
        r"(R2)\s+\((r)\)\:\s+(\d+)\s+(\d+)",
    ]

    data = {"read1": {}, "read2": {}}
    for pat in patterns:
        m = re.search(pat, f, re.MULTILINE)
        if m is not None:
            if m.group(1) == "R1":
                if m.group(2) == "f":
                    data["read1"]["ff"] = int(m.group(3))
                    data["read1"]["fr"] = int(m.group(4))
                else:
                    data["read1"]["rf"] = int(m.group(3))
                    data["read1"]["rr"] = int(m.group(4))
            else:
                if m.group(2) == "f":
                    data["read2"]["ff"] = int(m.group(3))
                    data["read2"]["fr"] = int(m.group(4))
                else:
                    data["read2"]["rf"] = int(m.group(3))
                    data["read2"]["rr"] = int(m.group(4))

    return data


def parse_align_isize(f, fn):
    """Parse _isize_table.txt"""
    file_data = f.splitlines()[2:]

    # Handle missing data
    if len(file_data) == 0:
        log.debug(f"No data available in {fn}. Will not fill corresponding entries.")
        return None

    data = {"percent": {}, "readcnt": {}}
    for line in file_data:
        fields = line.split("\t")
        key = int(fields[0])
        data["percent"][key] = 100.0 * float(fields[1])
        data["readcnt"][key] = float(fields[2])

    return data


def parse_dup_report(f, fn):
    """Parses _dup_report.txt"""
    # Handle missing data
    if len(f.splitlines()) != 5:
        log.debug(f"Incomplete data available in {fn}. Will not fill corresponding entries.")
        return None

    patterns = [
        (r"Number of duplicate reads:\s+(\d+)", r"Number of reads:\s+(\d+)", "all"),
        (r"Number of duplicate q40-reads:\s+(\d+)", r"Number of q40-reads:\s+(\d+)", "q40"),
    ]

    data = {}
    for pat_dup, pat_tot, key in patterns:
        m1 = re.search(pat_dup, f, re.MULTILINE)
        m2 = re.search(pat_tot, f, re.MULTILINE)
        if m1 is not None and m2 is not None:
            data[key] = 100.0 * float(m1.group(1)) / float(m2.group(1))
        else:
            log.debug(f"Incomplete data available in {fn}. Will not fill corresponding entries.")
            return None

    return data


def parse_qc_cv(f, fn):
    """Parses _cv_table.txt"""
    # Handle missing data
    if len(f.splitlines()) != 14:
        log.debug(f"Incomplete data available in {fn}. Will not fill corresponding entries.")
        return None

    targets = [
        "all_base",
        "all_cpg",
        "q40_base",
        "q40_cpg",
        "all_base_botgc",
        "all_cpg_botgc",
        "q40_base_botgc",
        "q40_cpg_botgc",
        "all_base_topgc",
        "all_cpg_topgc",
        "q40_base_topgc",
        "q40_cpg_topgc",
    ]

    data = {}
    for t in targets:
        m = re.search(rf"{t}\t([\d\.]+)\t([\d\.]+)\t([\d\.]+)", f, re.MULTILINE)
        if m is not None:
            data[t] = {"mu": float(m.group(1)), "sigma": float(m.group(2)), "cv": float(m.group(3))}
        else:
            log.debug(f"Incomplete data available in {fn}. Will not fill corresponding entries.")
            return None

    return data


def parse_covdist(f, fn):
    """Parses _covdist_*.txt"""
    file_data = f.splitlines()[2:]
    # Handle missing data
    if len(file_data) == 0:
        log.debug(f"No data available in {fn}. Will not fill corresponding entries.")
        return None

    data = {}
    for line in file_data:
        fields = line.split()
        data[int(float(fields[0]))] = int(float(fields[1]))

    covs = sorted([k for k in data])[:31]
    _ccov_cnt = sum(data.values())

    ccov_cnts = []
    for cov in covs:
        ccov_cnts.append(_ccov_cnt / 1000000.0)
        _ccov_cnt -= data[cov]

    return dict(zip(covs, ccov_cnts))


def parse_retention_readpos(f, fn):
    """Parses _*RetentionByReadPos.txt"""
    file_data = f.splitlines()[2:]

    # Handle missing data
    if len(file_data) == 0:
        log.debug(f"No data available in {fn}. Will not fill corresponding entries.")
        return None

    r1 = {"C": {}, "R": {}}
    r2 = {"C": {}, "R": {}}
    for line in file_data:
        fields = line.strip().split("\t")

        if fields[0] not in ["1", "2"] or fields[2] not in ["C", "R"]:
            return None
        if fields[0] == "1":
            r1[fields[2]][int(fields[1])] = int(fields[3])
        elif fields[0] == "2":
            r2[fields[2]][int(fields[1])] = int(fields[3])

    r1rate = {}
    for k in sorted(r1["C"].keys()):
        if k in r1["R"]:
            r1rate[k] = 100.0 * float(r1["R"][k]) / (r1["R"][k] + r1["C"][k])

    r2rate = {}
    for k in sorted(r2["C"].keys()):
        if k in r2["R"]:
            r2rate[k] = 100.0 * float(r2["R"][k]) / (r2["R"][k] + r2["C"][k])

    return {"1": r1rate, "2": r2rate}


def parse_avg_retention(f, fn):
    """Parses _total*ConversionRate.txt"""
    file_data = f.splitlines()[2:]

    # Handle missing data
    if len(file_data) == 0:
        log.debug(f"No data available in {fn}. Will not fill corresponding entries.")
        return None

    data = {}
    for line in file_data:
        fields = line.split("\t")
        # Skip rows that have NaNs as something went wrong in processing
        if "nan" in fields:
            log.debug(f"Found NaN in {fn}. Skipping.")
            continue

        # BISCUIT returns -1 if insufficient data. Only fill fields with value >= 0.
        if float(fields[0]) >= 0:
            data["ca"] = 100.0 * float(fields[0])
        if float(fields[1]) >= 0:
            data["cc"] = 100.0 * float(fields[1])
        if float(fields[2]) >= 0:
            data["cg"] = 100.0 * float(fields[2])
        if float(fields[3]) >= 0:
            data["ct"] = 100.0 * float(fields[3])

    return data
